कथन 1| रैखिक प्रतिगमन आकलक का प्रसरण सभी अपक्षपाती आकलकों में सबसे छोटा होता है। कथन 2| AdaBoost द्वारा एकत्रित वर्गीकरणों को सौंपे गए गुणांक α हमेशा गैर-नकारात्मक होते हैं।,"सत्य, सत्य","झूठ, झूठ","सत्य, असत्य","झूठ, सच",D
कथन 1| RoBERTa उस कॉर्पस पर पूर्व-प्रशिक्षण लेता है जो BERT के पूर्व-प्रशिक्षण कॉर्पस से लगभग 10 गुना बड़ा है। कथन 2| 2018 में ResNeXts आमतौर पर tanh सक्रियण फलनों का उपयोग करते थे।,"सत्य, सत्य","झूठ, झूठ","सत्य, असत्य","झूठ, सच",C
"कथन 1| समर्थन वेक्टर मशीनें, लॉजिस्टिक प्रतिगमन मॉडल की तरह, दिए गए इनपुट उदाहरण के लिए संभावित लेबल पर एक संभाव्यता वितरण देती हैं। कथन 2| हम उम्मीद करेंगे कि जैसे-जैसे हम रैखिक कर्नेल से उच्च क्रम के बहुपदीय कर्नेल की ओर बढ़ते हैं, समर्थन वेक्टर सामान्यतः समान रहेंगे।","सत्य, सत्य","झूठ, झूठ","सत्य, असत्य","झूठ, सच",B
"एक मशीन लर्निंग समस्या में चार विशेषताएँ और एक वर्ग शामिल हैं। विशेषताओं में क्रमशः 3, 2, 2, और 2 संभावित मान हैं। वर्ग के 3 संभावित मान हैं। अधिकतम कितने अलग-अलग संभावित उदाहरण हो सकते हैं?",१२,२४,४८,७२,D
"2020 तक, उच्च-रिज़ॉल्यूशन छवियों को वर्गीकृत करने के लिए कौन सी आर्किटेक्चर सबसे अच्छी है?",कॉन्वोल्यूशनल नेटवर्क,ग्राफ नेटवर्क,पूरी तरह से जुड़े नेटवर्क,आरबीएफ नेटवर्क,A
"कथन 1| अपेक्षा अधिकतमीकरण एल्गोरिथम के क्रमिक पुनरावृत्तियों के माध्यम से डेटा की लॉग-संभावना हमेशा बढ़ेगी।

कथन 2| Q-लर्निंग का एक नुकसान यह है कि इसका उपयोग केवल तभी किया जा सकता है जब सीखने वाले को पहले से यह ज्ञान हो कि उसके कार्य उसके परिवेश को कैसे प्रभावित करते हैं।","सत्य, सत्य","झूठ, झूठ","सत्य, असत्य","झूठ, सच",B
मान लीजिए कि हमने अपने लागत फ़ंक्शन का ग्रेडिएंट गणना किया है और उसे एक वेक्टर g में संग्रहीत किया है। ग्रेडिएंट को देखते हुए एक ग्रेडिएंट डिसेंट अपडेट की लागत क्या है?,O(D),O(N),O(ND),O(ND^2),A
"कथन 1| एक सतत यादृच्छिक चर x और उसके संभाव्यता वितरण फलन p(x) के लिए, यह सत्य है कि सभी x के लिए 0 ≤ p(x) ≤ 1 होता है। कथन 2| निर्णय वृक्ष को सूचना लाभ को न्यूनतम करके सीखा जाता है।","सत्य, सत्य","झूठ, झूठ","सत्य, असत्य","झूठ, सच",B
निम्नलिखित बेसियन नेटवर्क पर विचार करें। इस बेसियन नेटवर्क H -> U <- P <- W के लिए कितने स्वतंत्र पैरामीटर्स की आवश्यकता है?,२,४,८,१६,C
"जैसे-जैसे प्रशिक्षण उदाहरणों की संख्या अनंत तक जाती है, उस डेटा पर प्रशिक्षित आपका मॉडल होगा:",कम प्रसरण,उच्च प्रसरण,समान प्रसरण,उपरोक्त में से कोई नहीं,A
"कथन 1| 2D समतल में सभी आयतों का समूह (जिसमें गैर-अक्ष संरेखित आयत भी शामिल हैं) 5 बिंदुओं के एक समूह को विखंडित कर सकता है।
कथन 2| जब k = 1 होता है, तब k-निकटतम पड़ोसी वर्गीकरण का VC-आयाम अनंत होता है।","सत्य, सत्य","झूठ, झूठ","सत्य, असत्य","झूठ, सच",A
_ एक ऐसे मॉडल को संदर्भित करता है जो न तो प्रशिक्षण डेटा को मॉडल कर सकता है और न ही नए डेटा पर सामान्यीकरण कर सकता है।,अच्छी फिटिंग,अतिफिटिंग,अंडरफिटिंग,उपरोक्त सभी,C
कथन 1| F1 स्कोर विशेष रूप से उच्च वर्ग असंतुलन वाले डेटासेट के लिए उपयोगी हो सकता है। कथन 2| ROC वक्र के नीचे का क्षेत्र असामान्य डिटेक्टरों का मूल्यांकन करने के लिए उपयोग किए जाने वाले मुख्य मेट्रिक्स में से एक है।,"सत्य, सत्य","झूठ, झूठ","सत्य, असत्य","झूठ, सच",A
"कथन 1| बैक-प्रोपेगेशन एल्गोरिथम छिपी हुई परतों वाले एक वैश्विक रूप से इष्टतम तंत्रिका नेटवर्क को सीखता है।

कथन 2| एक रेखा का VC आयाम अधिकतम 2 होना चाहिए, क्योंकि मैं 3 बिंदुओं का कम से कम एक ऐसा मामला ढूंढ सकता हूं जिसे किसी भी रेखा द्वारा विखंडित नहीं किया जा सकता।","सत्य, सत्य","झूठ, झूठ","सत्य, असत्य","झूठ, सच",B
उच्च एन्ट्रॉपी का अर्थ है कि वर्गीकरण में विभाजन हैं,शुद्ध,शुद्ध नहीं,उपयोगी,बेकार,B
"कथन 1| मूल ResNet पेपर में बैच नॉर्मलाइजेशन नहीं, बल्कि लेयर नॉर्मलाइजेशन का उपयोग किया गया है। कथन 2| DCGANs प्रशिक्षण को स्थिर करने के लिए स्व-ध्यान का उपयोग करते हैं।","सत्य, सत्य","झूठ, झूठ","सत्य, असत्य","झूठ, सच",B
"किसी विशेष डेटा सेट के लिए एक रैखिक प्रतिगमन मॉडल बनाने में, आप एक विशेषता के गुणांक को अपेक्षाकृत उच्च नकारात्मक मूल्य रखते हुए देखते हैं। यह सुझाव देता है कि",इस विशेषता का मॉडल पर एक मजबूत प्रभाव पड़ता है (इसे बनाए रखा जाना चाहिए),इस सुविधा का मॉडल पर कोई मजबूत प्रभाव नहीं है (इसे नजरअंदाज किया जाना चाहिए),इस सुविधा के महत्व पर टिप्पणी करना अतिरिक्त जानकारी के बिना संभव नहीं है,कुछ भी निर्धारित नहीं किया जा सकता।,C
"एक तंत्रिका नेटवर्क के लिए, इनमें से कौन सी संरचनात्मक मान्यता वह है जो अंडरफिटिंग (यानी उच्च पूर्वाग्रह मॉडल) और ओवरफिटिंग (यानी उच्च विचरण मॉडल) के बीच संतुलन को सबसे अधिक प्रभावित करती है:",छिपे हुए नोड्स की संख्या,सीखने की दर,प्रारंभिक वजन का चयन,एक स्थिरांक-पद इकाई इनपुट का उपयोग,A
"बहुपद प्रतिगमन के लिए, इनमें से कौन सी संरचनात्मक मान्यता वह है जो अंडरफिटिंग और ओवरफिटिंग के बीच संतुलन को सबसे अधिक प्रभावित करती है:",बहुपद की डिग्री,चाहे हम वजनों को मैट्रिक्स इनवर्जन या ग्रेडिएंट डिसेंट द्वारा सीखें,गाउसीय शोर की मानी गई प्रसरण,एक स्थिरांक-पद इकाई इनपुट का उपयोग,A
"कथन 1| 2020 तक, कुछ मॉडल CIFAR-10 पर 98% से अधिक सटीकता प्राप्त करते हैं। कथन 2| मूल ResNets को Adam ऑप्टिमाइज़र के साथ अनुकूलित नहीं किया गया था।","सत्य, सत्य","झूठ, झूठ","सत्य, असत्य","झूठ, सच",A
K-मीन्स एल्गोरिथम,फीचर स्पेस का आयाम नमूनों की संख्या से बड़ा नहीं होना चाहिए,जब K = 1 होता है तो उद्देश्य फलन का सबसे छोटा मान होता है,दिए गए संख्या के क्लस्टर्स के लिए वर्ग के भीतर की विविधता को कम करता है,केवल तभी वैश्विक इष्टतम की ओर अभिसरित होता है जब प्रारंभिक माध्य स्वयं नमूनों में से कुछ के रूप में चुने जाते हैं,C
कथन 1| VGGNets में AlexNet की पहली परत के कर्नेल की तुलना में कम चौड़ाई और ऊंचाई वाले कन्वोल्यूशनल कर्नेल होते हैं। कथन 2| बैच नॉर्मलाइजेशन से पहले डेटा-निर्भर वेट इनिशियलाइजेशन प्रक्रियाएं पेश की गई थीं।,"सत्य, सत्य","झूठ, झूठ","सत्य, असत्य","झूठ, सच",A
"निम्नलिखित मैट्रिक्स की रैंक क्या है? A = [[1, 1, 1], [1, 1, 1], [1, 1, 1]]",०,१,२,३,B
"कथन 1| घनत्व अनुमान (जैसे, कर्नेल घनत्व अनुमानक का उपयोग करके) का उपयोग वर्गीकरण करने के लिए किया जा सकता है। कथन 2| लॉजिस्टिक प्रतिगमन और गाउसीय नेइव बेयस (पहचान वर्ग सहप्रसरण के साथ) के बीच संबंध का अर्थ है कि दोनों वर्गीकरणों के मापदंडों के बीच एक-से-एक संबंध है।","सत्य, सत्य","झूठ, झूठ","सत्य, असत्य","झूठ, सच",C
मान लीजिए कि हम स्थानिक डेटा पर क्लस्टरिंग करना चाहते हैं जैसे कि घरों के ज्यामितीय स्थान। हम कई अलग-अलग आकारों और रूपों के क्लस्टर बनाना चाहते हैं। निम्नलिखित में से कौन सी विधि सबसे उपयुक्त है?,निर्णय वृक्ष,घनत्व-आधारित क्लस्टरिंग,मॉडल-आधारित क्लस्टरिंग,के-मीन्स क्लस्टरिंग,B
"कथन 1| AdaBoost में गलत वर्गीकृत उदाहरणों के भार एक ही गुणात्मक कारक द्वारा बढ़ जाते हैं।
कथन 2| AdaBoost में, tवें कमजोर वर्गीकारक की भारित प्रशिक्षण त्रुटि e_t, D_t भारों के साथ प्रशिक्षण डेटा पर, t के फलन के रूप में बढ़ने की प्रवृत्ति रखती है।","सत्य, सत्य","झूठ, झूठ","सत्य, असत्य","झूठ, सच",A
MLE अनुमान अक्सर अवांछनीय होते हैं क्योंकि,वे पक्षपाती हैं,उनमें उच्च विचरण होता है,वे संगत आकलक नहीं हैं,उपरोक्त में से कोई नहीं,B
"ग्रेडिएंट डिसेंट की कम्प्यूटेशनल जटिलता है,",डी में रैखिक,एन के अनुरूप रैखिक,डी में बहुपद,संख्या पुनरावृत्तियों पर निर्भर,C
कई निर्णय वृक्षों के आउटपुट का औसत निकालना _ में मदद करता है।,पक्षपात बढ़ाएं,पक्षपात कम करें,विचरण बढ़ाएं,विचरण कम करें,D
"मौजूदा विशेषताओं के उपसमुच्चय पर रैखिक प्रतिगमन लागू करके प्राप्त मॉडल, उपसमुच्चय की पहचान की प्रक्रिया के अंत में प्राप्त मॉडल से भिन्न हो सकता है",सर्वोत्तम-उपसमुच्चय चयन,आगे की चरणबद्ध चयन,आगे की चरणबद्ध चयन प्रक्रिया,उपरोक्त सभी,C
न्यूरल नेटवर्क्स,उत्तल उद्देश्य फलन को अनुकूलित करें,केवल स्टोकास्टिक ग्रेडिएंट डिसेंट के साथ प्रशिक्षित किया जा सकता है,विभिन्न सक्रियण फलनों का मिश्रण उपयोग कर सकते हैं,उपरोक्त में से कोई नहीं,C
"रोग D की घटना लगभग 100 लोगों में 5 मामले है (यानी, P(D) = 0.05)। मान लीजिए कि बूलियन यादृच्छिक चर D का अर्थ है कि रोगी को ""रोग D है"" और बूलियन यादृच्छिक चर TP का अर्थ है ""परीक्षण सकारात्मक है।"" रोग D के लिए परीक्षण बहुत सटीक माने जाते हैं, इस अर्थ में कि जब आपको रोग है तो सकारात्मक परीक्षण की संभावना 0.99 है, और जब आपको रोग नहीं है तो नकारात्मक परीक्षण की संभावना 0.97 है। P(TP) क्या है, सकारात्मक परीक्षण की पूर्व संभावना।",0.0368,0.473,0.078,उपरोक्त में से कोई नहीं,C
"कथन 1| रेडियल आधार कर्नेल फ़ंक्शन के माध्यम से विशेषता स्थान Q में मैप किए जाने के बाद, अभारित यूक्लिडियन दूरी का उपयोग करने वाला 1-NN मूल स्थान की तुलना में बेहतर वर्गीकरण प्रदर्शन प्राप्त करने में सक्षम हो सकता है (हालांकि हम इसकी गारंटी नहीं दे सकते)। कथन 2| एक पर्सेप्ट्रॉन का VC आयाम एक साधारण रैखिक SVM के VC आयाम से छोटा होता है।","सत्य, सत्य","झूठ, झूठ","सत्य, असत्य","झूठ, सच",B
ग्रिड सर्च का नुकसान है,इसे गैर-अवकलनीय फलनों पर लागू नहीं किया जा सकता।,इसे असतत फलनों पर लागू नहीं किया जा सकता।,इसे लागू करना कठिन है।,यह बहु रैखिक प्रतिगमन के लिए उचित रूप से धीमी गति से चलता है।,D
किसी क्षेत्र में विभिन्न संकेतों के आधार पर वर्षा की मात्रा की भविष्यवाणी करना एक ______ समस्या है।,पर्यवेक्षित अधिगम,असुपरवाइज्ड लर्निंग,क्लस्टरिंग,उपरोक्त में से कोई नहीं,A
निम्नलिखित में से कौन सा वाक्य प्रतिगमन के संबंध में असत्य है?,यह इनपुट को आउटपुट से संबंधित करता है।,यह भविष्यवाणी के लिए उपयोग किया जाता है।,इसका उपयोग व्याख्या के लिए किया जा सकता है।,यह कारण-प्रभाव संबंधों का पता लगाता है,D
निम्नलिखित में से कौन सा एक निर्णय वृक्ष की छंटाई का मुख्य कारण है?,कंप्यूटिंग समय बचाने के लिए परीक्षण के दौरान,निर्णय वृक्ष को संग्रहित करने के लिए जगह बचाने के लिए,प्रशिक्षण सेट त्रुटि को छोटा करने के लिए,प्रशिक्षण सेट का अति-अनुकूलन (ओवरफिटिंग) रोकने के लिए,D
"कथन 1| कर्नेल घनत्व आकलक मूल डेटा सेट में प्रत्येक बिंदु Xi पर Yi = 1/n मान के साथ कर्नेल प्रतिगमन करने के समतुल्य है।

कथन 2| एक सीखे हुए निर्णय वृक्ष की गहराई वृक्ष बनाने के लिए उपयोग किए गए प्रशिक्षण उदाहरणों की संख्या से अधिक हो सकती है।","सत्य, सत्य","झूठ, झूठ","सत्य, असत्य","झूठ, सच",B
मान लीजिए कि आपका मॉडल ओवरफिटिंग कर रहा है। निम्नलिखित में से कौन सा ओवरफिटिंग को कम करने का एक वैध तरीका नहीं है?,प्रशिक्षण डेटा की मात्रा बढ़ाएं।,त्रुटि न्यूनीकरण के लिए उपयोग किए जा रहे अनुकूलन एल्गोरिथम में सुधार करें।,मॉडल की जटिलता कम करें।,प्रशिक्षण डेटा में शोर कम करें।,B
वक्तव्य 1| सॉफ्टमैक्स फ़ंक्शन आमतौर पर बहुवर्गीय लॉजिस्टिक प्रतिगमन में उपयोग किया जाता है। वक्तव्य 2| एक असमान सॉफ्टमैक्स वितरण का तापमान उसकी एन्ट्रॉपी को प्रभावित करता है।,"सत्य, सत्य","झूठ, झूठ","सत्य, असत्य","झूठ, सच",A
निम्नलिखित में से कौन सा/से SVM के संबंध में सत्य है/हैं?,"दो आयामी डेटा बिंदुओं के लिए, एक रैखिक SVM द्वारा सीखी गई विभाजक अतिसमतल एक सीधी रेखा होगी।","सिद्धांत में, एक गाउसियन कर्नेल SVM किसी भी जटिल विभाजक अतिसमतल को मॉडल नहीं कर सकता।","प्रत्येक कर्नेल फ़ंक्शन के लिए जो एक SVM में उपयोग किया जाता है, एक समतुल्य बंद रूप आधार विस्तार प्राप्त किया जा सकता है।",एसवीएम में ओवरफिटिंग सपोर्ट वेक्टर्स की संख्या का फलन नहीं है।,A
"निम्नलिखित में से कौन सा दिए गए बेसियन नेटवर्क H -> U <- P <- W द्वारा वर्णित H, U, P, और W की संयुक्त संभावना है? [नोट: सशर्त संभावनाओं के गुणनफल के रूप में]","पी(एच, यू, पी, डब्ल्यू) = पी(एच) * पी(डब्ल्यू) * पी(पी) * पी(यू)","पी(एच, यू, पी, डब्ल्यू) = पी(एच) * पी(डब्ल्यू) * पी(पी | डब्ल्यू) * पी(डब्ल्यू | एच, पी)","पी(एच, यू, पी, डब्ल्यू) = पी(एच) * पी(डब्ल्यू) * पी(पी | डब्ल्यू) * पी(यू | एच, पी)",उपरोक्त में से कोई नहीं,C
"कथन 1| चूंकि रेडियल बेस कर्नेल वाले SVM के लिए VC आयाम अनंत है, इसलिए ऐसा SVM बहुपदीय कर्नेल वाले SVM से खराब होना चाहिए जिसका VC आयाम परिमित है।

कथन 2| रैखिक सक्रियण फलनों वाला दो परतीय तंत्रिका नेटवर्क अनिवार्य रूप से रैखिक विभाजकों का एक भारित संयोजन है, जो दिए गए डेटासेट पर प्रशिक्षित होता है; रैखिक विभाजकों पर निर्मित बूस्टिंग एल्गोरिथ्म भी रैखिक विभाजकों का एक संयोजन खोजता है, इसलिए ये दोनों एल्गोरिथ्म एक ही परिणाम देंगे।","सत्य, सत्य","झूठ, झूठ","सत्य, असत्य","झूठ, सच",B
कथन 1| ID3 एल्गोरिथ्म इष्टतम निर्णय वृक्ष खोजने की गारंटी देता है। कथन 2| एक सतत संभाव्यता वितरण पर विचार करें जिसका घनत्व f() हर जगह शून्य नहीं है। किसी मान x की संभावना f(x) के बराबर होती है।,"सत्य, सत्य","झूठ, झूठ","सत्य, असत्य","झूठ, सच",B
"एक न्यूरल नेट दिया गया है जिसमें N इनपुट नोड्स हैं, कोई छिपी हुई परतें नहीं हैं, एक आउटपुट नोड है, एन्ट्रॉपी लॉस और सिग्मॉइड एक्टिवेशन फंक्शंस के साथ, निम्नलिखित में से कौन से एल्गोरिदम (उचित हाइपर-पैरामीटर्स और इनिशियलाइजेशन के साथ) वैश्विक ऑप्टिमम को खोजने के लिए उपयोग किए जा सकते हैं?",स्टोकास्टिक ग्रेडिएंट डिसेंट,मिनी-बैच ग्रेडिएंट डिसेंट,बैच ग्रेडिएंट डिसेंट,उपरोक्त सभी,D
"लिनियर मॉडल में अधिक आधार फ़ंक्शन जोड़ने पर, सबसे संभावित विकल्प चुनें:",मॉडल पूर्वाग्रह को कम करता है,अनुमान पूर्वाग्रह को कम करता है,विचरण को कम करता है,पूर्वाग्रह और प्रसरण को प्रभावित नहीं करता,A
निम्नलिखित बेसियन नेटवर्क पर विचार करें। यदि हम स्वतंत्रता या सशर्त स्वतंत्रता के बारे में कोई मान्यता नहीं बनाते हैं तो हमें कितने स्वतंत्र पैरामीटर की आवश्यकता होगी H -> U <- P <- W?,३,४,७,१५,D
आउट-ऑफ-डिस्ट्रीब्यूशन डिटेक्शन के लिए एक और शब्द क्या है?,असामान्यता पहचान,एकल-वर्ग पहचान,प्रशिक्षण-परीक्षण असंगति सुदृढ़ता,पृष्ठभूमि पहचान,A
"कथन 1| हम कमजोर शिक्षकों h को बढ़ावा देकर एक वर्गीकरणकर्ता f सीखते हैं। f की निर्णय सीमा का कार्यात्मक रूप h के समान होता है, लेकिन अलग-अलग मापदंडों के साथ। (उदाहरण के लिए, यदि h एक रैखिक वर्गीकरणकर्ता था, तो f भी एक रैखिक वर्गीकरणकर्ता है)। कथन 2| क्रॉस वैलिडेशन का उपयोग बूस्टिंग में पुनरावृत्तियों की संख्या का चयन करने के लिए किया जा सकता है; यह प्रक्रिया अतिफिटिंग को कम करने में मदद कर सकती है।","सत्य, सत्य","झूठ, झूठ","सत्य, असत्य","झूठ, सच",D
कथन 1| हाईवे नेटवर्क को ResNets के बाद पेश किया गया था और वे अधिकतम पूलिंग के बजाय कनवोल्यूशन का उपयोग करते हैं। कथन 2| DenseNets आमतौर पर ResNets की तुलना में अधिक मेमोरी का उपयोग करते हैं।,"सत्य, सत्य","झूठ, झूठ","सत्य, असत्य","झूठ, सच",D
"यदि N प्रशिक्षण डेटासेट में उदाहरणों की संख्या है, तो निकटतम पड़ोसी का वर्गीकरण चलने का समय होता है",O(1),O( N ),O(लॉग N),O( N^2 ),B
"कथन 1| मूल ResNets और ट्रांसफॉर्मर्स फीडफॉरवर्ड न्यूरल नेटवर्क हैं। कथन 2| मूल ट्रांसफॉर्मर्स स्व-ध्यान का उपयोग करते हैं, लेकिन मूल ResNet नहीं करता है।","सत्य, सत्य","झूठ, झूठ","सत्य, असत्य","झूठ, सच",A
"कथन 1| RELUs एकदिष्ट नहीं होते हैं, लेकिन सिग्मॉइड एकदिष्ट होते हैं। कथन 2| ग्रेडिएंट डिसेंट के साथ प्रशिक्षित न्यूरल नेटवर्क उच्च संभावना के साथ वैश्विक इष्टतम तक पहुंचते हैं।","सत्य, सत्य","झूठ, झूठ","सत्य, असत्य","झूठ, सच",D
एक तंत्रिका नेटवर्क में सिग्मॉइड नोड का संख्यात्मक आउटपुट:,"असीमित है, जो सभी वास्तविक संख्याओं को समाहित करता है।","असीमित है, जो सभी पूर्णांकों को समाहित करता है।",0 और 1 के बीच सीमित है।,-1 और 1 के बीच सीमित है।,C
निम्नलिखित में से किसका उपयोग केवल तब किया जा सकता है जब प्रशिक्षण डेटा रैखिक रूप से पृथक्करणीय हो?,रैखिक कठोर-सीमा एसवीएम,रैखिक लॉजिस्टिक प्रतिगमन,रैखिक मृदु मार्जिन एसवीएम,केंद्रक विधि।,A
निम्नलिखित में से कौन से स्थानिक क्लस्टरिंग एल्गोरिदम हैं?,विभाजन आधारित क्लस्टरिंग,के-मीन्स क्लस्टरिंग,ग्रिड आधारित क्लस्टरिंग,उपरोक्त सभी,D
कथन 1| सपोर्ट वेक्टर मशीनों द्वारा निर्मित अधिकतम मार्जिन निर्णय सीमाएं सभी रैखिक वर्गीकरणों में सबसे कम सामान्यीकरण त्रुटि रखती हैं। कथन 2| वर्ग-सशर्त गाउसीय वितरण वाले जनरेटिव मॉडल से प्राप्त कोई भी निर्णय सीमा सिद्धांत रूप में एसवीएम और तीन या उससे कम डिग्री के बहुपदीय कर्नेल के साथ पुनः उत्पादित की जा सकती है।,"सत्य, सत्य","झूठ, झूठ","सत्य, असत्य","झूठ, सच",D
कथन 1| रैखिक मॉडल का L2 नियमितीकरण L1 नियमितीकरण की तुलना में मॉडल को अधिक विरल बनाने की प्रवृत्ति रखता है। कथन 2| अवशिष्ट कनेक्शन ResNets और ट्रांसफॉर्मर्स में पाए जा सकते हैं।,"सत्य, सत्य","झूठ, झूठ","सत्य, असत्य","झूठ, सच",D
"मान लीजिए कि हम P(H|E, F) की गणना करना चाहते हैं और हमारे पास कोई सशर्त स्वतंत्रता जानकारी नहीं है। निम्नलिखित में से कौन सा संख्या समूह गणना के लिए पर्याप्त है?","P(E, F), P(H), P(E|H), P(F|H)","P(E, F), P(H), P(E, F|H)","पी(एच), पी(ई|एच), पी(एफ|एच)","P(E, F), P(E|H), P(F|H)",B
निम्नलिखित में से कौन बैगिंग करते समय ओवरफिटिंग को रोकता है?,प्रतिस्थापन के साथ नमूनाकरण का उपयोग नमूनाकरण तकनीक के रूप में,कमजोर वर्गीकारकों का उपयोग,वर्गीकरण एल्गोरिदम का उपयोग जो अतिफिटिंग के प्रति संवेदनशील नहीं हैं,प्रत्येक प्रशिक्षित वर्गीकरण पर किया जाने वाला सत्यापन का अभ्यास,B
"कथन 1| PCA और स्पेक्ट्रल क्लस्टरिंग (जैसे एंड्रयू एनजी का) दो अलग-अलग मैट्रिसेस पर आइगेनडिकंपोजिशन करते हैं। हालांकि, इन दोनों मैट्रिसेस का आकार समान होता है। कथन 2| चूंकि वर्गीकरण रिग्रेशन का एक विशेष मामला है, लॉजिस्टिक रिग्रेशन रैखिक रिग्रेशन का एक विशेष मामला है।","सत्य, सत्य","झूठ, झूठ","सत्य, असत्य","झूठ, सच",B
"कथन 1| स्टैनफोर्ड सेंटीमेंट ट्रीबैंक में फिल्म समीक्षाएँ थीं, पुस्तक समीक्षाएँ नहीं। कथन 2| पेन ट्रीबैंक का उपयोग भाषा मॉडलिंग के लिए किया गया है।","सत्य, सत्य","झूठ, झूठ","सत्य, असत्य","झूठ, सच",A
"निम्नलिखित मैट्रिक्स के शून्य स्थान की आयामिकता क्या है? A = [[3, 2, −9], [−6, −4, 18], [12, 8, −36]]",०,१,२,३,C
सपोर्ट वेक्टर्स क्या हैं?,निर्णय सीमा से सबसे दूर के उदाहरण।,एसवीएम में f(x) की गणना करने के लिए केवल आवश्यक उदाहरण।,डेटा केंद्रक।,एसवीएम में सभी उदाहरण जिनका गैर-शून्य भार αk है।,B
"कथन 1| Word2Vec पैरामीटर्स को प्रतिबंधित बोल्ट्जमान मशीन का उपयोग करके आरंभ नहीं किया गया था।
कथन 2| tanh फ़ंक्शन एक अरैखिक सक्रियण फ़ंक्शन है।","सत्य, सत्य","झूठ, झूठ","सत्य, असत्य","झूठ, सच",A
"यदि आपका प्रशिक्षण नुकसान युगों की संख्या के साथ बढ़ता है, तो निम्नलिखित में से कौन सा सीखने की प्रक्रिया में एक संभावित समस्या हो सकती है?",रेगुलराइज़ेशन बहुत कम है और मॉडल ओवरफिटिंग कर रहा है,रेगुलराइज़ेशन बहुत अधिक है और मॉडल अंडरफिटिंग कर रहा है,कदम का आकार बहुत बड़ा है,कदम का आकार बहुत छोटा है,C
"किसी बीमारी D की घटना लगभग 100 लोगों में 5 मामले है (यानी, P(D) = 0.05)। मान लीजिए कि बूलियन यादृच्छिक चर D का अर्थ है कि एक रोगी को ""बीमारी D है"" और बूलियन यादृच्छिक चर TP का अर्थ है ""परीक्षण सकारात्मक है।"" बीमारी D के लिए परीक्षण बहुत सटीक माने जाते हैं, इस अर्थ में कि जब आपको बीमारी है तो सकारात्मक परीक्षण की संभावना 0.99 है, और जब आपको बीमारी नहीं है तो नकारात्मक परीक्षण की संभावना 0.97 है। P(D | TP) क्या है, यानी जब परीक्षण सकारात्मक हो तो आपको बीमारी D होने की पश्चात संभावना क्या है?",0.0495,0.078,0.635,0.97,C
"कथन 1| पारंपरिक मशीन लर्निंग परिणाम यह मानते हैं कि प्रशिक्षण और परीक्षण सेट स्वतंत्र और समान रूप से वितरित होते हैं। कथन 2| 2017 में, COCO मॉडल आमतौर पर ImageNet पर पूर्व-प्रशिक्षित होते थे।","सत्य, सत्य","झूठ, झूठ","सत्य, असत्य","झूठ, सच",A
"कथन 1| दो अलग-अलग कर्नेल K1(x, x0) और K2(x, x0) द्वारा एक ही प्रशिक्षण सेट पर प्राप्त मार्जिन के मान हमें यह नहीं बताते कि कौन सा वर्गीकरण परीक्षण सेट पर बेहतर प्रदर्शन करेगा। कथन 2| BERT का सक्रियण फलन GELU है।","सत्य, सत्य","झूठ, झूठ","सत्य, असत्य","झूठ, सच",A
मशीन लर्निंग में निम्नलिखित में से कौन सा एक क्लस्टरिंग एल्गोरिदम है?,अपेक्षा अधिकतमीकरण,कार्ट,गाउसियन नेइव बेयस,अप्रिओरी,A
"आपने अभी-अभी स्पैम वर्गीकरण के लिए एक निर्णय वृक्ष का प्रशिक्षण पूरा किया है, और यह आपके प्रशिक्षण और परीक्षण सेट दोनों पर असामान्य रूप से खराब प्रदर्शन कर रहा है। आप जानते हैं कि आपके कार्यान्वयन में कोई बग नहीं है, तो समस्या का कारण क्या हो सकता है?",आपके निर्णय वृक्ष बहुत उथले हैं।,आपको सीखने की दर बढ़ानी होगी।,आप ओवरफिटिंग कर रहे हैं।,उपरोक्त में से कोई नहीं।,A
के-फोल्ड क्रॉस-वैलिडेशन है,K के अनुरूप रैखिक,K के वर्ग में,घन मीटर में K,K के घातीय,A
"कथन 1| औद्योगिक स्तर के न्यूरल नेटवर्क आमतौर पर GPUs पर नहीं, बल्कि CPUs पर प्रशिक्षित किए जाते हैं। कथन 2| ResNet-50 मॉडल में 1 अरब से अधिक पैरामीटर हैं।","सत्य, सत्य","झूठ, झूठ","सत्य, असत्य","झूठ, सच",B
"दो बूलियन यादृच्छिक चर, A और B दिए गए हैं, जहाँ P(A) = 1/2, P(B) = 1/3, और P(A | ¬B) = 1/4 है, तो P(A | B) क्या है?",१/६,१/४,३/४,१,D
एआई द्वारा उत्पन्न अस्तित्वगत जोखिमों को सबसे अधिक किन प्रोफेसरों से जोड़ा जाता है?,नांडो डी फ्रीटास,यान लेकुन,स्टुअर्ट रसेल,जितेंद्र मलिक,C
कथन 1| लॉजिस्टिक प्रतिगमन मॉडल की संभावना को अधिकतम करने से कई स्थानीय इष्टतम परिणाम प्राप्त होते हैं। कथन 2| यदि डेटा का वितरण ज्ञात है तो कोई भी वर्गीकरण नैव बेयस वर्गीकरण से बेहतर प्रदर्शन नहीं कर सकता।,"सत्य, सत्य","झूठ, झूठ","सत्य, असत्य","झूठ, सच",B
"कर्नेल रिग्रेशन के लिए, इनमें से कौन सी संरचनात्मक मान्यता वह है जो अंडरफिटिंग और ओवरफिटिंग के बीच संतुलन को सबसे अधिक प्रभावित करती है:",चाहे कर्नेल फ़ंक्शन गाउसियन हो या त्रिकोणीय या बॉक्स के आकार का हो,चाहे हम यूक्लिडियन बनाम L1 बनाम L∞ मेट्रिक्स का उपयोग करें,कर्नेल की चौड़ाई,कर्नेल फ़ंक्शन की अधिकतम ऊंचाई,C
"कथन 1| SVM सीखने का एल्गोरिथम अपने ऑब्जेक्ट फंक्शन के संबंध में वैश्विक रूप से इष्टतम परिकल्पना को खोजने की गारंटी देता है।

कथन 2| एक रेडियल बेसिस कर्नेल फंक्शन के माध्यम से फीचर स्पेस Q में मैप किए जाने के बाद, एक पर्सेप्ट्रॉन अपने मूल स्पेस की तुलना में बेहतर वर्गीकरण प्रदर्शन प्राप्त कर सकता है (हालांकि हम इसकी गारंटी नहीं दे सकते)।","सत्य, सत्य","झूठ, झूठ","सत्य, असत्य","झूठ, सच",A
"गाउसियन बेयस वर्गीकरण के लिए, इनमें से कौन सी संरचनात्मक मान्यता वह है जो अंडरफिटिंग और ओवरफिटिंग के बीच संतुलन को सबसे अधिक प्रभावित करती है:",चाहे हम क्लास केंद्रों को अधिकतम संभावना या ग्रेडिएंट डिसेंट द्वारा सीखें,चाहे हम पूर्ण वर्ग सहप्रसरण मैट्रिक्स या विकर्ण वर्ग सहप्रसरण मैट्रिक्स मान लें,चाहे हमारे पास समान वर्ग प्रायिकताएँ हों या डेटा से अनुमानित प्रायिकताएँ हों।,चाहे हम कक्षाओं को अलग-अलग माध्य वेक्टर रखने की अनुमति दें या हम उन्हें एक ही माध्य वेक्टर साझा करने के लिए मजबूर करें,B
कथन 1| अतिफिटिंग होने की संभावना तब अधिक होती है जब प्रशिक्षण डेटा का सेट छोटा होता है। कथन 2| अतिफिटिंग होने की संभावना तब अधिक होती है जब परिकल्पना स्थान छोटा होता है।,"सत्य, सत्य","झूठ, झूठ","सत्य, असत्य","झूठ, सच",D
"वक्तव्य 1| ईएम के अलावा, गाउसियन मिश्रण मॉडल पर अनुमान या सीखने के लिए ग्रेडिएंट डिसेंट का उपयोग किया जा सकता है। वक्तव्य 2 | विशेषताओं की एक निश्चित संख्या मानते हुए, एक गाउसियन-आधारित बेज़ इष्टतम वर्गीकरण को डेटासेट में रिकॉर्ड की संख्या के रैखिक समय में सीखा जा सकता है।","सत्य, सत्य","झूठ, झूठ","सत्य, असत्य","झूठ, सच",A
"कथन 1| एक बेसियन नेटवर्क में, जंक्शन ट्री एल्गोरिथम के अनुमान परिणाम वेरिएबल एलिमिनेशन के अनुमान परिणामों के समान होते हैं। कथन 2| यदि दो यादृच्छिक चर X और Y किसी अन्य यादृच्छिक चर Z के दिए जाने पर सशर्त स्वतंत्र हैं, तो संबंधित बेसियन नेटवर्क में, X और Y के लिए नोड्स Z के दिए जाने पर d-पृथक होते हैं।","सत्य, सत्य","झूठ, झूठ","सत्य, असत्य","झूठ, सच",C
"हृदय रोग से पीड़ित रोगियों के चिकित्सा रिकॉर्ड के एक बड़े डेटासेट को देखते हुए, यह जानने की कोशिश करें कि क्या ऐसे रोगियों के अलग-अलग समूह हो सकते हैं जिनके लिए हम अलग-अलग उपचार तैयार कर सकते हैं। यह किस प्रकार की सीखने की समस्या है?",पर्यवेक्षित अधिगम,असुपरवाइज्ड लर्निंग,(ए) और (बी) दोनों,न तो (a) और न ही (b),B
PCA में आप SVD के समान प्रक्षेपण प्राप्त करने के लिए क्या करेंगे?,डेटा को शून्य माध्य में परिवर्तित करें,डेटा को शून्य माध्यिका में परिवर्तित करें,संभव नहीं,इनमें से कोई नहीं,A
"कथन 1| 1-निकटतम पड़ोसी वर्गीकरण का प्रशिक्षण त्रुटि 0 है।

कथन 2| जैसे-जैसे डेटा बिंदुओं की संख्या अनंत तक बढ़ती है, सभी संभावित प्राथमिकताओं के लिए MAP अनुमान MLE अनुमान के करीब पहुंचता जाता है। दूसरे शब्दों में, पर्याप्त डेटा होने पर, प्राथमिकता का चुनाव अप्रासंगिक हो जाता है।","सत्य, सत्य","झूठ, झूठ","सत्य, असत्य","झूठ, सच",C
"जब नियमितीकरण के साथ न्यूनतम-वर्ग प्रतिगमन किया जाता है (यह मानते हुए कि अनुकूलन सटीक रूप से किया जा सकता है), नियमितीकरण पैरामीटर λ का मान बढ़ाने से परीक्षण त्रुटि।",कभी भी प्रशिक्षण त्रुटि को कम नहीं करेगा।,कभी भी प्रशिक्षण त्रुटि को नहीं बढ़ाएगा।,कभी भी परीक्षण त्रुटि को कम नहीं करेगा।,कभी नहीं बढ़ेगा,A
निम्नलिखित में से कौन सा विकल्प सबसे अच्छी तरह से वर्णन करता है कि भेदभावपूर्ण दृष्टिकोण किस चीज को मॉडल करने का प्रयास करते हैं? (w मॉडल में पैरामीटर हैं),"p(y|x, w)","p(y, x)","p(w|x, w)",उपरोक्त में से कोई नहीं,A
कथन 1| कन्वोल्यूशन न्यूरल नेटवर्क के लिए CIFAR-10 वर्गीकरण प्रदर्शन 95% से अधिक हो सकता है। कथन 2| न्यूरल नेटवर्क के समुच्चय वर्गीकरण सटीकता में सुधार नहीं करते हैं क्योंकि उनके द्वारा सीखे गए प्रतिनिधित्व अत्यधिक सहसंबंधित होते हैं।,"सत्य, सत्य","झूठ, झूठ","सत्य, असत्य","झूठ, सच",C
निम्नलिखित में से किन बिंदुओं पर बेसियन और फ्रीक्वेंटिस्ट असहमत होंगे?,गैर-गॉसीय शोर मॉडल का प्रायिकता आधारित प्रतिगमन में उपयोग।,प्रतिगमन के लिए संभाव्यता मॉडलिंग का उपयोग।,प्रायिकता मॉडल में पैरामीटर्स पर पूर्व वितरणों का उपयोग।,गाउसियन डिस्क्रिमिनेंट एनालिसिस में क्लास प्रायर्स का उपयोग।,C
"कथन 1| BLEU मेट्रिक सटीकता का उपयोग करता है, जबकि ROGUE मेट्रिक रिकॉल का उपयोग करता है। कथन 2| अंग्रेजी वाक्यों को मॉडल करने के लिए छिपे हुए मार्कोव मॉडल का अक्सर उपयोग किया जाता था।","सत्य, सत्य","झूठ, झूठ","सत्य, असत्य","झूठ, सच",A
कथन 1| ImageNet में विभिन्न रिज़ॉल्यूशन की छवियाँ हैं। कथन 2| Caltech-101 में ImageNet से अधिक छवियाँ हैं।,"सत्य, सत्य","झूठ, झूठ","सत्य, असत्य","झूठ, सच",C
निम्नलिखित में से कौन सा विशेषता चयन करने के लिए अधिक उपयुक्त है?,रिज,लस्सो,(ए) और (बी) दोनों,न तो (a) और न ही (b),B
मान लीजिए कि आपको एक EM एल्गोरिथम दिया गया है जो गुप्त चरों वाले मॉडल के लिए अधिकतम संभावना अनुमान प्राप्त करता है। आपसे इस एल्गोरिथम को संशोधित करने के लिए कहा जाता है ताकि यह MAP अनुमान प्राप्त कर सके। आपको किस चरण या चरणों को संशोधित करने की आवश्यकता है?,अपेक्षा,अधिकतमीकरण,कोई संशोधन आवश्यक नहीं,दोनों,B
"गाउसियन बेयस वर्गीकरण के लिए, इनमें से कौन सी संरचनात्मक मान्यता वह है जो अंडरफिटिंग और ओवरफिटिंग के बीच संतुलन को सबसे अधिक प्रभावित करती है:",चाहे हम क्लास केंद्रों को अधिकतम संभावना या ग्रेडिएंट डिसेंट द्वारा सीखें,चाहे हम पूर्ण वर्ग सहप्रसरण मैट्रिक्स या विकर्ण वर्ग सहप्रसरण मैट्रिक्स मान लें,चाहे हमारे पास समान वर्ग प्रायिकताएँ हों या डेटा से अनुमानित प्रायिकताएँ हों,चाहे हम कक्षाओं को अलग-अलग माध्य वेक्टर रखने की अनुमति दें या हम उन्हें एक ही माध्य वेक्टर साझा करने के लिए मजबूर करें,B
"कथन 1| किन्हीं दो चरों x और y के लिए जिनका संयुक्त वितरण p(x, y) है, हमारे पास हमेशा H[x, y] ≥ H[x] + H[y] होता है जहाँ H एन्ट्रॉपी फलन है। कथन 2| कुछ निर्देशित ग्राफ़ों के लिए, नैतिकीकरण ग्राफ़ में मौजूद किनारों की संख्या को कम कर देता है।","सत्य, सत्य","झूठ, झूठ","सत्य, असत्य","झूठ, सच",B
निम्नलिखित में से कौन सा पर्यवेक्षित अधिगम (सुपरवाइज्ड लर्निंग) नहीं है?,पीसीए,निर्णय वृक्ष,रैखिक प्रतिगमन,नाइव बेसियन,A
कथन 1| एक तंत्रिका नेटवर्क का अभिसरण सीखने की दर पर निर्भर करता है। कथन 2| ड्रॉपआउट यादृच्छिक रूप से चुने गए सक्रियण मानों को शून्य से गुणा करता है।,"सत्य, सत्य","झूठ, झूठ","सत्य, असत्य","झूठ, सच",A
"निम्नलिखित में से कौन सा बूलियन यादृच्छिक चर A, B और C के लिए P(A, B, C) के बराबर है, जहां किसी भी चर के बीच कोई स्वतंत्रता या सशर्त स्वतंत्रता की मान्यता नहीं है?",P(A | B) * P(B | C) * P(C | A),"P(C | A, B) * P(A) * P(B)","P(A, B | C) * P(C)","पी(ए | बी, सी) * पी(बी | ए, सी) * पी(सी | ए, बी)",C
निम्नलिखित में से कौन सा कार्य क्लस्टरिंग का उपयोग करके सबसे अच्छी तरह से हल किया जा सकता है।,विभिन्न संकेतों के आधार पर वर्षा की मात्रा की भविष्यवाणी करना,धोखाधड़ी वाले क्रेडिट कार्ड लेनदेन का पता लगाना,एक रोबोट को भूलभुलैया हल करने के लिए प्रशिक्षित करना,उपरोक्त सभी,B
"रैखिक प्रतिगमन में नियमितीकरण दंड लागू करने के बाद, आप पाते हैं कि w के कुछ गुणांक शून्य हो गए हैं। निम्नलिखित में से कौन से दंड का उपयोग किया गया हो सकता है?",L0 नॉर्म,L1 मानक,L2 मानक,या तो (ए) या (बी),D
"A और B दो घटनाएँ हैं। यदि P(A, B) घटता है जबकि P(A) बढ़ता है, तो निम्नलिखित में से कौन सा सत्य है?",P(A|B) घटता है,P(B|A) घटता है,P(B) घटता है,उपरोक्त सभी,B
"कथन 1| जब एक निश्चित अवलोकन समूह के लिए एक HMM सीखते हैं, मान लें कि हम वास्तविक छिपे हुए अवस्थाओं की संख्या नहीं जानते (जो अक्सर मामला होता है), हम हमेशा अधिक छिपी हुई अवस्थाओं की अनुमति देकर प्रशिक्षण डेटा की संभावना को बढ़ा सकते हैं। कथन 2| सहयोगात्मक फ़िल्टरिंग अक्सर उपयोगकर्ताओं की फिल्म पसंद को मॉडल करने के लिए एक उपयोगी मॉडल होता है।","सत्य, सत्य","झूठ, झूठ","सत्य, असत्य","झूठ, सच",A
"आप एक सरल अनुमान कार्य के लिए एक रैखिक प्रतिगमन मॉडल को प्रशिक्षित कर रहे हैं, और आप देखते हैं कि मॉडल डेटा पर अतिफिट हो रहा है। आप वजनों को दंडित करने के लिए $\ell_2$ नियमितीकरण जोड़ने का निर्णय लेते हैं। जैसे-जैसे आप $\ell_2$ नियमितीकरण गुणांक को बढ़ाते हैं, मॉडल के पूर्वाग्रह और प्रसरण का क्या होगा?",पक्षपात वृद्धि; प्रसरण वृद्धि,पक्षपात बढ़ता है; विचरण घटता है,पक्षपात कम होना ; विचरण बढ़ना,पक्षपात में कमी ; प्रसरण में कमी,B
"कौन सा PyTorch 1.8 कमांड $10\times 5$ गाउसीय मैट्रिक्स उत्पन्न करता है जिसमें प्रत्येक प्रविष्टि $\mathcal{N}(\mu=5,\sigma^2=16)$ से स्वतंत्र और समान रूप से वितरित नमूना है और एक $10\times 10$ एकसमान मैट्रिक्स जिसमें प्रत्येक प्रविष्टि $U[-1,1)$ से स्वतंत्र और समान रूप से वितरित नमूना है?","\texttt{5 + torch.randn(10,5) * 16} ; \texttt{torch.rand(10,10,low=-1,high=1)}","\texttt{5 + torch.randn(10,5) * 16} ; \texttt{(torch.rand(10,10) - 0.5) / 0.5}","\texttt{5 + torch.randn(10,5) * 4} ; \texttt{2 * torch.rand(10,10) - 1}","टॉर्च.नॉर्मल(टॉर्च.वन्स(10,5)*5,टॉर्च.वन्स(5,5)*16) ; 2 * टॉर्च.रैंड(10,10) - 1",C
"कथन 1| ReLU का ग्रेडिएंट $x<0$ के लिए शून्य होता है, और सिग्मॉइड ग्रेडिएंट $\sigma(x)(1-\sigma(x))\le \frac{1}{4}$ सभी $x$ के लिए होता है। कथन 2| सिग्मॉइड का एक सतत ग्रेडिएंट होता है और ReLU का एक असतत ग्रेडिएंट होता है।","सत्य, सत्य","झूठ, झूठ","सत्य, असत्य","झूठ, सच",A
बैच नॉर्मलाइजेशन के बारे में क्या सही है?,"बैच नॉर्मलाइज़ेशन लागू करने के बाद, लेयर की सक्रियताएँ एक मानक गाउसीय वितरण का पालन करेंगी।",अफाइन परतों का बायस पैरामीटर अनावश्यक हो जाता है यदि उसके तुरंत बाद एक बैच नॉर्मलाइजेशन परत आती है।,बैच नॉर्मलाइज़ेशन का उपयोग करते समय मानक वजन प्रारंभीकरण को बदलना आवश्यक है।,बैच नॉर्मलाइजेशन कन्वोल्यूशनल न्यूरल नेटवर्क के लिए लेयर नॉर्मलाइजेशन के समतुल्य है।,B
मान लीजिए कि हमारे पास निम्नलिखित उद्देश्य फलन है: $\argmin_{w} \frac{1}{2} \norm{Xw-y}^2_2 + \frac{1}{2}\gamma \norm{w}^2_2$ $\frac{1}{2} \norm{Xw-y}^2_2 + \frac{1}{2}\lambda \norm{w}^2_2$ का $w$ के संदर्भ में ग्रेडिएंट क्या है?,"यह एक गणितीय समीकरण है जिसे हिंदी में अनुवाद नहीं किया जा सकता। इसलिए मूल पाठ ही दिया जा रहा है:

$\nabla_w f(w) = (X^\top X + \lambda I)w - X^\top y + \lambda w$",$\nabla_w f(w) = X^\top X w - X^\top y + \lambda$,$\nabla_w f(w) = X^\top X w - X^\top y + \lambda w$,$\nabla_w f(w) = X^\top X w - X^\top y + (\lambda+1) w$,C
निम्नलिखित में से कौन सा कथन कनवोल्यूशन कर्नेल के बारे में सत्य है?,$\begin{bmatrix}1 & 0 & 0\\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix}$ के साथ एक छवि को कनवॉल्व करने से छवि में कोई परिवर्तन नहीं होगा,किसी छवि को $\begin{bmatrix}0 & 0 & 0\\ 0 & 1 & 0 \\ 0 & 0 & 0 \end{bmatrix}$ के साथ कनवॉल्व करने से छवि में कोई परिवर्तन नहीं होगा,छवि को $\begin{bmatrix}1 & 1 & 1\\ 1 & 1 & 1 \\ 1 & 1 & 1 \end{bmatrix}$ के साथ कनवॉल्व करने से छवि में कोई परिवर्तन नहीं होगा,किसी छवि को $\begin{bmatrix}0 & 0 & 0\\ 0 & 0 & 0 \\ 0 & 0 & 0 \end{bmatrix}$ के साथ कनवॉल्व करने से छवि में कोई परिवर्तन नहीं होगा,B
निम्नलिखित में से कौन सा गलत है?,"सिमेंटिक सेगमेंटेशन मॉडल प्रत्येक पिक्सेल के वर्ग की भविष्यवाणी करते हैं, जबकि मल्टीक्लास इमेज क्लासिफायर पूरी छवि के वर्ग की भविष्यवाणी करते हैं।","एक बाउंडिंग बॉक्स जिसका IoU (इंटरसेक्शन ओवर यूनियन) $96\%$ के बराबर है, संभवतः एक सच्चा सकारात्मक माना जाएगा।","जब एक अनुमानित बाउंडिंग बॉक्स दृश्य में किसी भी वस्तु से मेल नहीं खाता, तो इसे एक फॉल्स पॉजिटिव माना जाता है।","एक बाउंडिंग बॉक्स जिसका IoU (इंटरसेक्शन ओवर यूनियन) $3\%$ के बराबर है, संभवतः एक फॉल्स नेगेटिव माना जाएगा।",D
निम्नलिखित में से कौन सा गलत है?,"निम्नलिखित पूरी तरह से जुड़ा हुआ नेटवर्क बिना सक्रियण फलनों के रैखिक है: $g_3(g_2(g_1(x)))$, जहाँ $g_i(x) = W_i x$ और $W_i$ मैट्रिसेस हैं।","लीकी रेल्यू $\max\{0.01x,x\}$ उत्तल है।",रेलू का एक संयोजन जैसे कि $ReLU(x) - ReLU(x-1)$ उत्तल होता है।,हानि $\log \sigma(x)= -\log(1+e^{-x})$ अवतल है,C
"हम आवास की कीमतों की भविष्यवाणी करने के लिए दो छिपी हुई परतों वाले पूरी तरह से जुड़े नेटवर्क को प्रशिक्षित कर रहे हैं। इनपुट $100$-आयामी हैं, और उनमें कई विशेषताएं हैं जैसे वर्ग फुट की संख्या, मध्यम पारिवारिक आय, आदि। पहली छिपी हुई परत में $1000$ सक्रियण हैं। दूसरी छिपी हुई परत में $10$ सक्रियण हैं। आउटपुट एक अदिश है जो घर की कीमत का प्रतिनिधित्व करता है। एफाइन रूपांतरणों के साथ एक सादे नेटवर्क और बिना बैच सामान्यीकरण और सक्रियण फ़ंक्शन में कोई सीखने योग्य पैरामीटर नहीं मानते हुए, इस नेटवर्क में कितने पैरामीटर हैं?",१११०२१,११००१०,१११११०,110011,A
कथन 1| सिग्मॉइड $\sigma(x)=(1+e^{-x})^{-1}$ का $x$ के संदर्भ में व्युत्पन्न $\text{Var}(B)$ के बराबर होता है जहाँ $B\sim \text{Bern}(\sigma(x))$ एक बर्नूली यादृच्छिक चर है। कथन 2| न्यूरल नेटवर्क की प्रत्येक परत में बायस पैरामीटर को 0 पर सेट करने से बायस-वेरिएंस ट्रेड-ऑफ इस प्रकार बदलता है कि मॉडल का वेरिएंस बढ़ जाता है और मॉडल का बायस कम हो जाता है।,"सत्य, सत्य","झूठ, झूठ","सत्य, असत्य","झूठ, सच",C
