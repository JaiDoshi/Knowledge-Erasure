"Aussage 1| Der Schätzer der linearen Regression hat die kleinste Varianz unter allen unverzerrten Schätzern. Aussage 2| Die Koeffizienten α, die den von AdaBoost zusammengestellten Klassifikatoren zugewiesen werden, sind immer nicht-negativ.","Wahr, Wahr","Falsch, Falsch","Wahr, Falsch","Falsch, Wahr",D
"Aussage 1| RoBERTa wird auf einem Korpus vortrainiert, der ungefähr 10-mal größer ist als der Korpus, auf dem BERT vortrainiert wurde. Aussage 2| ResNeXts verwendeten 2018 üblicherweise tanh-Aktivierungsfunktionen.","Wahr, Wahr","Falsch, Falsch","Wahr, Falsch","Falsch, Wahr",C
"Statement 1| Support-Vektor-Maschinen geben, ähnlich wie logistische Regressionsmodelle, eine Wahrscheinlichkeitsverteilung über die möglichen Bezeichnungen für ein gegebenes Eingabebeispiel.

Statement 2| Wir würden erwarten, dass die Support-Vektoren im Allgemeinen gleich bleiben, wenn wir von einem linearen Kernel zu Kernels höherer polynomialer Ordnung übergehen.","Wahr, Wahr","Falsch, Falsch","Wahr, Falsch","Falsch, Wahr",B
"Ein maschinelles Lernproblem umfasst vier Attribute plus eine Klasse. Die Attribute haben jeweils 3, 2, 2 und 2 mögliche Werte. Die Klasse hat 3 mögliche Werte. Wie viele maximal mögliche verschiedene Beispiele gibt es?",12,24,48,72,D
"Stand 2020, welche Architektur ist am besten für die Klassifizierung hochauflösender Bilder geeignet?",Faltungsnetzwerke,Graphennetzwerke,vollständig verbundene Netzwerke,RBF-Netzwerke,A
"Statement 1| Die Log-Likelihood der Daten wird durch aufeinanderfolgende Iterationen des Expectation-Maximization-Algorithmus immer zunehmen. Statement 2| Ein Nachteil von Q-Learning ist, dass es nur verwendet werden kann, wenn der Lernende vorheriges Wissen darüber hat, wie seine Aktionen seine Umgebung beeinflussen.","Wahr, Wahr","Falsch, Falsch","Wahr, Falsch","Falsch, Wahr",B
"Nehmen wir an, dass wir den Gradienten unserer Kostenfunktion berechnet und in einem Vektor g gespeichert haben. Was sind die Kosten für eine Gradientenabstiegs-Aktualisierung, wenn der Gradient gegeben ist?",O(D),O(N),O(ND),O(ND^2),A
"Aussage 1| Für eine stetige Zufallsvariable x und ihre Wahrscheinlichkeitsverteilungsfunktion p(x) gilt, dass 0 ≤ p(x) ≤ 1 für alle x. Aussage 2| Ein Entscheidungsbaum wird durch Minimierung des Informationsgewinns gelernt.","Wahr, Wahr","Falsch, Falsch","Wahr, Falsch","Falsch, Wahr",B
Betrachten Sie das unten angegebene Bayessche Netzwerk. Wie viele unabhängige Parameter werden für dieses Bayessche Netzwerk H -> U <- P <- W benötigt?,2,4,8,16,C
"Wenn die Anzahl der Trainingsbeispiele gegen unendlich geht, wird Ihr auf diesen Daten trainiertes Modell:",Geringere Varianz,Höhere Varianz,Gleiche Varianz,Nichts davon,A
Statement 1| Die Menge aller Rechtecke in der 2D-Ebene (einschließlich nicht achsenparalleler Rechtecke) kann eine Menge von 5 Punkten zerschmettern. Statement 2| Die VC-Dimension des k-Nächste-Nachbarn-Klassifikators ist bei k = 1 unendlich.,"Wahr, Wahr","Falsch, Falsch","Wahr, Falsch","Falsch, Wahr",A
"_ bezieht sich auf ein Modell, das weder die Trainingsdaten modellieren noch auf neue Daten verallgemeinern kann.",gut sitzend,Überanpassung,Unteranpassung,alles oben Genannte,C
"Statement 1| Der F1-Score kann besonders nützlich für Datensätze mit hoher Klassenungleichheit sein.

Statement 2| Die Fläche unter der ROC-Kurve ist eine der wichtigsten Metriken zur Bewertung von Anomaliedetektoren.","Wahr, Wahr","Falsch, Falsch","Wahr, Falsch","Falsch, Wahr",A
"Aussage 1| Der Backpropagation-Algorithmus lernt ein global optimales neuronales Netzwerk mit versteckten Schichten. Aussage 2| Die VC-Dimension einer Linie sollte höchstens 2 sein, da ich mindestens einen Fall von 3 Punkten finden kann, die von keiner Linie zerschmettert werden können.","Wahr, Wahr","Falsch, Falsch","Wahr, Falsch","Falsch, Wahr",B
"Hohe Entropie bedeutet, dass die Partitionen in der Klassifikation sind",rein,nicht rein,nützlich,nutzlos,B
"Aussage 1| Im ursprünglichen ResNet-Paper wird Layer-Normalisierung verwendet, nicht Batch-Normalisierung. Aussage 2| DCGANs verwenden Selbstaufmerksamkeit, um das Training zu stabilisieren.","Wahr, Wahr","Falsch, Falsch","Wahr, Falsch","Falsch, Wahr",B
"Bei der Erstellung eines linearen Regressionsmodells für einen bestimmten Datensatz beobachten Sie, dass der Koeffizient eines der Merkmale einen relativ hohen negativen Wert aufweist. Dies deutet darauf hin, dass",Diese Funktion hat einen starken Einfluss auf das Modell (sollte beibehalten werden),Diese Funktion hat keinen starken Einfluss auf das Modell (sollte ignoriert werden),"Es ist nicht möglich, die Bedeutung dieses Merkmals ohne zusätzliche Informationen zu kommentieren.",Nichts kann bestimmt werden.,C
"Für ein neuronales Netzwerk, welche dieser strukturellen Annahmen ist diejenige, die den Kompromiss zwischen Underfitting (d.h. einem Modell mit hohem Bias) und Overfitting (d.h. einem Modell mit hoher Varianz) am stärksten beeinflusst:",Die Anzahl der versteckten Knoten,Die Lernrate,Die anfängliche Auswahl der Gewichte,Die Verwendung eines Eingangs mit konstantem Einheitsterm,A
"Für die polynomiale Regression ist welche dieser strukturellen Annahmen diejenige, die den Kompromiss zwischen Underfitting und Overfitting am stärksten beeinflusst:",Der Polynomgrad,Ob wir die Gewichte durch Matrixinversion oder Gradientenabstieg lernen,Die angenommene Varianz des Gauß'schen Rauschens,Die Verwendung eines Eingangs mit konstantem Einheitsterm,A
Statement 1| Stand 2020 erreichen einige Modelle eine Genauigkeit von über 98% auf CIFAR-10. Statement 2| Die ursprünglichen ResNets wurden nicht mit dem Adam-Optimierer optimiert.,"Wahr, Wahr","Falsch, Falsch","Wahr, Falsch","Falsch, Wahr",A
Der K-Means-Algorithmus:,"Erfordert, dass die Dimension des Merkmalsraums nicht größer ist als die Anzahl der Stichproben","Hat den kleinsten Wert der Zielfunktion, wenn K = 1",Minimiert die Varianz innerhalb der Klassen für eine gegebene Anzahl von Clustern,"Konvergiert zum globalen Optimum dann und nur dann, wenn die anfänglichen Mittelwerte als einige der Proben selbst gewählt werden",C
Aussage 1| VGGNets haben Faltungskerne mit geringerer Breite und Höhe als die Kerne der ersten Schicht von AlexNet. Aussage 2| Datenabhängige Gewichtsinitialisierungsverfahren wurden vor der Batch-Normalisierung eingeführt.,"Wahr, Wahr","Falsch, Falsch","Wahr, Falsch","Falsch, Wahr",A
"Was ist der Rang der folgenden Matrix? A = [[1, 1, 1], [1, 1, 1], [1, 1, 1]]",0,1,2,3,B
"Aussage 1| Dichteschätzung (zum Beispiel mit dem Kerndichteschätzer) kann zur Durchführung von Klassifikationen verwendet werden. Aussage 2| Die Entsprechung zwischen logistischer Regression und Gaussian Naive Bayes (mit identischen Klassenkovarianzen) bedeutet, dass es eine eins-zu-eins Entsprechung zwischen den Parametern der beiden Klassifikatoren gibt.","Wahr, Wahr","Falsch, Falsch","Wahr, Falsch","Falsch, Wahr",C
"Angenommen, wir möchten ein Clustering auf räumlichen Daten durchführen, wie zum Beispiel den geometrischen Standorten von Häusern. Wir möchten Cluster in vielen verschiedenen Größen und Formen erzeugen. Welche der folgenden Methoden ist am besten geeignet?",Entscheidungsbäume,Dichtebasierte Clusteranalyse,Modellbasiertes Clustering,K-Means-Clustering,B
"Aussage 1| Bei AdaBoost steigen die Gewichte der falsch klassifizierten Beispiele um den gleichen multiplikativen Faktor.

Aussage 2| Bei AdaBoost tendiert der gewichtete Trainingsfehler e_t des t-ten schwachen Klassifikators auf Trainingsdaten mit Gewichten D_t dazu, als Funktion von t zuzunehmen.","Wahr, Wahr","Falsch, Falsch","Wahr, Falsch","Falsch, Wahr",A
"MLE-Schätzungen sind oft unerwünscht, weil",sie sind voreingenommen,sie haben eine hohe Varianz,sie sind keine konsistenten Schätzer,Nichts davon,B
"Die rechnerische Komplexität des Gradientenabstiegsverfahrens ist,",linear in D,linear in N,Polynom in D,abhängig von der Anzahl der Iterationen,C
Die Mittelwertbildung der Ausgabe mehrerer Entscheidungsbäume hilft _.,Voreingenommenheit erhöhen,Voreingenommenheit reduzieren,Varianz erhöhen,Varianz verringern,D
"Das durch Anwendung der linearen Regression auf die identifizierte Teilmenge von Merkmalen erhaltene Modell kann sich von dem Modell unterscheiden, das am Ende des Prozesses zur Identifizierung der Teilmenge während",Beste-Teilmenge-Auswahl,Vorwärtsgerichtete schrittweise Selektion,Vorwärtsgerichtete stufenweise Auswahl,Alles oben Genannte,C
Neuronale Netze,Optimiere eine konvexe Zielfunktion,Kann nur mit stochastischem Gradientenabstieg trainiert werden,Kann eine Mischung verschiedener Aktivierungsfunktionen verwenden,Nichts davon,C
"Angenommen, die Inzidenz einer Krankheit D beträgt etwa 5 Fälle pro 100 Personen (d.h. P(D) = 0,05). Sei D eine boolesche Zufallsvariable, die bedeutet, dass ein Patient ""Krankheit D hat"", und sei TP eine boolesche Zufallsvariable, die für ""Test positiv"" steht. Es ist bekannt, dass Tests für Krankheit D sehr genau sind, in dem Sinne, dass die Wahrscheinlichkeit, positiv zu testen, wenn man die Krankheit hat, 0,99 beträgt, und die Wahrscheinlichkeit, negativ zu testen, wenn man die Krankheit nicht hat, 0,97 beträgt. Was ist P(TP), die A-priori-Wahrscheinlichkeit, positiv zu testen?","0,0368","0,473","0,078",Nichts davon,C
Statement 1| Nach der Abbildung in den Merkmalsraum Q durch eine radiale Basiskernfunktion kann 1-NN mit ungewichteter euklidischer Distanz möglicherweise eine bessere Klassifizierungsleistung erzielen als im ursprünglichen Raum (obwohl wir dies nicht garantieren können). Statement 2| Die VC-Dimension eines Perceptrons ist kleiner als die VC-Dimension eines einfachen linearen SVM.,"Wahr, Wahr","Falsch, Falsch","Wahr, Falsch","Falsch, Wahr",B
Der Nachteil der Rastersuche ist,Es kann nicht auf nicht-differenzierbare Funktionen angewendet werden.,Es kann nicht auf nicht-stetige Funktionen angewendet werden.,Es ist schwer umzusetzen.,Es läuft einigermaßen langsam für multiple lineare Regression.,D
Das Vorhersagen der Regenmenge in einer Region basierend auf verschiedenen Hinweisen ist ein ______ Problem.,Überwachtes Lernen,Unüberwachtes Lernen,Clustering,Nichts davon,A
Welcher der folgenden Sätze ist FALSCH in Bezug auf Regression?,Es bringt Eingaben mit Ausgaben in Verbindung.,Es wird zur Vorhersage verwendet.,Es kann zur Interpretation verwendet werden.,Es entdeckt kausale Zusammenhänge,D
Welcher der folgenden Gründe ist der Hauptgrund für das Beschneiden eines Entscheidungsbaums?,Um Rechenzeit während des Testens zu sparen,Um Platz für die Speicherung des Entscheidungsbaums zu sparen,Um den Trainingssatzfehler zu verringern,Um ein Überanpassen des Trainingsdatensatzes zu vermeiden,D
"Aussage 1| Der Kerndichteschätzer entspricht der Durchführung einer Kernregression mit dem Wert Yi = 1/n an jedem Punkt Xi im ursprünglichen Datensatz. Aussage 2| Die Tiefe eines gelernten Entscheidungsbaums kann größer sein als die Anzahl der Trainingsbeispiele, die zur Erstellung des Baums verwendet wurden.","Wahr, Wahr","Falsch, Falsch","Wahr, Falsch","Falsch, Wahr",B
"Angenommen, Ihr Modell überanpasst sich. Welche der folgenden Möglichkeiten ist KEINE gültige Methode, um zu versuchen, die Überanpassung zu reduzieren?",Erhöhen Sie die Menge der Trainingsdaten.,"Verbessern Sie den Optimierungsalgorithmus, der zur Fehlerminimierung verwendet wird.",Verringern Sie die Modellkomplexität.,Reduzieren Sie das Rauschen in den Trainingsdaten.,B
"Statement 1| Die Softmax-Funktion wird häufig in der multiklassen logistischen Regression verwendet.

Statement 2| Die Temperatur einer nicht-uniformen Softmax-Verteilung beeinflusst ihre Entropie.","Wahr, Wahr","Falsch, Falsch","Wahr, Falsch","Falsch, Wahr",A
Welche der folgenden Aussagen ist/sind in Bezug auf eine SVM zutreffend?,Für zweidimensionale Datenpunkte wird die von einem linearen SVM erlernte trennende Hyperebene eine gerade Linie sein.,In der Theorie kann ein SVM mit Gauß-Kernel keine komplexe Trennebene modellieren.,Für jede in einem SVM verwendete Kernelfunktion kann man eine äquivalente Basisexpansion in geschlossener Form erhalten.,Overfitting in einem SVM ist keine Funktion der Anzahl der Support-Vektoren.,A
"Welche der folgenden ist die gemeinsame Wahrscheinlichkeit von H, U, P und W, die durch das gegebene Bayessche Netzwerk H -> U <- P <- W beschrieben wird? [Hinweis: als Produkt der bedingten Wahrscheinlichkeiten]","P(H, U, P, W) = P(H) * P(W) * P(P) * P(U)","P(H, U, P, W) = P(H) * P(W) * P(P | W) * P(W | H, P)","P(H, U, P, W) = P(H) * P(W) * P(P | W) * P(U | H, P)",Nichts davon,C
"Aussage 1| Da die VC-Dimension für einen SVM mit einem radialen Basiskern unendlich ist, muss ein solcher SVM schlechter sein als ein SVM mit polynomialem Kern, der eine endliche VC-Dimension hat.

Aussage 2| Ein zweischichtiges neuronales Netzwerk mit linearen Aktivierungsfunktionen ist im Wesentlichen eine gewichtete Kombination linearer Separatoren, die auf einem gegebenen Datensatz trainiert werden; der Boosting-Algorithmus, der auf linearen Separatoren aufbaut, findet ebenfalls eine Kombination von linearen Separatoren, daher werden diese beiden Algorithmen das gleiche Ergebnis liefern.","Wahr, Wahr","Falsch, Falsch","Wahr, Falsch","Falsch, Wahr",B
"Statement 1| Der ID3-Algorithmus garantiert das Finden des optimalen Entscheidungsbaums. Statement 2| Betrachten Sie eine stetige Wahrscheinlichkeitsverteilung mit einer Dichte f(), die überall ungleich Null ist. Die Wahrscheinlichkeit eines Wertes x ist gleich f(x).","Wahr, Wahr","Falsch, Falsch","Wahr, Falsch","Falsch, Wahr",B
"Gegeben sei ein neuronales Netz mit N Eingabeknoten, ohne versteckte Schichten, einem Ausgabeknoten, mit Entropie-Verlustfunktion und Sigmoid-Aktivierungsfunktionen. Welche der folgenden Algorithmen (mit den richtigen Hyperparametern und der richtigen Initialisierung) können verwendet werden, um das globale Optimum zu finden?",Stochastischer Gradientenabstieg,Mini-Batch Gradientenabstieg,Batch-Gradientenabstieg,Alles oben Genannte,D
"Hinzufügen von mehr Basisfunktionen in einem linearen Modell, wählen Sie die wahrscheinlichste Option:",Verringert die Modellverzerrung,Verringert den Schätzungsfehler,Verringert die Varianz,Beeinflusst nicht Bias und Varianz,A
"Betrachten Sie das unten angegebene Bayessche Netzwerk. Wie viele unabhängige Parameter würden wir benötigen, wenn wir keine Annahmen über Unabhängigkeit oder bedingte Unabhängigkeit treffen würden H -> U <- P <- W?",3,4,7,15,D
Ein anderer Begriff für die Erkennung außerhalb der Verteilung ist?,Anomalieerkennung,Erkennung einer einzelnen Klasse,Robustheit bei Abweichungen zwischen Trainings- und Testdaten,Hintergrunderkennung,A
"Aussage 1| Wir lernen einen Klassifikator f durch Boosting schwacher Lerner h. Die funktionale Form der Entscheidungsgrenze von f ist die gleiche wie die von h, jedoch mit anderen Parametern. (z.B. wenn h ein linearer Klassifikator war, dann ist f auch ein linearer Klassifikator).

Aussage 2| Kreuzvalidierung kann verwendet werden, um die Anzahl der Iterationen beim Boosting auszuwählen; dieses Verfahren kann dazu beitragen, Überanpassung zu reduzieren.","Wahr, Wahr","Falsch, Falsch","Wahr, Falsch","Falsch, Wahr",D
Aussage 1| Highway-Netzwerke wurden nach ResNets eingeführt und verzichten zugunsten von Faltungen auf Max-Pooling. Aussage 2| DenseNets benötigen in der Regel mehr Speicher als ResNets.,"Wahr, Wahr","Falsch, Falsch","Wahr, Falsch","Falsch, Wahr",D
"Wenn N die Anzahl der Instanzen im Trainingsdatensatz ist, hat der nächste Nachbar eine Klassifizierungslaufzeit von",O(1),O( N ),O(log N),O( N^2 ),B
"Statement 1| Die ursprünglichen ResNets und Transformers sind vorwärtsgerichtete neuronale Netzwerke. Statement 2| Die ursprünglichen Transformers verwenden Selbstaufmerksamkeit, aber das ursprüngliche ResNet nicht.","Wahr, Wahr","Falsch, Falsch","Wahr, Falsch","Falsch, Wahr",A
"Aussage 1| RELUs sind nicht monoton, aber Sigmoide sind monoton. Aussage 2| Mit hoher Wahrscheinlichkeit konvergieren neuronale Netze, die mit Gradientenabstieg trainiert werden, zum globalen Optimum.","Wahr, Wahr","Falsch, Falsch","Wahr, Falsch","Falsch, Wahr",D
Die numerische Ausgabe eines Sigmoid-Knotens in einem neuronalen Netzwerk:,Ist unbegrenzt und umfasst alle reellen Zahlen.,Ist unbegrenzt und umfasst alle ganzen Zahlen.,Ist zwischen 0 und 1 begrenzt.,Ist zwischen -1 und 1 begrenzt.,C
"Welche der folgenden Methoden kann nur verwendet werden, wenn die Trainingsdaten linear trennbar sind?",Lineares SVM mit hartem Rand,Lineare logistische Regression,Lineare Soft-Margin-SVM,Die Zentroid-Methode.,A
Welche der folgenden sind die räumlichen Clustering-Algorithmen?,Partitionsbasierte Clusterbildung,K-Means-Clustering,Rasterbasierte Clusterbildung,Alles oben Genannte,D
"Aussage 1| Die Entscheidungsgrenzen mit maximalem Abstand, die Support-Vektor-Maschinen konstruieren, haben den geringsten Generalisierungsfehler unter allen linearen Klassifikatoren. Aussage 2| Jede Entscheidungsgrenze, die wir aus einem generativen Modell mit klassenabhängigen Gauß-Verteilungen erhalten, könnte im Prinzip mit einer SVM und einem polynomiellen Kernel vom Grad kleiner oder gleich drei reproduziert werden.","Wahr, Wahr","Falsch, Falsch","Wahr, Falsch","Falsch, Wahr",D
"Aussage 1| Die L2-Regularisierung linearer Modelle neigt dazu, Modelle spärlicher zu machen als die L1-Regularisierung. Aussage 2| Residuale Verbindungen finden sich in ResNets und Transformern.","Wahr, Wahr","Falsch, Falsch","Wahr, Falsch","Falsch, Wahr",D
"Angenommen, wir möchten P(H|E, F) berechnen und haben keine Informationen über bedingte Unabhängigkeit. Welche der folgenden Zahlenmengen sind für die Berechnung ausreichend?","P(E, F), P(H), P(E|H), P(F|H)","P(E, F), P(H), P(E, F|H)","P(H), P(E|H), P(F|H)","P(E, F), P(E|H), P(F|H)",B
"Welches der folgenden verhindert Überanpassung, wenn wir Bagging durchführen?",Die Verwendung von Stichprobenziehung mit Zurücklegen als Stichprobentechnik,Der Einsatz von schwachen Klassifikatoren,"Die Verwendung von Klassifizierungsalgorithmen, die nicht anfällig für Überanpassung sind","Die Praxis der Validierung, die bei jedem trainierten Klassifikator durchgeführt wird",B
"Aussage 1| PCA und Spektrales Clustering (wie das von Andrew Ng) führen eine Eigenzerlegung auf zwei verschiedenen Matrizen durch. Die Größe dieser beiden Matrizen ist jedoch gleich. Aussage 2| Da Klassifikation ein Sonderfall der Regression ist, ist die logistische Regression ein Sonderfall der linearen Regression.","Wahr, Wahr","Falsch, Falsch","Wahr, Falsch","Falsch, Wahr",B
"Aussage 1| Das Stanford Sentiment Treebank enthielt Filmkritiken, keine Buchrezensionen. Aussage 2| Das Penn Treebank wurde für die Sprachmodellierung verwendet.","Wahr, Wahr","Falsch, Falsch","Wahr, Falsch","Falsch, Wahr",A
"Was ist die Dimensionalität des Nullraums der folgenden Matrix? A = [[3, 2, −9], [−6, −4, 18], [12, 8, −36]]",0,1,2,3,C
Was sind Stützvektoren?,"Die Beispiele, die am weitesten von der Entscheidungsgrenze entfernt sind.","Die einzigen Beispiele, die notwendig sind, um f(x) in einer SVM zu berechnen.",Der Datenschwerpunkt.,"Alle Beispiele, die in einem SVM ein von Null verschiedenes Gewicht αk haben.",B
"Statement 1| Word2Vec-Parameter wurden nicht mit einer Restricted Boltzman Machine initialisiert.

Statement 2| Die tanh-Funktion ist eine nichtlineare Aktivierungsfunktion.","Wahr, Wahr","Falsch, Falsch","Wahr, Falsch","Falsch, Wahr",A
"Wenn Ihr Trainingsverlust mit der Anzahl der Epochen zunimmt, welches der folgenden Probleme könnte ein mögliches Problem im Lernprozess sein?",Die Regularisierung ist zu niedrig und das Modell überanpasst sich,Die Regularisierung ist zu hoch und das Modell unterpasst,Schrittgröße ist zu groß,Schrittgröße ist zu klein,C
"Angenommen, die Inzidenz einer Krankheit D beträgt etwa 5 Fälle pro 100 Personen (d.h. P(D) = 0,05). Sei D eine boolesche Zufallsvariable, die bedeutet, dass ein Patient ""Krankheit D hat"", und sei TP eine boolesche Zufallsvariable, die für ""Test positiv"" steht. Tests für Krankheit D sind bekanntermaßen sehr genau in dem Sinne, dass die Wahrscheinlichkeit, positiv zu testen, wenn man die Krankheit hat, 0,99 beträgt, und die Wahrscheinlichkeit, negativ zu testen, wenn man die Krankheit nicht hat, 0,97 beträgt. Wie hoch ist P(D | TP), die posteriore Wahrscheinlichkeit, dass Sie Krankheit D haben, wenn der Test positiv ist?","0,0495","0,078","0,635","0,97",C
"Statement 1| Traditionelle Ergebnisse des maschinellen Lernens gehen davon aus, dass die Trainings- und Testsets unabhängig und identisch verteilt sind. Statement 2| Im Jahr 2017 wurden COCO-Modelle in der Regel auf ImageNet vortrainiert.","Wahr, Wahr","Falsch, Falsch","Wahr, Falsch","Falsch, Wahr",A
"Aussage 1| Die Werte der Margen, die durch zwei verschiedene Kerne K1(x, x0) und K2(x, x0) auf demselben Trainingssatz erhalten werden, sagen uns nicht, welcher Klassifikator auf dem Testsatz besser abschneiden wird. Aussage 2| Die Aktivierungsfunktion von BERT ist die GELU.","Wahr, Wahr","Falsch, Falsch","Wahr, Falsch","Falsch, Wahr",A
Welches der folgenden ist ein Clustering-Algorithmus im maschinellen Lernen?,Erwartungs-Maximierung,WARENKORB,Gaußscher Naiver Bayes,Apriori,A
"Sie haben gerade das Training eines Entscheidungsbaums für die Spam-Klassifizierung abgeschlossen, und er erzielt ungewöhnlich schlechte Leistungen sowohl auf Ihren Trainings- als auch auf Ihren Testdatensätzen. Sie wissen, dass Ihre Implementierung keine Fehler enthält, also was könnte das Problem verursachen?",Deine Entscheidungsbäume sind zu flach.,Sie müssen die Lernrate erhöhen.,Du überfittest.,Nichts davon.,A
K-Fold-Kreuzvalidierung ist,linear in K,quadratisch in K,kubisch in K,exponentiell in K,A
"Aussage 1| Neuronale Netzwerke im industriellen Maßstab werden normalerweise auf CPUs trainiert, nicht auf GPUs. Aussage 2| Das ResNet-50-Modell hat über 1 Milliarde Parameter.","Wahr, Wahr","Falsch, Falsch","Wahr, Falsch","Falsch, Wahr",B
"Gegeben sind zwei boolesche Zufallsvariablen, A und B, wobei P(A) = 1/2, P(B) = 1/3 und P(A | ¬B) = 1/4. Was ist P(A | B)?",1/6,1/4,3/4,1,D
Existenzielle Risiken durch KI werden am häufigsten mit welchem der folgenden Professoren in Verbindung gebracht?,Nando de Frietas,Yann LeCun,Stuart Russell,Jitendra Malik,C
"Aussage 1| Die Maximierung der Wahrscheinlichkeit des logistischen Regressionsmodells ergibt mehrere lokale Optima. Aussage 2| Kein Klassifikator kann besser sein als ein naiver Bayes-Klassifikator, wenn die Verteilung der Daten bekannt ist.","Wahr, Wahr","Falsch, Falsch","Wahr, Falsch","Falsch, Wahr",B
"Für die Kernel-Regression ist welche dieser strukturellen Annahmen diejenige, die den Kompromiss zwischen Underfitting und Overfitting am stärksten beeinflusst:","Ob die Kernelfunktion gaußförmig, dreieckig oder kastenförmig ist","Ob wir euklidische, L1- oder L∞-Metriken verwenden",Die Kernelbreite,Die maximale Höhe der Kernfunktion,C
"Statement 1| Der SVM-Lernalgorithmus garantiert, dass er die global optimale Hypothese in Bezug auf seine Zielfunktion findet.

Statement 2| Nachdem er durch eine radiale Basiskernfunktion in den Merkmalsraum Q abgebildet wurde, kann ein Perceptron möglicherweise eine bessere Klassifizierungsleistung erzielen als in seinem ursprünglichen Raum (obwohl wir dies nicht garantieren können).","Wahr, Wahr","Falsch, Falsch","Wahr, Falsch","Falsch, Wahr",A
"Für einen Gauß'schen Bayes-Klassifikator, welche dieser strukturellen Annahmen ist diejenige, die den Kompromiss zwischen Underfitting und Overfitting am stärksten beeinflusst:",Ob wir die Klassenzentren durch Maximum-Likelihood oder Gradientenabstieg lernen,Ob wir vollständige Klassenkovarianzmatrizen oder diagonale Klassenkovarianzmatrizen annehmen,Ob wir gleiche Klassenprioris oder aus den Daten geschätzte Prioris haben.,"Ob wir Klassen erlauben, unterschiedliche Mittelwertvektoren zu haben, oder ob wir sie zwingen, denselben Mittelwertvektor zu teilen",B
"Statement 1| Overfitting ist wahrscheinlicher, wenn der Satz der Trainingsdaten klein ist. Statement 2| Overfitting ist wahrscheinlicher, wenn der Hypothesenraum klein ist.","Wahr, Wahr","Falsch, Falsch","Wahr, Falsch","Falsch, Wahr",D
"Statement 1| Neben EM kann der Gradientenabstieg verwendet werden, um Inferenz oder Lernen auf einem Gauß'schen Mischungsmodell durchzuführen. Statement 2 | Unter der Annahme einer festen Anzahl von Attributen kann ein Gauß-basierter Bayes-optimaler Klassifikator in einer Zeit gelernt werden, die linear zur Anzahl der Datensätze im Datensatz ist.","Wahr, Wahr","Falsch, Falsch","Wahr, Falsch","Falsch, Wahr",A
"Aussage 1| In einem Bayesschen Netzwerk sind die Inferenzergebnisse des Junction-Tree-Algorithmus identisch mit den Inferenzergebnissen der Variablenelimination.

Aussage 2| Wenn zwei Zufallsvariablen X und Y bedingt unabhängig sind, gegeben eine andere Zufallsvariable Z, dann sind im entsprechenden Bayesschen Netzwerk die Knoten für X und Y d-separiert, gegeben Z.","Wahr, Wahr","Falsch, Falsch","Wahr, Falsch","Falsch, Wahr",C
"Angesichts eines großen Datensatzes von Krankenakten von Patienten, die an Herzerkrankungen leiden, versuchen Sie herauszufinden, ob es möglicherweise verschiedene Cluster solcher Patienten gibt, für die wir separate Behandlungen entwickeln könnten. Um welche Art von Lernproblem handelt es sich hierbei?",Überwachtes Lernen,Unüberwachtes Lernen,Sowohl (a) als auch (b),Weder (a) noch (b),B
"Was würden Sie bei der PCA tun, um die gleiche Projektion wie bei der SVD zu erhalten?",Daten auf Mittelwert Null transformieren,Daten auf Nullmedian transformieren,Nicht möglich,Keines davon,A
"Aussage 1| Der Trainingsfehler des 1-Nächster-Nachbar-Klassifikators beträgt 0.

Aussage 2| Wenn die Anzahl der Datenpunkte gegen Unendlich geht, nähert sich die MAP-Schätzung der MLE-Schätzung für alle möglichen Prioris an. Mit anderen Worten, bei ausreichend Daten ist die Wahl der Priori irrelevant.","Wahr, Wahr","Falsch, Falsch","Wahr, Falsch","Falsch, Wahr",C
"Bei der Durchführung einer Regression nach der Methode der kleinsten Quadrate mit Regularisierung (unter der Annahme, dass die Optimierung exakt durchgeführt werden kann) erhöht die Steigerung des Wertes des Regularisierungsparameters λ den Testfehler.",wird den Trainingsfehler niemals verringern.,wird den Trainingsfehler niemals erhöhen.,wird den Testfehler niemals verringern.,wird niemals zunehmen,A
"Welche der folgenden Aussagen beschreibt am besten, was diskriminative Ansätze zu modellieren versuchen? (w sind die Parameter im Modell)","p(y|x, w)","p(y, x)","p(w|x, w)",Nichts davon,A
"Statement 1| Die Klassifizierungsleistung von Faltungsneuronalen Netzen für CIFAR-10 kann 95% überschreiten. Statement 2| Ensembles von neuronalen Netzen verbessern die Klassifizierungsgenauigkeit nicht, da die von ihnen gelernten Repräsentationen stark korreliert sind.","Wahr, Wahr","Falsch, Falsch","Wahr, Falsch","Falsch, Wahr",C
Über welche der folgenden Punkte würden Bayesianer und Frequentisten unterschiedlicher Meinung sein?,Die Verwendung eines nicht-gaußschen Rauschmodells in der probabilistischen Regression.,Die Verwendung von probabilistischer Modellierung für Regression.,Die Verwendung von A-priori-Verteilungen für die Parameter in einem probabilistischen Modell.,Die Verwendung von Klassenpriors in der Gaußschen Diskriminanzanalyse.,C
"Aussage 1| Die BLEU-Metrik verwendet Präzision, während die ROGUE-Metrik Recall verwendet. Aussage 2| Hidden-Markov-Modelle wurden häufig verwendet, um englische Sätze zu modellieren.","Wahr, Wahr","Falsch, Falsch","Wahr, Falsch","Falsch, Wahr",A
Statement 1| ImageNet hat Bilder in verschiedenen Auflösungen. Statement 2| Caltech-101 hat mehr Bilder als ImageNet.,"Wahr, Wahr","Falsch, Falsch","Wahr, Falsch","Falsch, Wahr",C
"Welche der folgenden Methoden ist besser geeignet, um eine Merkmalsauswahl durchzuführen?",Grat,Lasso,sowohl (a) als auch (b),weder (a) noch (b),B
"Angenommen, Sie erhalten einen EM-Algorithmus, der Maximum-Likelihood-Schätzungen für ein Modell mit latenten Variablen findet. Sie werden gebeten, den Algorithmus so zu modifizieren, dass er stattdessen MAP-Schätzungen findet. Welchen Schritt oder welche Schritte müssen Sie modifizieren?",Erwartung,Maximierung,Keine Änderung erforderlich,Beide,B
"Für einen Gauß'schen Bayes-Klassifikator, welche dieser strukturellen Annahmen ist diejenige, die den Kompromiss zwischen Underfitting und Overfitting am stärksten beeinflusst:",Ob wir die Klassenzentren durch Maximum-Likelihood oder Gradientenabstieg lernen,Ob wir vollständige Klassenkovarianzmatrizen oder diagonale Klassenkovarianzmatrizen annehmen,Ob wir gleiche Klassenprioris oder aus den Daten geschätzte Prioris haben,"Ob wir Klassen erlauben, unterschiedliche Mittelwertvektoren zu haben, oder ob wir sie zwingen, denselben Mittelwertvektor zu teilen",B
"Statement 1| Für zwei beliebige Variablen x und y mit gemeinsamer Verteilung p(x, y) gilt immer H[x, y] ≥ H[x] + H[y], wobei H die Entropiefunktion ist. Statement 2| Bei einigen gerichteten Graphen verringert die Moralisierung die Anzahl der im Graphen vorhandenen Kanten.","Wahr, Wahr","Falsch, Falsch","Wahr, Falsch","Falsch, Wahr",B
Welches der folgenden ist KEINE überwachte Lernmethode?,PCA,Entscheidungsbaum,Lineare Regression,Naiver Bayes,A
Statement 1| Die Konvergenz eines neuronalen Netzwerks hängt von der Lernrate ab. Statement 2| Dropout multipliziert zufällig ausgewählte Aktivierungswerte mit Null.,"Wahr, Wahr","Falsch, Falsch","Wahr, Falsch","Falsch, Wahr",A
"Welche der folgenden Optionen ist gleich P(A, B, C), gegeben die booleschen Zufallsvariablen A, B und C, und ohne Annahmen von Unabhängigkeit oder bedingter Unabhängigkeit zwischen ihnen?",P(A | B) * P(B | C) * P(C | A),"P(C | A, B) * P(A) * P(B)","P(A, B | C) * P(C)","P(A | B, C) * P(B | A, C) * P(C | A, B)",C
Welche der folgenden Aufgaben kann am besten mit Clustering gelöst werden.,Vorhersage der Niederschlagsmenge anhand verschiedener Hinweise,Betrügerische Kreditkartentransaktionen erkennen,"Einen Roboter trainieren, ein Labyrinth zu lösen",Alles oben Genannte,B
"Nach Anwendung einer Regularisierungsstrafe in der linearen Regression stellen Sie fest, dass einige der Koeffizienten von w auf Null gesetzt wurden. Welche der folgenden Strafen könnte verwendet worden sein?",L0-Norm,L1-Norm,L2-Norm,entweder (a) oder (b),D
"A und B sind zwei Ereignisse. Wenn P(A, B) abnimmt, während P(A) zunimmt, welche der folgenden Aussagen ist dann wahr?",P(A|B) nimmt ab,P(B|A) nimmt ab,P(B) nimmt ab,Alles oben Genannte,B
"Statement 1| Wenn wir ein HMM für eine feste Menge von Beobachtungen lernen und davon ausgehen, dass wir die wahre Anzahl der verborgenen Zustände nicht kennen (was oft der Fall ist), können wir die Wahrscheinlichkeit der Trainingsdaten immer erhöhen, indem wir mehr verborgene Zustände zulassen. Statement 2| Kollaboratives Filtern ist oft ein nützliches Modell zur Modellierung der Filmpräferenzen von Benutzern.","Wahr, Wahr","Falsch, Falsch","Wahr, Falsch","Falsch, Wahr",A
"Sie trainieren ein lineares Regressionsmodell für eine einfache Schätzungsaufgabe und bemerken, dass das Modell die Daten überanpasst. Sie beschließen, $\ell_2$-Regularisierung hinzuzufügen, um die Gewichte zu bestrafen. Was wird mit dem Bias und der Varianz des Modells passieren, wenn Sie den $\ell_2$-Regularisierungskoeffizienten erhöhen?",Verzerrung nimmt zu ; Varianz nimmt zu,Verzerrung nimmt zu; Varianz nimmt ab,Bias-Abnahme ; Varianz-Zunahme,Verzerrung abnehmen ; Varianz abnehmen,B
"Welche PyTorch 1.8-Befehle erzeugen eine $10\times 5$ Gauß'sche Matrix, bei der jeder Eintrag unabhängig und identisch verteilt aus $\mathcal{N}(\mu=5,\sigma^2=16)$ gezogen wird, und eine $10\times 10$ gleichverteilte Matrix, bei der jeder Eintrag unabhängig und identisch verteilt aus $U[-1,1)$ gezogen wird?","\texttt{5 + torch.randn(10,5) * 16} ; \texttt{torch.rand(10,10,low=-1,high=1)}","\texttt{5 + torch.randn(10,5) * 16} ; \texttt{(torch.rand(10,10) - 0,5) / 0,5}","\texttt{5 + torch.randn(10,5) * 4} ; \texttt{2 * torch.rand(10,10) - 1}","\texttt{torch.normal(torch.ones(10,5)*5,torch.ones(5,5)*16)} ; \texttt{2 * torch.rand(10,10) - 1}",C
"Aussage 1| Der Gradient der ReLU ist null für $x<0$, und der Gradient der Sigmoid $\sigma(x)(1-\sigma(x))\le \frac{1}{4}$ für alle $x$. Aussage 2| Die Sigmoid hat einen kontinuierlichen Gradienten und die ReLU hat einen diskontinuierlichen Gradienten.","Wahr, Wahr","Falsch, Falsch","Wahr, Falsch","Falsch, Wahr",A
Welche Aussage über Batch-Normalisierung ist zutreffend?,Nach Anwendung der Batch-Normalisierung folgen die Aktivierungen der Schicht einer standardisierten Gauß-Verteilung.,"Der Bias-Parameter von affinen Schichten wird redundant, wenn unmittelbar danach eine Batch-Normalisierungsschicht folgt.","Die standardmäßige Gewichtsinitialisierung muss geändert werden, wenn Batch-Normalisierung verwendet wird.",Batch-Normalisierung ist äquivalent zur Layer-Normalisierung für konvolutionäre neuronale Netze.,B
"Angenommen, wir haben die folgende Zielfunktion: $\argmin_{w} \frac{1}{2} \norm{Xw-y}^2_2 + \frac{1}{2}\gamma \norm{w}^2_2$ Was ist der Gradient von $\frac{1}{2} \norm{Xw-y}^2_2 + \frac{1}{2}\lambda \norm{w}^2_2$ in Bezug auf $w$?",$\nabla_w f(w) = (X^\top X + \lambda I)w - X^\top y + \lambda w$,$\nabla_w f(w) = X^\top X w - X^\top y + \lambda$,$\nabla_w f(w) = X^\top X w - X^\top y + \lambda w$,$\nabla_w f(w) = X^\top X w - X^\top y + (\lambda+1) w$,C
Welche der folgenden Aussagen trifft auf einen Faltungskern zu?,Die Faltung eines Bildes mit $\begin{bmatrix}1 & 0 & 0\\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix}$ würde das Bild nicht verändern,Die Faltung eines Bildes mit $\begin{bmatrix}0 & 0 & 0\\ 0 & 1 & 0 \\ 0 & 0 & 0 \end{bmatrix}$ würde das Bild nicht verändern,Die Faltung eines Bildes mit $\begin{bmatrix}1 & 1 & 1\\ 1 & 1 & 1 \\ 1 & 1 & 1 \end{bmatrix}$ würde das Bild nicht verändern,Die Faltung eines Bildes mit $\begin{bmatrix}0 & 0 & 0\\ 0 & 0 & 0 \\ 0 & 0 & 0 \end{bmatrix}$ würde das Bild nicht verändern,B
Welche der folgenden Aussagen ist falsch?,"Semantische Segmentierungsmodelle sagen die Klasse jedes Pixels voraus, während Mehrklassen-Bildklassifikatoren die Klasse des gesamten Bildes vorhersagen.",Ein Begrenzungsrahmen mit einem IoU (Intersection over Union) von 96% würde wahrscheinlich als richtig positiv betrachtet werden.,"Wenn ein vorhergesagter Begrenzungsrahmen keinem Objekt in der Szene entspricht, wird er als falsch positiv betrachtet.",Ein Begrenzungsrahmen mit einem IoU (Schnittmenge geteilt durch Vereinigungsmenge) von $3\%$ würde wahrscheinlich als falsch negativ betrachtet werden.,D
Welche der folgenden Aussagen ist falsch?,"Das folgende vollständig verbundene Netzwerk ohne Aktivierungsfunktionen ist linear: $g_3(g_2(g_1(x)))$, wobei $g_i(x) = W_i x$ und $W_i$ Matrizen sind.","Leaky ReLU $\max\{0.01x,x\}$ ist konvex.",Eine Kombination von ReLUs wie $ReLU(x) - ReLU(x-1)$ ist konvex.,Der Verlust $\log \sigma(x)= -\log(1+e^{-x})$ ist konkav,C
"Wir trainieren ein vollständig verbundenes Netzwerk mit zwei versteckten Schichten, um Immobilienpreise vorherzusagen. Die Eingaben sind 100-dimensional und haben mehrere Merkmale wie die Anzahl der Quadratfuß, das mittlere Familieneinkommen usw. Die erste versteckte Schicht hat 1000 Aktivierungen. Die zweite versteckte Schicht hat 10 Aktivierungen. Die Ausgabe ist ein Skalar, der den Hauspreis repräsentiert. Unter der Annahme eines einfachen Netzwerks mit affinen Transformationen und ohne Batch-Normalisierung und ohne lernbare Parameter in der Aktivierungsfunktion, wie viele Parameter hat dieses Netzwerk?",111021,110010,111110,110011,A
"Aussage 1| Die Ableitung der Sigmoid-Funktion $\sigma(x)=(1+e^{-x})^{-1}$ in Bezug auf $x$ ist gleich $\text{Var}(B)$, wobei $B\sim \text{Bern}(\sigma(x))$ eine Bernoulli-Zufallsvariable ist. Aussage 2| Das Setzen der Bias-Parameter in jeder Schicht eines neuronalen Netzwerks auf 0 verändert den Bias-Varianz-Kompromiss so, dass die Varianz des Modells zunimmt und der Bias des Modells abnimmt.","Wahr, Wahr","Falsch, Falsch","Wahr, Falsch","Falsch, Wahr",C
