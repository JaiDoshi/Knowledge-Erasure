진술 1| 선형 회귀 추정량은 모든 불편 추정량 중에서 가장 작은 분산을 가진다. 진술 2| AdaBoost에 의해 조합된 분류기에 할당된 계수 α는 항상 음수가 아니다.,"참, 참","거짓, 거짓","참, 거짓","거짓, 참",D
진술 1| RoBERTa는 BERT가 사전 훈련한 코퍼스보다 약 10배 더 큰 코퍼스에서 사전 훈련을 합니다. 진술 2| 2018년의 ResNeXt는 일반적으로 tanh 활성화 함수를 사용했습니다.,"참, 참","거짓, 거짓","참, 거짓","거짓, 참",C
"진술 1| 서포트 벡터 머신은 로지스틱 회귀 모델과 마찬가지로 입력 예시가 주어졌을 때 가능한 레이블에 대한 확률 분포를 제공합니다.

진술 2| 선형 커널에서 더 높은 차수의 다항식 커널로 이동할 때 일반적으로 서포트 벡터는 동일하게 유지될 것으로 예상됩니다.","참, 참","거짓, 거짓","참, 거짓","거짓, 참",B
"기계 학습 문제에는 네 가지 속성과 하나의 클래스가 포함됩니다. 각 속성은 3, 2, 2, 2개의 가능한 값을 가집니다. 클래스는 3개의 가능한 값을 가집니다. 최대 몇 개의 서로 다른 예시가 가능할까요?",열둘,24,48,72,D
2020년 기준으로 고해상도 이미지 분류에 가장 적합한 아키텍처는 무엇입니까?,합성곱 신경망,그래프 네트워크,완전 연결 네트워크,RBF 네트워크,A
진술 1| 기대값 최대화 알고리즘의 연속적인 반복을 통해 데이터의 로그 우도는 항상 증가할 것이다. 진술 2| Q-학습의 한 가지 단점은 학습자가 자신의 행동이 환경에 어떤 영향을 미치는지에 대한 사전 지식이 있을 때만 사용할 수 있다는 것이다.,"참, 참","거짓, 거짓","참, 거짓","거짓, 참",B
비용 함수의 기울기를 계산하여 벡터 g에 저장했다고 가정해 봅시다. 주어진 기울기를 사용하여 한 번의 경사 하강 업데이트를 수행하는 데 드는 비용은 무엇일까요?,O(D),O(N),O(ND),O(ND^2),A
"진술 1| 연속 확률 변수 x와 그 확률 분포 함수 p(x)에 대해, 모든 x에 대해 0 ≤ p(x) ≤ 1이 성립한다. 진술 2| 의사결정 트리는 정보 이득을 최소화하여 학습된다.","참, 참","거짓, 거짓","참, 거짓","거짓, 참",B
다음과 같은 베이지안 네트워크를 고려해보세요. 이 베이지안 네트워크 H -> U <- P <- W에 필요한 독립적인 매개변수의 수는 몇 개입니까?,2,4,8,16,C
"훈련 예제의 수가 무한대로 갈수록, 그 데이터로 훈련된 당신의 모델은 다음과 같은 특성을 갖게 될 것입니다:",낮은 분산,더 높은 분산,동일한 분산,위의 것들 중 어느 것도 해당되지 않음,A
"진술 1| 2D 평면에 있는 모든 직사각형의 집합(축에 정렬되지 않은 직사각형 포함)은 5개의 점으로 이루어진 집합을 분할할 수 있다.

진술 2| k가 1일 때 k-최근접 이웃 분류기의 VC 차원은 무한대이다.","참, 참","거짓, 거짓","참, 거짓","거짓, 참",A
_ 는 훈련 데이터를 모델링하지도 못하고 새로운 데이터에 일반화하지도 못하는 모델을 가리킵니다.,잘 맞는,과적합,과소적합,위의 모든 것,C
"Statement 1| F1 점수는 클래스 불균형이 심한 데이터셋에 특히 유용할 수 있습니다.

Statement 2| ROC 곡선 아래 면적은 이상 탐지기를 평가하는 데 사용되는 주요 지표 중 하나입니다.","참, 참","거짓, 거짓","참, 거짓","거짓, 참",A
진술 1| 역전파 알고리즘은 은닉층이 있는 전역적으로 최적화된 신경망을 학습합니다. 진술 2| 선의 VC 차원은 최대 2여야 합니다. 왜냐하면 어떤 선으로도 분리할 수 없는 3개의 점이 있는 경우를 적어도 하나 찾을 수 있기 때문입니다.,"참, 참","거짓, 거짓","참, 거짓","거짓, 참",B
높은 엔트로피는 분류에서의 분할이,순수한,순수하지 않은,유용한,쓸모없는,B
진술 1| 원래 ResNet 논문에서는 배치 정규화가 아닌 레이어 정규화를 사용합니다. 진술 2| DCGAN은 훈련을 안정화하기 위해 자기 주의를 사용합니다.,"참, 참","거짓, 거짓","참, 거짓","거짓, 참",B
"특정 데이터 세트에 대한 선형 회귀 모델을 구축할 때, 특성 중 하나의 계수가 상대적으로 높은 음의 값을 가지는 것을 관찰합니다. 이는 다음을 시사합니다:",이 기능은 모델에 강한 영향을 미칩니다 (유지되어야 함),이 기능은 모델에 강한 영향을 미치지 않습니다 (무시해도 됨),추가 정보 없이는 이 기능의 중요성에 대해 언급하는 것이 불가능합니다,아무것도 결정할 수 없습니다.,C
"신경망에서 과소적합(즉, 높은 편향 모델)과 과대적합(즉, 높은 분산 모델) 사이의 균형에 가장 큰 영향을 미치는 구조적 가정은 다음 중 어느 것입니까:",은닉 노드의 수,학습률,가중치의 초기 선택,상수항 단위 입력의 사용,A
"다항식 회귀에서, 과소적합과 과대적합 사이의 균형에 가장 큰 영향을 미치는 구조적 가정은 다음 중 어느 것입니까:",다항식의 차수,가중치를 행렬 역변환으로 학습하든 경사 하강법으로 학습하든,가우시안 노이즈의 가정된 분산,상수항 단위 입력의 사용,A
"진술 1| 2020년 기준으로, 일부 모델들은 CIFAR-10에서 98% 이상의 정확도를 달성하고 있습니다. 진술 2| 원래의 ResNet들은 Adam 옵티마이저로 최적화되지 않았습니다.","참, 참","거짓, 거짓","참, 거짓","거짓, 참",A
K-평균 알고리즘,특징 공간의 차원이 샘플 수보다 크지 않아야 합니다,목적 함수의 값이 K = 1일 때 가장 작다,주어진 클러스터 수에 대해 클래스 내 분산을 최소화합니다,초기 평균값들이 샘플들 중 일부로 선택되는 경우에만 전역 최적값으로 수렴한다,C
진술 1| VGGNet은 AlexNet의 첫 번째 층 커널보다 폭과 높이가 더 작은 합성곱 커널을 가지고 있다. 진술 2| 데이터 의존적 가중치 초기화 절차는 배치 정규화 이전에 도입되었다.,"참, 참","거짓, 거짓","참, 거짓","거짓, 참",A
"다음 행렬의 계수는 무엇입니까? A = [[1, 1, 1], [1, 1, 1], [1, 1, 1]]",0,1,2,3,B
"진술 1| 밀도 추정(예를 들어, 커널 밀도 추정기를 사용하여)은 분류를 수행하는 데 사용될 수 있습니다. 진술 2| 로지스틱 회귀와 가우시안 나이브 베이즈(동일한 클래스 공분산을 가진) 사이의 대응 관계는 두 분류기의 매개변수 사이에 일대일 대응 관계가 있음을 의미합니다.","참, 참","거짓, 거짓","참, 거짓","거짓, 참",C
집의 기하학적 위치와 같은 공간 데이터에 대해 클러스터링을 수행하고자 한다고 가정해 봅시다. 우리는 다양한 크기와 모양의 클러스터를 생성하고자 합니다. 다음 중 어떤 방법이 가장 적절할까요?,의사결정 트리,밀도 기반 클러스터링,모델 기반 군집화,K-평균 군집화,B
"진술 1| AdaBoost에서 잘못 분류된 예시들의 가중치는 동일한 곱셈 인자만큼 증가합니다. 진술 2| AdaBoost에서, 가중치 D_t를 가진 훈련 데이터에 대한 t번째 약한 분류기의 가중치 훈련 오차 e_t는 t의 함수로 증가하는 경향이 있습니다.","참, 참","거짓, 거짓","참, 거짓","거짓, 참",A
MLE 추정치는 종종 바람직하지 않은데 그 이유는,그들은 편견을 가지고 있다,그들은 높은 분산을 가지고 있다,그것들은 일관된 추정량이 아닙니다,위의 것들 중 어느 것도 해당되지 않음,B
"경사 하강법의 계산 복잡도는,",D에 대해 선형,N에 대해 선형적,D의 다항식,반복 횟수에 따라 달라짐,C
여러 결정 트리의 출력을 평균화하는 것은 _에 도움이 됩니다.,편향성 증가,편견 감소,분산 증가,분산 감소,D
선형 회귀를 식별된 특성 부분집합에 적용하여 얻은 모델은 부분집합을 식별하는 과정의 끝에서 얻은 모델과 다를 수 있습니다,최적 부분집합 선택,전진 단계별 선택,전진 단계별 선택,위의 모든 것,C
신경망,볼록 목적 함수를 최적화하다,확률적 경사 하강법으로만 훈련될 수 있음,다양한 활성화 함수를 혼합하여 사용할 수 있습니다,위의 것들 중 어느 것도 해당되지 않음,C
"질병 D의 발생률이 100명당 약 5건이라고 합시다(즉, P(D) = 0.05). 부울 확률 변수 D는 환자가 ""질병 D를 가지고 있음""을 의미하고, 부울 확률 변수 TP는 ""양성 판정""을 의미합니다. 질병 D에 대한 검사는 매우 정확한 것으로 알려져 있어서, 질병이 있을 때 양성 판정을 받을 확률은 0.99이고, 질병이 없을 때 음성 판정을 받을 확률은 0.97입니다. P(TP), 즉 양성 판정을 받을 사전 확률은 얼마입니까?",0.0368,0.473,0.078,위의 것들 중 어느 것도 해당되지 않음,C
"진술 1| 방사형 기저 커널 함수를 통해 특징 공간 Q로 매핑된 후, 가중치가 없는 유클리드 거리를 사용하는 1-NN은 원래 공간에서보다 더 나은 분류 성능을 달성할 수 있습니다 (비록 이를 보장할 수는 없지만). 진술 2| 퍼셉트론의 VC 차원은 단순 선형 SVM의 VC 차원보다 작습니다.","참, 참","거짓, 거짓","참, 거짓","거짓, 참",B
Grid search의 단점은,미분 불가능한 함수에는 적용할 수 없습니다.,연속적이지 않은 함수에는 적용할 수 없습니다.,구현하기가 어렵습니다.,다중 선형 회귀에 대해서는 상당히 느리게 실행됩니다.,D
다양한 단서를 바탕으로 한 지역의 강수량을 예측하는 것은 ______ 문제입니다.,지도 학습,비지도 학습,클러스터링,위의 것들 중 어느 것도 해당되지 않음,A
다음 중 회귀에 관한 거짓된 문장은 무엇입니까?,입력을 출력과 연관시킵니다.,예측에 사용됩니다.,통역에 사용될 수 있습니다.,인과 관계를 발견합니다,D
다음 중 의사결정 트리를 가지치기하는 주된 이유는 무엇입니까?,테스트 중 컴퓨팅 시간을 절약하기 위해,의사결정 트리를 저장하기 위한 공간을 절약하기 위해,훈련 세트 오차를 더 작게 만들기 위해,훈련 세트의 과적합을 피하기 위해,D
진술 1| 커널 밀도 추정기는 원래 데이터 세트의 각 점 Xi에서 Yi = 1/n 값으로 커널 회귀를 수행하는 것과 동등합니다. 진술 2| 학습된 결정 트리의 깊이는 트리를 만드는 데 사용된 훈련 예제의 수보다 클 수 있습니다.,"참, 참","거짓, 거짓","참, 거짓","거짓, 참",B
모델이 과적합되고 있다고 가정해 봅시다. 다음 중 과적합을 줄이기 위한 유효한 방법이 아닌 것은 무엇일까요?,훈련 데이터의 양을 늘리세요.,오류 최소화를 위해 사용되는 최적화 알고리즘을 개선하십시오.,모델 복잡성을 줄이세요.,훈련 데이터의 노이즈를 줄이세요.,B
진술 1| 소프트맥스 함수는 다중 클래스 로지스틱 회귀에서 흔히 사용됩니다. 진술 2| 비균일 소프트맥스 분포의 온도는 그 엔트로피에 영향을 미칩니다.,"참, 참","거짓, 거짓","참, 거짓","거짓, 참",A
다음 중 SVM에 관해 맞는 것은 무엇입니까?,"2차원 데이터 포인트의 경우, 선형 SVM이 학습한 분리 초평면은 직선이 될 것입니다.","이론적으로, 가우시안 커널 SVM은 복잡한 분리 초평면을 모델링할 수 없습니다.","SVM에서 사용되는 모든 커널 함수에 대해, 동등한 닫힌 형태의 기저 확장을 얻을 수 있다.",SVM에서의 과적합은 서포트 벡터의 수와 관련이 없습니다.,A
"다음 중 주어진 베이지안 네트워크 H -> U <- P <- W에 의해 설명되는 H, U, P, W의 결합 확률은 무엇입니까? [참고: 조건부 확률의 곱으로]","P(H, U, P, W) = P(H) * P(W) * P(P) * P(U)","P(H, U, P, W) = P(H) * P(W) * P(P | W) * P(W | H, P)","P(H, U, P, W) = P(H) * P(W) * P(P | W) * P(U | H, P)",위의 것들 중 어느 것도 해당되지 않음,C
"진술 1| VC 차원이 무한대인 방사형 기저 커널을 가진 SVM은 유한한 VC 차원을 가진 다항식 커널 SVM보다 반드시 성능이 떨어져야 한다. 진술 2| 선형 활성화 함수를 가진 2층 신경망은 본질적으로 주어진 데이터셋에 대해 훈련된 선형 분리기의 가중 조합이다; 선형 분리기를 기반으로 한 부스팅 알고리즘 또한 선형 분리기의 조합을 찾으므로, 이 두 알고리즘은 동일한 결과를 제공할 것이다.","참, 참","거짓, 거짓","참, 거짓","거짓, 참",B
Statement 1| ID3 알고리즘은 최적의 의사결정 트리를 찾는 것이 보장됩니다. Statement 2| 밀도 f()가 모든 곳에서 0이 아닌 연속 확률 분포를 고려해보세요. 값 x의 확률은 f(x)와 같습니다.,"참, 참","거짓, 거짓","참, 거짓","거짓, 참",B
"주어진 신경망은 N개의 입력 노드, 은닉층 없음, 하나의 출력 노드, 엔트로피 손실 함수와 시그모이드 활성화 함수를 가지고 있습니다. 다음 중 어떤 알고리즘(적절한 하이퍼파라미터와 초기화를 사용하여)이 전역 최적해를 찾는 데 사용될 수 있습니까?",확률적 경사 하강법,미니배치 경사 하강법,배치 경사 하강법,위의 모든 것,D
"선형 모델에서 더 많은 기저 함수를 추가할 때, 가장 가능성 있는 옵션을 선택하세요:",모델 편향 감소,추정 편향을 감소시킵니다,분산을 감소시킴,편향과 분산에 영향을 미치지 않음,A
다음과 같은 베이지안 네트워크를 고려해 봅시다. 독립성이나 조건부 독립성에 대한 어떠한 가정도 하지 않는다면 H -> U <- P <- W에 대해 몇 개의 독립적인 매개변수가 필요할까요?,3,4,7,15,D
분포 외 탐지의 또 다른 용어는 무엇입니까?,이상 탐지,일원 클래스 탐지,훈련-테스트 불일치 견고성,배경 감지,A
"진술 1| 우리는 약한 학습기 h를 부스팅하여 분류기 f를 학습합니다. f의 결정 경계의 함수 형태는 h와 동일하지만 매개변수가 다릅니다. (예: h가 선형 분류기였다면 f도 선형 분류기입니다).

진술 2| 교차 검증은 부스팅에서 반복 횟수를 선택하는 데 사용될 수 있습니다; 이 절차는 과적합을 줄이는 데 도움이 될 수 있습니다.","참, 참","거짓, 거짓","참, 거짓","거짓, 참",D
진술 1| 고속도로 네트워크는 ResNet 이후에 도입되었으며 최대 풀링 대신 컨볼루션을 선호합니다. 진술 2| DenseNet은 일반적으로 ResNet보다 더 많은 메모리를 필요로 합니다.,"참, 참","거짓, 거짓","참, 거짓","거짓, 참",D
"N이 훈련 데이터셋의 인스턴스 수일 때, 최근접 이웃 알고리즘의 분류 실행 시간은",O(1),O( N ),O(log N),O( N^2 ),B
"진술 1| 원래의 ResNet과 Transformer는 순방향 신경망입니다. 진술 2| 원래의 Transformer는 자기 주의(self-attention)를 사용하지만, 원래의 ResNet은 그렇지 않습니다.","참, 참","거짓, 거짓","참, 거짓","거짓, 참",A
"진술 1| ReLU는 단조롭지 않지만, 시그모이드는 단조롭습니다. 진술 2| 경사 하강법으로 훈련된 신경망은 높은 확률로 전역 최적점에 수렴합니다.","참, 참","거짓, 거짓","참, 거짓","거짓, 참",D
신경망의 시그모이드 노드의 수치 출력:,"무한하며, 모든 실수를 포함합니다.","무한하며, 모든 정수를 포함합니다.",0과 1 사이에 제한됩니다.,-1과 1 사이에 제한됩니다.,C
다음 중 훈련 데이터가 선형적으로 분리 가능할 때만 사용할 수 있는 것은 무엇입니까?,선형 하드 마진 SVM,선형 로지스틱 회귀,선형 소프트 마진 SVM,중심법,A
다음 중 공간 클러스터링 알고리즘은 무엇입니까?,분할 기반 클러스터링,K-평균 군집화,격자 기반 클러스터링,위의 모든 것,D
"진술 1| 서포트 벡터 머신이 구성하는 최대 마진 결정 경계는 모든 선형 분류기 중에서 가장 낮은 일반화 오류를 가집니다.

진술 2| 클래스 조건부 가우시안 분포를 가진 생성 모델에서 얻을 수 있는 모든 결정 경계는 원칙적으로 SVM과 3차 이하의 다항식 커널을 사용하여 재현할 수 있습니다.","참, 참","거짓, 거짓","참, 거짓","거짓, 참",D
진술 1| L2 정규화는 L1 정규화보다 선형 모델을 더 희소하게 만드는 경향이 있다. 진술 2| 잔차 연결은 ResNet과 트랜스포머에서 찾을 수 있다.,"참, 참","거짓, 거짓","참, 거짓","거짓, 참",D
"다음과 같은 숫자 집합 중 어느 것이 계산에 충분한지 가정해 봅시다. P(H|E, F)를 계산하고 싶고 조건부 독립 정보가 없다고 가정합니다.","P(E, F), P(H), P(E|H), P(F|H)","P(E, F), P(H), P(E, F|H)","P(H), P(E|H), P(F|H)","P(E, F), P(E|H), P(F|H)",B
다음 중 어느 것이 배깅을 수행할 때 과적합을 방지합니까?,복원 추출을 표본 추출 기법으로 사용하는 것,약한 분류기의 사용,과적합에 취약하지 않은 분류 알고리즘의 사용,모든 훈련된 분류기에 대해 수행되는 검증 실습,B
"진술 1| PCA와 스펙트럴 클러스터링(예: Andrew Ng의)은 두 개의 서로 다른 행렬에 대해 고유값 분해를 수행합니다. 그러나 이 두 행렬의 크기는 동일합니다. 진술 2| 분류는 회귀의 특수한 경우이므로, 로지스틱 회귀는 선형 회귀의 특수한 경우입니다.","참, 참","거짓, 거짓","참, 거짓","거짓, 참",B
진술 1| 스탠포드 감성 트리뱅크는 도서 리뷰가 아닌 영화 리뷰를 포함하고 있었다. 진술 2| 펜 트리뱅크는 언어 모델링에 사용되어 왔다.,"참, 참","거짓, 거짓","참, 거짓","거짓, 참",A
"다음 행렬의 영공간의 차원은 무엇입니까? A = [[3, 2, −9], [−6, −4, 18], [12, 8, −36]]",0,1,2,3,C
서포트 벡터란 무엇입니까?,결정 경계에서 가장 멀리 떨어진 예시들.,SVM에서 f(x)를 계산하는 데 필요한 유일한 예시들.,데이터 중심점,SVM에서 0이 아닌 가중치 αk를 가진 모든 예시들.,B
"Statement 1| Word2Vec 매개변수는 제한된 볼츠만 머신을 사용하여 초기화되지 않았습니다.
Statement 2| tanh 함수는 비선형 활성화 함수입니다.","참, 참","거짓, 거짓","참, 거짓","거짓, 참",A
"에포크 수가 증가함에 따라 훈련 손실이 증가한다면, 다음 중 어떤 것이 학습 과정의 가능한 문제일 수 있습니까?",정규화가 너무 낮고 모델이 과적합되고 있습니다,정규화가 너무 높아 모델이 과소적합되고 있습니다,단계 크기가 너무 큽니다,단계 크기가 너무 작습니다,C
"질병 D의 발생률이 100명당 약 5건이라고 합시다(즉, P(D) = 0.05). 부울 확률 변수 D는 환자가 ""질병 D를 가지고 있음""을 의미하고, 부울 확률 변수 TP는 ""양성 판정""을 의미합니다. 질병 D에 대한 검사는 매우 정확한 것으로 알려져 있어서, 질병이 있을 때 양성 판정을 받을 확률은 0.99이고, 질병이 없을 때 음성 판정을 받을 확률은 0.97입니다. 검사 결과가 양성일 때 질병 D를 가지고 있을 사후 확률 P(D | TP)는 얼마입니까?",0.0495,0.078,0.635,0.97,C
Statement 1| 전통적인 기계 학습 결과는 훈련 세트와 테스트 세트가 독립적이고 동일하게 분포되어 있다고 가정합니다. Statement 2| 2017년에는 COCO 모델들이 일반적으로 ImageNet에서 사전 훈련되었습니다.,"참, 참","거짓, 거짓","참, 거짓","거짓, 참",A
"진술 1| 동일한 훈련 세트에서 두 개의 서로 다른 커널 K1(x, x0)과 K2(x, x0)로 얻은 마진의 값들은 어떤 분류기가 테스트 세트에서 더 나은 성능을 보일지 알려주지 않는다. 진술 2| BERT의 활성화 함수는 GELU이다.","참, 참","거짓, 거짓","참, 거짓","거짓, 참",A
다음 중 기계 학습에서 군집화 알고리즘은 무엇입니까?,기대값 최대화,카트,가우시안 나이브 베이즈,선험적,A
"스팸 분류를 위한 의사결정 트리 훈련을 막 끝냈는데, 훈련 세트와 테스트 세트 모두에서 비정상적으로 나쁜 성능을 보이고 있습니다. 구현에는 버그가 없다는 것을 알고 있습니다. 그렇다면 무엇이 이 문제를 일으키고 있을까요?",당신의 의사결정 트리가 너무 얕습니다.,학습률을 높여야 합니다.,과적합하고 있습니다.,위의 것들 중 어느 것도 해당되지 않음.,A
K-겹 교차 검증은,K에 대해 선형,K에 대해 2차,K에서 입방,K에 대해 지수적,A
진술 1| 산업 규모의 신경망은 일반적으로 GPU가 아닌 CPU에서 훈련됩니다. 진술 2| ResNet-50 모델은 10억 개 이상의 매개변수를 가지고 있습니다.,"참, 참","거짓, 거짓","참, 거짓","거짓, 참",B
"두 개의 부울 확률 변수 A와 B가 주어졌을 때, P(A) = 1/2, P(B) = 1/3, 그리고 P(A | ¬B) = 1/4라면, P(A | B)는 얼마인가?",6분의 1,4분의 1,4분의 3,1,D
AI가 제기하는 실존적 위험은 다음 중 어느 교수와 가장 일반적으로 연관되어 있습니까?,난도 데 프레이타스,얀 르쿤,스튜어트 러셀,지텐드라 말리크,C
"Statement 1| 로지스틱 회귀 모델의 가능도를 최대화하면 여러 개의 국소 최적점이 생깁니다.

Statement 2| 데이터의 분포를 알고 있다면 어떤 분류기도 나이브 베이즈 분류기보다 더 나은 성능을 낼 수 없습니다.","참, 참","거짓, 거짓","참, 거짓","거짓, 참",B
커널 회귀에서 과소적합과 과대적합 사이의 균형에 가장 큰 영향을 미치는 구조적 가정은 다음 중 어느 것입니까:,커널 함수가 가우시안인지 삼각형인지 박스 모양인지,"유클리드 거리, L1 거리, L∞ 거리 중 어떤 메트릭을 사용하는지",커널 폭,커널 함수의 최대 높이,C
"진술 1| SVM 학습 알고리즘은 그 목적 함수와 관련하여 전역적으로 최적의 가설을 찾는 것이 보장됩니다.

진술 2| 방사형 기저 커널 함수를 통해 특징 공간 Q로 매핑된 후, 퍼셉트론은 원래 공간에서보다 더 나은 분류 성능을 달성할 수 있습니다(비록 이를 보장할 수는 없지만).","참, 참","거짓, 거짓","참, 거짓","거짓, 참",A
"가우시안 베이즈 분류기에서, 과소적합과 과대적합 사이의 균형에 가장 큰 영향을 미치는 구조적 가정은 다음 중 어느 것입니까:",최대 우도법 또는 경사 하강법으로 클래스 중심을 학습하는지 여부,우리가 완전한 클래스 공분산 행렬을 가정하든 대각선 클래스 공분산 행렬을 가정하든,클래스 사전 확률이 동일한지 또는 데이터에서 추정된 사전 확률을 사용하는지 여부.,클래스들이 서로 다른 평균 벡터를 가지도록 허용할지 아니면 동일한 평균 벡터를 공유하도록 강제할지,B
진술 1| 과적합은 훈련 데이터 세트가 작을 때 더 발생하기 쉽습니다. 진술 2| 과적합은 가설 공간이 작을 때 더 발생하기 쉽습니다.,"참, 참","거짓, 거짓","참, 거짓","거짓, 참",D
"Statement 1| EM 외에도 가우시안 혼합 모델에서 추론이나 학습을 수행하기 위해 경사 하강법을 사용할 수 있습니다.

Statement 2 | 고정된 수의 속성을 가정할 때, 가우시안 기반 베이즈 최적 분류기는 데이터셋의 레코드 수에 선형적인 시간 내에 학습될 수 있습니다.","참, 참","거짓, 거짓","참, 거짓","거짓, 참",A
"진술 1| 베이지안 네트워크에서 정션 트리 알고리즘의 추론 결과는 변수 소거법의 추론 결과와 동일하다.

진술 2| 만약 두 개의 랜덤 변수 X와 Y가 다른 랜덤 변수 Z가 주어졌을 때 조건부 독립이라면, 해당 베이지안 네트워크에서 X와 Y의 노드는 Z가 주어졌을 때 d-분리된다.","참, 참","거짓, 거짓","참, 거짓","거짓, 참",C
"심장병을 앓고 있는 환자들의 대규모 의료 기록 데이터셋이 주어졌을 때, 우리가 별도의 치료법을 맞춤화할 수 있는 서로 다른 환자 군집이 있는지 알아보려고 합니다. 이것은 어떤 종류의 학습 문제입니까?",지도 학습,비지도 학습,(a)와 (b) 모두,(a)도 (b)도 아님,B
PCA에서 SVD와 동일한 투영을 얻으려면 어떻게 해야 할까요?,데이터를 평균이 0이 되도록 변환,데이터를 0 중앙값으로 변환,불가능,이 중 어느 것도 아님,A
"진술 1| 1-최근접 이웃 분류기의 훈련 오차는 0이다.

진술 2| 데이터 포인트의 수가 무한대로 증가함에 따라, MAP 추정치는 모든 가능한 사전 확률에 대해 MLE 추정치에 접근한다. 다시 말해, 충분한 데이터가 주어지면 사전 확률의 선택은 무관해진다.","참, 참","거짓, 거짓","참, 거짓","거짓, 참",C
"정규화를 사용한 최소 제곱 회귀를 수행할 때 (최적화가 정확히 수행될 수 있다고 가정하면), 정규화 매개변수 λ의 값을 증가시키면 테스트 오차가.",절대 훈련 오류를 감소시키지 않을 것입니다.,절대 훈련 오류를 증가시키지 않을 것입니다.,절대 테스트 오류를 감소시키지 않을 것입니다.,절대 증가하지 않을 것이다,A
다음 중 판별적 접근법이 모델링하려는 것을 가장 잘 설명한 것은 무엇입니까? (w는 모델의 매개변수입니다),"p(y|x, w)","p(y, x)","p(w|x, w)",위의 것들 중 어느 것도 해당되지 않음,A
진술 1| CIFAR-10 분류에 대한 합성곱 신경망의 성능은 95%를 초과할 수 있습니다. 진술 2| 신경망 앙상블은 학습하는 표현이 높은 상관관계를 가지고 있기 때문에 분류 정확도를 향상시키지 않습니다.,"참, 참","거짓, 거짓","참, 거짓","거짓, 참",C
다음 중 베이지안과 빈도주의자들이 의견을 달리할 수 있는 점은 무엇일까요?,확률적 회귀에서 비가우시안 노이즈 모델의 사용.,회귀 분석을 위한 확률적 모델링의 사용.,확률론적 모델에서 매개변수에 대한 사전 분포의 사용.,가우시안 판별 분석에서 클래스 사전 확률의 사용.,C
"문장 1| BLEU 메트릭은 정밀도를 사용하는 반면, ROGUE 메트릭은 재현율을 사용합니다. 문장 2| 은닉 마르코프 모델은 영어 문장을 모델링하는 데 자주 사용되었습니다.","참, 참","거짓, 거짓","참, 거짓","거짓, 참",A
진술 1| ImageNet은 다양한 해상도의 이미지를 가지고 있습니다. 진술 2| Caltech-101은 ImageNet보다 더 많은 이미지를 가지고 있습니다.,"참, 참","거짓, 거짓","참, 거짓","거짓, 참",C
다음 중 어느 것이 특성 선택을 하는 데 더 적절합니까?,능선,올가미,(a)와 (b) 모두,(a)도 (b)도 아님,B
EM 알고리즘이 잠재 변수가 있는 모델에 대한 최대 우도 추정치를 찾는다고 가정해 봅시다. 이 알고리즘을 수정하여 MAP 추정치를 찾도록 요청받았습니다. 어떤 단계 또는 단계들을 수정해야 할까요?,기대,최대화,수정 불필요,둘 다,B
"가우시안 베이즈 분류기에서, 과소적합과 과대적합 사이의 균형에 가장 큰 영향을 미치는 구조적 가정은 다음 중 어느 것입니까:",최대 우도법 또는 경사 하강법으로 클래스 중심을 학습하는지 여부,우리가 완전한 클래스 공분산 행렬을 가정하든 대각선 클래스 공분산 행렬을 가정하든,우리가 동등한 클래스 사전 확률을 가지고 있는지 또는 데이터에서 추정된 사전 확률을 가지고 있는지,클래스들이 서로 다른 평균 벡터를 가지도록 허용할지 아니면 동일한 평균 벡터를 공유하도록 강제할지,B
"진술 1| 결합 분포 p(x, y)를 가진 두 변수 x와 y에 대해, H가 엔트로피 함수일 때 항상 H[x, y] ≥ H[x] + H[y]가 성립한다. 진술 2| 일부 방향성 그래프의 경우, 도덕화(moralization)는 그래프에 존재하는 간선의 수를 감소시킨다.","참, 참","거짓, 거짓","참, 거짓","거짓, 참",B
다음 중 지도 학습이 아닌 것은 무엇입니까?,PCA,의사결정 트리,선형 회귀,나이브 베이지안,A
진술 1| 신경망의 수렴은 학습률에 따라 달라집니다. 진술 2| 드롭아웃은 무작위로 선택된 활성화 값을 0으로 곱합니다.,"참, 참","거짓, 거짓","참, 거짓","거짓, 참",A
"다음 중 어느 것이 부울 확률 변수 A, B, C가 주어지고 이들 사이에 독립성이나 조건부 독립성 가정이 없을 때 P(A, B, C)와 동일합니까?",P(A | B) * P(B | C) * P(C | A),"P(C | A, B) * P(A) * P(B)","P(A, B | C) * P(C)","P(A | B, C) * P(B | A, C) * P(C | A, B)",C
다음 중 어떤 작업이 군집화를 사용하여 가장 잘 해결될 수 있습니까?,다양한 단서를 바탕으로 강수량 예측하기,사기성 신용카드 거래 탐지,로봇을 미로 해결을 위해 훈련시키기,위의 모든 것,B
"선형 회귀에서 정규화 패널티를 적용한 후, w의 일부 계수가 0으로 처리된 것을 발견했습니다. 다음 중 어떤 패널티가 사용되었을 가능성이 있습니까?",L0 노름,L1 노름,L2 노름,(a) 또는 (b),D
"A와 B는 두 가지 사건입니다. P(A, B)가 감소하는 동안 P(A)가 증가한다면, 다음 중 어느 것이 사실입니까?",P(A|B)가 감소한다,P(B|A)가 감소한다,P(B)가 감소한다,위의 모든 것,B
"진술 1| 고정된 관측치 집합에 대해 HMM을 학습할 때, 실제 은닉 상태의 수를 모른다고 가정하면(이는 흔한 경우임), 더 많은 은닉 상태를 허용함으로써 항상 훈련 데이터의 가능도를 높일 수 있습니다.

진술 2| 협업 필터링은 종종 사용자의 영화 선호도를 모델링하는 데 유용한 모델입니다.","참, 참","거짓, 거짓","참, 거짓","거짓, 참",A
"간단한 추정 작업을 위해 선형 회귀 모델을 훈련시키고 있는데, 모델이 데이터에 과적합되고 있음을 알아챘습니다. 가중치에 페널티를 주기 위해 $\ell_2$ 정규화를 추가하기로 결정했습니다. $\ell_2$ 정규화 계수를 증가시킬 때, 모델의 편향과 분산은 어떻게 변할까요?",편향 증가 ; 분산 증가,편향 증가 ; 분산 감소,편향 감소 ; 분산 증가,편향 감소 ; 분산 감소,B
"PyTorch 1.8에서 각 항목이 $\mathcal{N}(\mu=5,\sigma^2=16)$에서 독립적이고 동일하게 샘플링된 $10\times 5$ 가우시안 행렬과 각 항목이 $U[-1,1)$에서 독립적이고 동일하게 샘플링된 $10\times 10$ 균일 행렬을 생성하는 명령어는 무엇인가요?","\texttt{5 + torch.randn(10,5) * 16} ; \texttt{torch.rand(10,10,low=-1,high=1)}","\texttt{5 + torch.randn(10,5) * 16} ; \texttt{(torch.rand(10,10) - 0.5) / 0.5}","\texttt{5 + torch.randn(10,5) * 4} ; \texttt{2 * torch.rand(10,10) - 1}","\texttt{torch.normal(torch.ones(10,5)*5,torch.ones(5,5)*16)} ; \texttt{2 * torch.rand(10,10) - 1}",C
"진술 1| ReLU의 기울기는 $x<0$일 때 0이며, 시그모이드의 기울기 $\sigma(x)(1-\sigma(x))$는 모든 $x$에 대해 $\frac{1}{4}$ 이하입니다. 진술 2| 시그모이드는 연속적인 기울기를 가지고 있고 ReLU는 불연속적인 기울기를 가지고 있습니다.","참, 참","거짓, 거짓","참, 거짓","거짓, 참",A
Batch Normalization에 대해 어떤 것이 사실인가요?,"배치 정규화를 적용한 후, 해당 층의 활성화는 표준 가우시안 분포를 따르게 됩니다.",어파인 레이어의 편향 매개변수는 배치 정규화 레이어가 바로 뒤에 따라오는 경우 불필요해집니다.,배치 정규화를 사용할 때는 표준 가중치 초기화를 변경해야 합니다.,배치 정규화는 합성곱 신경망에서 레이어 정규화와 동등합니다.,B
다음과 같은 목적 함수가 있다고 가정해 봅시다: $\argmin_{w} \frac{1}{2} \norm{Xw-y}^2_2 + \frac{1}{2}\gamma \norm{w}^2_2$ $\frac{1}{2} \norm{Xw-y}^2_2 + \frac{1}{2}\lambda \norm{w}^2_2$의 $w$에 대한 그래디언트는 무엇입니까?,$\nabla_w f(w) = (X^\top X + \lambda I)w - X^\top y + \lambda w$,$\nabla_w f(w) = X^\top X w - X^\top y + \lambda$,$\nabla_w f(w) = X^\top X w - X^\top y + \lambda w$,$\nabla_w f(w) = X^\top X w - X^\top y + (\lambda+1) w$,C
다음 중 컨볼루션 커널에 대해 사실인 것은 무엇입니까?,이미지를 $\begin{bmatrix}1 & 0 & 0\\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix}$로 컨볼루션하면 이미지가 변하지 않을 것입니다,이미지를 $\begin{bmatrix}0 & 0 & 0\\ 0 & 1 & 0 \\ 0 & 0 & 0 \end{bmatrix}$로 컨볼루션하면 이미지가 변하지 않을 것입니다,$\begin{bmatrix}1 & 1 & 1\\ 1 & 1 & 1 \\ 1 & 1 & 1 \end{bmatrix}$로 이미지를 컨볼루션하면 이미지가 변하지 않을 것입니다,$\begin{bmatrix}0 & 0 & 0\\ 0 & 0 & 0 \\ 0 & 0 & 0 \end{bmatrix}$로 이미지를 컨볼루션하면 이미지가 변하지 않을 것입니다,B
다음 중 거짓인 것은 무엇입니까?,"의미론적 분할 모델은 각 픽셀의 클래스를 예측하는 반면, 다중 클래스 이미지 분류기는 전체 이미지의 클래스를 예측합니다.",IoU(교집합/합집합)가 96%인 경계 상자는 일반적으로 참 양성으로 간주될 것입니다.,"예측된 경계 상자가 장면의 어떤 물체와도 일치하지 않을 때, 이는 거짓 양성으로 간주됩니다.",IoU(교집합/합집합)가 3%인 경계 상자는 거짓 음성으로 간주될 가능성이 높습니다.,D
다음 중 거짓인 것은 무엇입니까?,"다음과 같은 활성화 함수가 없는 완전 연결 네트워크는 선형적입니다: $g_3(g_2(g_1(x)))$, 여기서 $g_i(x) = W_i x$이고 $W_i$는 행렬입니다.","Leaky ReLU $\max\{0.01x,x\}$는 볼록합니다.","ReLU의 조합, 예를 들어 $ReLU(x) - ReLU(x-1)$는 볼록하다.",손실 $\log \sigma(x)= -\log(1+e^{-x})$는 오목합니다,C
"우리는 주택 가격을 예측하기 위해 두 개의 은닉층을 가진 완전 연결 네트워크를 훈련하고 있습니다. 입력은 $100$차원이며, 평방 피트 수, 가구 중간 소득 등 여러 특성을 가지고 있습니다. 첫 번째 은닉층은 $1000$개의 활성화를 가집니다. 두 번째 은닉층은 $10$개의 활성화를 가집니다. 출력은 주택 가격을 나타내는 스칼라입니다. 아핀 변환을 사용하고 배치 정규화와 활성화 함수에 학습 가능한 매개변수가 없는 기본적인 네트워크를 가정할 때, 이 네트워크는 몇 개의 매개변수를 가지고 있습니까?",111021,110010,111110,110011,A
진술 1| 시그모이드 $\sigma(x)=(1+e^{-x})^{-1}$의 $x$에 대한 도함수는 $B\sim \text{Bern}(\sigma(x))$인 베르누이 확률 변수 $B$의 $\text{Var}(B)$와 같다. 진술 2| 신경망의 각 층에서 편향 매개변수를 0으로 설정하면 편향-분산 트레이드오프가 변경되어 모델의 분산은 증가하고 모델의 편향은 감소한다.,"참, 참","거짓, 거짓","참, 거짓","거짓, 참",C
