Tvrzení 1| Lineární regresní estimátor má nejmenší rozptyl mezi všemi nestrannými estimátory. Tvrzení 2| Koeficienty α přiřazené klasifikátorům sestaveným pomocí AdaBoost jsou vždy nezáporné.,"Pravda, Pravda","Nepravda, Nepravda","Pravda, Nepravda","Nepravda, Pravda",D
"Tvrzení 1| RoBERTa se předtrénuje na korpusu, který je přibližně 10krát větší než korpus, na kterém se předtrénoval BERT. Tvrzení 2| ResNeXty v roce 2018 obvykle používaly aktivační funkce tanh.","Pravda, Pravda","Nepravda, Nepravda","Pravda, Nepravda","Nepravda, Pravda",C
"Tvrzení 1| Stroje podpůrných vektorů, podobně jako modely logistické regrese, poskytují pravděpodobnostní rozdělení možných označení pro daný vstupní příklad. Tvrzení 2| Očekávali bychom, že podpůrné vektory zůstanou obecně stejné, když přejdeme od lineárního jádra k jádrům polynomů vyšších řádů.","Pravda, Pravda","Nepravda, Nepravda","Pravda, Nepravda","Nepravda, Pravda",B
"Problém strojového učení zahrnuje čtyři atributy plus třídu. Atributy mají 3, 2, 2 a 2 možné hodnoty. Třída má 3 možné hodnoty. Kolik je maximálně možných různých příkladů?",12,24,48,72,D
"K roku 2020, která architektura je nejlepší pro klasifikaci obrázků s vysokým rozlišením?",konvoluční sítě,grafové sítě,plně propojené sítě,RBF sítě,A
"Tvrzení 1| Věrohodnost dat se bude vždy zvyšovat během po sobě jdoucích iterací algoritmu očekávané maximalizace. Tvrzení 2| Jednou z nevýhod Q-učení je, že ho lze použít pouze tehdy, když má učící se subjekt předchozí znalosti o tom, jak jeho akce ovlivňují jeho prostředí.","Pravda, Pravda","Nepravda, Nepravda","Pravda, Nepravda","Nepravda, Pravda",B
"Řekněme, že jsme vypočítali gradient naší nákladové funkce a uložili jej do vektoru g. Jaké jsou náklady na jednu aktualizaci gradientního sestupu při daném gradientu?",O(D),O(N),O(ND),O(ND^2),A
"Tvrzení 1| Pro spojitou náhodnou proměnnou x a její funkci hustoty pravděpodobnosti p(x) platí, že 0 ≤ p(x) ≤ 1 pro všechna x. Tvrzení 2| Rozhodovací strom se učí minimalizací informačního zisku.","Pravda, Pravda","Nepravda, Nepravda","Pravda, Nepravda","Nepravda, Pravda",B
Uvažujte Bayesovskou síť uvedenou níže. Kolik nezávislých parametrů je potřeba pro tuto Bayesovskou síť H -> U <- P <- W?,2,4,8,16,C
"S tím, jak se počet tréninkových příkladů blíží nekonečnu, váš model trénovaný na těchto datech bude mít:",Nižší rozptyl,Vyšší rozptyl,Stejný rozptyl,Nic z výše uvedeného,A
"Tvrzení 1| Množina všech obdélníků ve 2D rovině (která zahrnuje i obdélníky, které nejsou zarovnány s osami) může rozdělit množinu 5 bodů. Tvrzení 2| VC-dimenze klasifikátoru k-nejbližších sousedů, když k = 1, je nekonečná.","Pravda, Pravda","Nepravda, Nepravda","Pravda, Nepravda","Nepravda, Pravda",A
"_ označuje model, který nedokáže ani modelovat trénovací data, ani zobecňovat na nová data.",dobře padnoucí,přeučení,podtrénování,všechno výše uvedené,C
"Statement 1| F1 skóre může být obzvláště užitečné pro datové sady s vysokou nevyvážeností tříd.

Statement 2| Plocha pod křivkou ROC je jednou z hlavních metrik používaných k hodnocení detektorů anomálií.","Pravda, Pravda","Nepravda, Nepravda","Pravda, Nepravda","Nepravda, Pravda",A
"Tvrzení 1| Algoritmus zpětného šíření se učí globálně optimální neuronovou síť se skrytými vrstvami. Tvrzení 2| VC dimenze přímky by měla být nejvýše 2, protože mohu najít alespoň jeden případ 3 bodů, které nemohou být rozděleny žádnou přímkou.","Pravda, Pravda","Nepravda, Nepravda","Pravda, Nepravda","Nepravda, Pravda",B
"Vysoká entropie znamená, že rozdělení v klasifikaci jsou",čistý,nečistý,užitečný,zbytečný,B
"Tvrzení 1| V původním článku o ResNet se používá normalizace vrstev, nikoli normalizace dávek. Tvrzení 2| DCGANs používají sebepoznání k stabilizaci tréninku.","Pravda, Pravda","Nepravda, Nepravda","Pravda, Nepravda","Nepravda, Pravda",B
"Při vytváření modelu lineární regrese pro konkrétní datový soubor pozorujete, že koeficient jedné z proměnných má relativně vysokou zápornou hodnotu. To naznačuje, že",Tato funkce má silný vliv na model (měla by být zachována),Tato funkce nemá na model silný vliv (měla by být ignorována),Bez dalších informací není možné komentovat důležitost této funkce.,Nic nelze určit.,C
"Pro neuronovou síť, který z těchto strukturálních předpokladů nejvíce ovlivňuje kompromis mezi nedostatečným přizpůsobením (tj. model s vysokou systematickou chybou) a přetrénováním (tj. model s vysokou variancí):",Počet skrytých uzlů,Rychlost učení,Počáteční volba vah,Použití jednotkového vstupu s konstantním členem,A
"Pro polynomiální regresi, který z těchto strukturálních předpokladů nejvíce ovlivňuje kompromis mezi nedostatečným a přílišným přizpůsobením:",Stupeň polynomu,Ať už se učíme váhy pomocí inverze matice nebo gradientního sestupu,Předpokládaný rozptyl Gaussova šumu,Použití jednotkového vstupu s konstantním členem,A
Statement 1| K roku 2020 některé modely dosahují přesnosti vyšší než 98 % na CIFAR-10. Statement 2| Původní ResNety nebyly optimalizovány pomocí optimalizátoru Adam.,"Pravda, Pravda","Nepravda, Nepravda","Pravda, Nepravda","Nepravda, Pravda",A
Algoritmus K-means,"Vyžaduje, aby dimenze prostoru příznaků nebyla větší než počet vzorků","Má nejmenší hodnotu účelové funkce, když K = 1",Minimalizuje rozptyl uvnitř tříd pro daný počet shluků,"Konverguje ke globálnímu optimu tehdy a jen tehdy, pokud jsou počáteční průměry zvoleny jako některé ze samotných vzorků",C
Tvrzení 1| VGGNets mají konvoluční jádra s menší šířkou a výškou než jádra první vrstvy AlexNetu. Tvrzení 2| Postupy inicializace vah závislé na datech byly zavedeny před Dávkovou normalizací.,"Pravda, Pravda","Nepravda, Nepravda","Pravda, Nepravda","Nepravda, Pravda",A
"Jaká je hodnost následující matice? A = [[1, 1, 1], [1, 1, 1], [1, 1, 1]]",0,1,2,3,B
"Tvrzení 1| Odhad hustoty (například pomocí jádrového odhadu hustoty) lze použít k provádění klasifikace. Tvrzení 2| Korespondence mezi logistickou regresí a Gaussovým naivním Bayesem (s identickými kovariancemi tříd) znamená, že existuje vzájemně jednoznačná korespondence mezi parametry těchto dvou klasifikátorů.","Pravda, Pravda","Nepravda, Nepravda","Pravda, Nepravda","Nepravda, Pravda",C
"Předpokládejme, že bychom chtěli provést shlukování prostorových dat, jako jsou geometrické polohy domů. Chceme vytvořit shluky mnoha různých velikostí a tvarů. Která z následujících metod je nejvhodnější?",Rozhodovací stromy,Shlukování založené na hustotě,Shlukování založené na modelu,Shlukování metodou K-průměrů,B
Tvrzení 1| V AdaBoostu se váhy chybně klasifikovaných příkladů zvyšují o stejný multiplikativní faktor. Tvrzení 2| V AdaBoostu má vážená trénovací chyba e_t t-tého slabého klasifikátoru na trénovacích datech s váhami D_t tendenci růst jako funkce t.,"Pravda, Pravda","Nepravda, Nepravda","Pravda, Nepravda","Nepravda, Pravda",A
"Odhady MLE jsou často nežádoucí, protože",jsou předpojatí,mají vysokou variabilitu,nejsou konzistentní odhadci,Nic z výše uvedeného,B
"Výpočetní složitost gradientního sestupu je,",lineární v D,lineární v N,polynom v D,závislé na počtu iterací,C
Zprůměrování výstupu více rozhodovacích stromů pomáhá _.,Zvýšit předpojatost,Snížit předpojatost,Zvýšit rozptyl,Snížit rozptyl,D
Model získaný aplikací lineární regrese na identifikovanou podmnožinu vlastností se může lišit od modelu získaného na konci procesu identifikace podmnožiny během,Výběr nejlepší podmnožiny,Dopředná kroková selekce,Dopředná postupná selekce,Vše výše uvedené,C
Neuronové sítě,Optimalizovat konvexní účelovou funkci,Lze trénovat pouze pomocí stochastického gradientního sestupu,Lze použít kombinaci různých aktivačních funkcí,Nic z výše uvedeného,C
"Řekněme, že výskyt nemoci D je přibližně 5 případů na 100 lidí (tj. P(D) = 0,05). Nechť booleovská náhodná proměnná D znamená, že pacient ""má nemoc D"", a nechť booleovská náhodná proměnná TP znamená ""test je pozitivní"". Je známo, že testy na nemoc D jsou velmi přesné v tom smyslu, že pravděpodobnost pozitivního testu, když máte nemoc, je 0,99, a pravděpodobnost negativního testu, když nemoc nemáte, je 0,97. Jaká je P(TP), apriorní pravděpodobnost pozitivního testu?","0,0368","0,473","0,078",Nic z výše uvedeného,C
Tvrzení 1| Po mapování do prostoru příznaků Q pomocí radiální bázové jádrové funkce může 1-NN s použitím nevážené Euklidovské vzdálenosti dosáhnout lepší klasifikační výkonnosti než v původním prostoru (i když to nemůžeme zaručit). Tvrzení 2| VC dimenze Perceptronu je menší než VC dimenze jednoduchého lineárního SVM.,"Pravda, Pravda","Nepravda, Nepravda","Pravda, Nepravda","Nepravda, Pravda",B
Nevýhodou metody Grid search je,Nelze to aplikovat na nediferencovatelné funkce.,Nelze to aplikovat na nespojité funkce.,Je těžké to implementovat.,Běží poměrně pomalu pro vícenásobnou lineární regresi.,D
Předpovídání množství srážek v regionu na základě různých ukazatelů je ______ problém.,Učení s učitelem,Učení bez učitele,Shlukování,Nic z výše uvedeného,A
Která z následujících vět je NEPRAVDIVÁ ohledně regrese?,Vztahuje vstupy k výstupům.,Je to používáno pro předpověď.,Může být použito k interpretaci.,Objevuje příčinné vztahy,D
Který z následujících důvodů je hlavním důvodem pro prořezávání rozhodovacího stromu?,Pro úsporu výpočetního času během testování,Pro úsporu místa při ukládání rozhodovacího stromu,Aby se zmenšila chyba trénovací sady,Aby se zabránilo přetrénování trénovací sady,D
Tvrzení 1| Odhad jádrové hustoty je ekvivalentní provedení jádrové regrese s hodnotou Yi = 1/n v každém bodě Xi v původním datovém souboru. Tvrzení 2| Hloubka naučeného rozhodovacího stromu může být větší než počet trénovacích příkladů použitých k vytvoření stromu.,"Pravda, Pravda","Nepravda, Nepravda","Pravda, Nepravda","Nepravda, Pravda",B
"Předpokládejme, že váš model se přeučuje. Která z následujících možností NENÍ platným způsobem, jak se pokusit snížit přeučování?",Zvyšte množství trénovacích dat.,Vylepšete optimalizační algoritmus používaný pro minimalizaci chyb.,Snižte složitost modelu.,Snižte šum v trénovacích datech.,B
Tvrzení 1| Softmax funkce se běžně používá v multiclass logistické regresi. Tvrzení 2| Teplota nerovnoměrného softmax rozdělení ovlivňuje jeho entropii.,"Pravda, Pravda","Nepravda, Nepravda","Pravda, Nepravda","Nepravda, Pravda",A
Které z následujících tvrzení je/jsou pravdivé ohledně SVM (metody podpůrných vektorů)?,Pro dvourozměrné datové body bude dělicí nadrovina naučená lineárním SVM přímkou.,V teorii nemůže SVM s Gaussovým jádrem modelovat žádnou komplexní separační nadrovinu.,Pro každou jádrovou funkci použitou v SVM lze získat ekvivalentní uzavřenou formu rozšíření báze.,Přeučení v SVM není funkcí počtu podpůrných vektorů.,A
"Která z následujících možností je společná pravděpodobnost H, U, P a W popsaná danou Bayesovskou sítí H -> U <- P <- W? [poznámka: jako součin podmíněných pravděpodobností]","P(H, U, P, W) = P(H) * P(W) * P(P) * P(U)","P(H, U, P, W) = P(H) * P(W) * P(P | W) * P(W | H, P)","P(H, U, P, W) = P(H) * P(W) * P(P | W) * P(U | H, P)",Nic z výše uvedeného,C
"Tvrzení 1| Jelikož VC dimenze pro SVM s radiálním bázovým jádrem je nekonečná, takové SVM musí být horší než SVM s polynomiálním jádrem, které má konečnou VC dimenzi. Tvrzení 2| Dvouvrstvá neuronová síť s lineárními aktivačními funkcemi je v podstatě váženou kombinací lineárních separátorů, trénovaných na daném datasetu; algoritmus posilování postavený na lineárních separátorech také nachází kombinaci lineárních separátorů, proto tyto dva algoritmy dají stejný výsledek.","Pravda, Pravda","Nepravda, Nepravda","Pravda, Nepravda","Nepravda, Pravda",B
"Tvrzení 1| Algoritmus ID3 zaručeně najde optimální rozhodovací strom. Tvrzení 2| Uvažujme spojité rozdělení pravděpodobnosti s hustotou f(), která je všude nenulová. Pravděpodobnost hodnoty x se rovná f(x).","Pravda, Pravda","Nepravda, Nepravda","Pravda, Nepravda","Nepravda, Pravda",B
"Vzhledem k neuronové síti s N vstupními uzly, bez skrytých vrstev, s jedním výstupním uzlem, s entropickou ztrátovou funkcí a sigmoidními aktivačními funkcemi, který z následujících algoritmů (s vhodnými hyperparametry a inicializací) lze použít k nalezení globálního optima?",Stochastický gradientní sestup,Gradientní sestup s mini-dávkami,Dávkový gradientní sestup,Vše výše uvedené,D
"Přidání více bázových funkcí do lineárního modelu, vyberte nejpravděpodobnější možnost:",Snižuje zkreslení modelu,Snižuje zkreslení odhadu,Snižuje rozptyl,Nemá vliv na zkreslení a rozptyl,A
"Uvažujte Bayesovskou síť uvedenou níže. Kolik nezávislých parametrů bychom potřebovali, kdybychom nedělali žádné předpoklady o nezávislosti nebo podmíněné nezávislosti H -> U <- P <- W?",3,4,7,15,D
Další termín pro detekci mimo distribuci je?,detekce anomálií,detekce jedné třídy,odolnost vůči nesouladu mezi tréninkovými a testovacími daty,detekce pozadí,A
"Tvrzení 1| Učíme se klasifikátor f posílením slabých učících se algoritmů h. Funkční forma rozhodovací hranice f je stejná jako u h, ale s jinými parametry. (např. pokud h byl lineární klasifikátor, pak f je také lineární klasifikátor). Tvrzení 2| Křížová validace může být použita k výběru počtu iterací při posilování; tento postup může pomoci snížit přeučení.","Pravda, Pravda","Nepravda, Nepravda","Pravda, Nepravda","Nepravda, Pravda",D
Tvrzení 1| Dálniční sítě byly zavedeny po ResNets a vyhýbají se max poolingu ve prospěch konvolucí. Tvrzení 2| DenseNets obvykle stojí více paměti než ResNets.,"Pravda, Pravda","Nepravda, Nepravda","Pravda, Nepravda","Nepravda, Pravda",D
"Pokud N je počet instancí v tréninkovém datovém souboru, nejbližší sousedé mají klasifikační dobu běhu",O(1),O( N ),O(log N),O( N^2 ),B
"Tvrzení 1| Původní ResNets a Transformery jsou dopředné neuronové sítě. Tvrzení 2| Původní Transformery používají sebepoznání (self-attention), ale původní ResNet nikoliv.","Pravda, Pravda","Nepravda, Nepravda","Pravda, Nepravda","Nepravda, Pravda",A
"Tvrzení 1| ReLU funkce nejsou monotónní, ale sigmoidní funkce jsou monotónní. Tvrzení 2| Neuronové sítě trénované gradientním sestupem s vysokou pravděpodobností konvergují ke globálnímu optimu.","Pravda, Pravda","Nepravda, Nepravda","Pravda, Nepravda","Nepravda, Pravda",D
Číselný výstup sigmoidního uzlu v neuronové síti,"Je neomezený, zahrnující všechna reálná čísla.","Je neomezená, zahrnující všechna celá čísla.",Je ohraničeno mezi 0 a 1.,Je ohraničeno mezi -1 a 1.,C
"Kterou z následujících možností lze použít pouze v případě, že jsou trénovací data lineárně separovatelná?",Lineární SVM s tvrdou hranicí,Lineární logistická regrese,Lineární SVM s měkkým okrajem,Metoda centroidu.,A
Které z následujících jsou algoritmy prostorového shlukování?,Shlukování založené na rozdělování,Shlukování metodou K-průměrů,Shlukování založené na mřížce,Vše výše uvedené,D
"Tvrzení 1| Rozhodovací hranice s maximálním rozpětím, které konstruují support vector machines, mají nejnižší chybu generalizace mezi všemi lineárními klasifikátory. Tvrzení 2| Jakákoli rozhodovací hranice, kterou získáme z generativního modelu s podmíněnými gaussovskými distribucemi tříd, by v principu mohla být reprodukována pomocí SVM a polynomiálního jádra stupně menšího nebo rovného třem.","Pravda, Pravda","Nepravda, Nepravda","Pravda, Nepravda","Nepravda, Pravda",D
Tvrzení 1| L2 regularizace lineárních modelů má tendenci dělat modely řidší než L1 regularizace. Tvrzení 2| Reziduální spojení lze nalézt v ResNets a Transformerech.,"Pravda, Pravda","Nepravda, Nepravda","Pravda, Nepravda","Nepravda, Pravda",D
"Předpokládejme, že chceme vypočítat P(H|E, F) a nemáme žádné informace o podmíněné nezávislosti. Která z následujících sad čísel je dostačující pro tento výpočet?","P(E, F), P(H), P(E|H), P(F|H)","P(E, F), P(H), P(E, F|H)","P(H), P(E|H), P(F|H)","P(E, F), P(E|H), P(F|H)",B
Která z následujících možností zabraňuje přeučení při provádění baggingu?,Použití výběru s vracením jako techniky vzorkování,Použití slabých klasifikátorů,"Použití klasifikačních algoritmů, které nejsou náchylné k přeučení",Praxe validace prováděná na každém trénovaném klasifikátoru,B
"Tvrzení 1| PCA a spektrální shlukování (jako například Andrew Ngovo) provádějí rozklad vlastních čísel na dvou různých maticích. Velikost těchto dvou matic je však stejná. Tvrzení 2| Jelikož klasifikace je speciálním případem regrese, logistická regrese je speciálním případem lineární regrese.","Pravda, Pravda","Nepravda, Nepravda","Pravda, Nepravda","Nepravda, Pravda",B
"Tvrzení 1| Stanford Sentiment Treebank obsahoval recenze filmů, nikoli recenze knih. Tvrzení 2| Penn Treebank byl použit pro modelování jazyka.","Pravda, Pravda","Nepravda, Nepravda","Pravda, Nepravda","Nepravda, Pravda",A
"Jaká je dimenze nulového prostoru následující matice? A = [[3, 2, −9], [−6, −4, 18], [12, 8, −36]]",0,1,2,3,C
Co jsou podpůrné vektory?,Příklady nejvzdálenější od rozhodovací hranice.,Jediné příklady potřebné k výpočtu f(x) v SVM.,Datový centroid.,"Všechny příklady, které mají nenulovou váhu αk v SVM.",B
"Statement 1| Parametry Word2Vec nebyly inicializovány pomocí omezené Boltzmannovy mašiny.
Statement 2| Funkce tanh je nelineární aktivační funkce.","Pravda, Pravda","Nepravda, Nepravda","Pravda, Nepravda","Nepravda, Pravda",A
"Pokud se vaše tréninková ztráta zvyšuje s počtem epoch, který z následujících problémů by mohl být možným problémem v procesu učení?",Regularizace je příliš nízká a model se přeučuje,Regularizace je příliš vysoká a model je podtrénovaný,Velikost kroku je příliš velká,Velikost kroku je příliš malá,C
"Řekněme, že výskyt nemoci D je přibližně 5 případů na 100 lidí (tj. P(D) = 0,05). Nechť booleovská náhodná proměnná D znamená, že pacient ""má nemoc D"", a nechť booleovská náhodná proměnná TP znamená ""test je pozitivní"". Je známo, že testy na nemoc D jsou velmi přesné v tom smyslu, že pravděpodobnost pozitivního testu, když máte nemoc, je 0,99, a pravděpodobnost negativního testu, když nemoc nemáte, je 0,97. Jaká je P(D | TP), posteriorní pravděpodobnost, že máte nemoc D, když je test pozitivní?","0,0495","0,078","0,635","0,97",C
"Statement 1| Tradiční výsledky strojového učení předpokládají, že trénovací a testovací sady jsou nezávislé a identicky distribuované. Statement 2| V roce 2017 byly modely COCO obvykle předtrénovány na ImageNetu.","Pravda, Pravda","Nepravda, Nepravda","Pravda, Nepravda","Nepravda, Pravda",A
"Tvrzení 1| Hodnoty okrajů získané dvěma různými jádry K1(x, x0) a K2(x, x0) na stejné trénovací sadě nám neříkají, který klasifikátor bude mít lepší výkon na testovací sadě. Tvrzení 2| Aktivační funkcí BERT je GELU.","Pravda, Pravda","Nepravda, Nepravda","Pravda, Nepravda","Nepravda, Pravda",A
Který z následujících je shlukovací algoritmus ve strojovém učení?,Maximalizace očekávání,KOŠÍK,Gaussovský naivní Bayes,Apriori,A
"Právě jste dokončili trénink rozhodovacího stromu pro klasifikaci spamu a ten vykazuje abnormálně špatné výsledky jak na trénovacích, tak na testovacích sadách. Víte, že vaše implementace neobsahuje žádné chyby, takže co by mohlo být příčinou problému?",Vaše rozhodovací stromy jsou příliš mělké.,Musíte zvýšit rychlost učení.,Přetrénováváte.,Žádná z uvedených možností.,A
K-násobná křížová validace je,lineární v K,kvadratický v K,kubický v K,exponenciální v K,A
"Tvrzení 1| Neuronové sítě v průmyslovém měřítku jsou obvykle trénovány na procesorech (CPU), nikoli na grafických kartách (GPU). Tvrzení 2| Model ResNet-50 má přes 1 miliardu parametrů.","Pravda, Pravda","Nepravda, Nepravda","Pravda, Nepravda","Nepravda, Pravda",B
"Jsou dány dvě booleovské náhodné proměnné, A a B, kde P(A) = 1/2, P(B) = 1/3 a P(A | ¬B) = 1/4. Jaká je hodnota P(A | B)?",1/6,1/4,3/4,1,D
Existenciální rizika spojená s umělou inteligencí jsou nejčastěji spojována s kterým z následujících profesorů?,Nando de Frietas,Yann LeCun,Stuart Russell,Jitendra Malik,C
"Tvrzení 1| Maximalizace věrohodnosti modelu logistické regrese vede k několika lokálním optimům. Tvrzení 2| Žádný klasifikátor nemůže být lepší než naivní Bayesův klasifikátor, pokud je známo rozdělení dat.","Pravda, Pravda","Nepravda, Nepravda","Pravda, Nepravda","Nepravda, Pravda",B
"Pro jádrovou regresi, který z těchto strukturálních předpokladů nejvíce ovlivňuje kompromis mezi nedostatečným a přílišným přizpůsobením:",Zda je jádrová funkce gaussovská versus trojúhelníková versus krabicová,Zda používáme euklidovské versus L1 versus L∞ metriky,Šířka jádra,Maximální výška jádrové funkce,C
Tvrzení 1| Učící algoritmus SVM zaručeně najde globálně optimální hypotézu vzhledem ke své objektové funkci. Tvrzení 2| Po mapování do prostoru příznaků Q pomocí radiální bázové jádrové funkce může Perceptron dosáhnout lepšího klasifikačního výkonu než v původním prostoru (i když to nemůžeme zaručit).,"Pravda, Pravda","Nepravda, Nepravda","Pravda, Nepravda","Nepravda, Pravda",A
"Pro gaussovský bayesovský klasifikátor, který z těchto strukturálních předpokladů nejvíce ovlivňuje kompromis mezi podtrénováním a přetrénováním:",Zda se učíme středy tříd pomocí maximální věrohodnosti nebo gradientního sestupu,Ať už předpokládáme úplné kovarianční matice tříd nebo diagonální kovarianční matice tříd,Zda máme rovnoměrné apriorní pravděpodobnosti tříd nebo apriorní pravděpodobnosti odhadnuté z dat.,"Zda povolíme třídám mít různé vektory průměrů, nebo je přinutíme sdílet stejný vektor průměru",B
"Tvrzení 1| Přeučení je pravděpodobnější, když je soubor trénovacích dat malý. Tvrzení 2| Přeučení je pravděpodobnější, když je prostor hypotéz malý.","Pravda, Pravda","Nepravda, Nepravda","Pravda, Nepravda","Nepravda, Pravda",D
Statement 1| Kromě EM lze k provádění inference nebo učení na modelu Gaussovské směsi použít gradientní sestup. Statement 2 | Za předpokladu pevného počtu atributů lze Gaussovský optimální Bayesův klasifikátor naučit v čase lineárním vzhledem k počtu záznamů v datové sadě.,"Pravda, Pravda","Nepravda, Nepravda","Pravda, Nepravda","Nepravda, Pravda",A
"Tvrzení 1| V Bayesovské síti jsou výsledky inference algoritmu spojovacího stromu stejné jako výsledky inference eliminace proměnných. Tvrzení 2| Pokud jsou dvě náhodné proměnné X a Y podmíněně nezávislé vzhledem k jiné náhodné proměnné Z, pak v odpovídající Bayesovské síti jsou uzly pro X a Y d-separované vzhledem k Z.","Pravda, Pravda","Nepravda, Nepravda","Pravda, Nepravda","Nepravda, Pravda",C
"Máme-li k dispozici rozsáhlý soubor dat lékařských záznamů pacientů trpících srdečním onemocněním, pokusme se zjistit, zda by mohly existovat různé skupiny takových pacientů, pro které bychom mohli přizpůsobit samostatné léčebné postupy. O jaký typ učebního problému se jedná?",Učení s učitelem,Učení bez učitele,Obojí (a) i (b),"Ani (a), ani (b)",B
"Co byste udělali v PCA, abyste získali stejnou projekci jako u SVD?",Transformovat data na nulový průměr,Transformovat data na nulový medián,Není možné,Žádný z nich,A
"Tvrzení 1| Chyba trénování klasifikátoru 1-nejbližšího souseda je 0. Tvrzení 2| Jak počet datových bodů roste do nekonečna, odhad MAP se blíží odhadu MLE pro všechny možné apriorní pravděpodobnosti. Jinými slovy, při dostatečném množství dat je volba apriorní pravděpodobnosti irelevantní.","Pravda, Pravda","Nepravda, Nepravda","Pravda, Nepravda","Nepravda, Pravda",C
"Při provádění regrese metodou nejmenších čtverců s regularizací (za předpokladu, že optimalizaci lze provést přesně), zvýšení hodnoty regularizačního parametru λ testovací chybu.",nikdy nesníží chybu při tréninku.,nikdy nezvýší chybu při tréninku.,nikdy nesníží chybu testování.,nikdy se nezvýší,A
"Která z následujících možností nejlépe popisuje, co se snaží modelovat diskriminativní přístupy? (w jsou parametry v modelu)","p(y|x, w)","p(y, x)","p(w|x, w)",Nic z výše uvedeného,A
"Tvrzení 1| Výkon klasifikace CIFAR-10 pro konvoluční neuronové sítě může přesáhnout 95 %. Tvrzení 2| Soubory neuronových sítí nezlepšují přesnost klasifikace, protože reprezentace, které se učí, jsou vysoce korelované.","Pravda, Pravda","Nepravda, Nepravda","Pravda, Nepravda","Nepravda, Pravda",C
O kterém z následujících bodů by se bayesiáni a frekventisté neshodli?,Použití negaussovského modelu šumu v pravděpodobnostní regresi.,Použití pravděpodobnostního modelování pro regresi.,Použití apriorních rozdělení parametrů v pravděpodobnostním modelu.,Použití apriorních pravděpodobností tříd v Gaussovské diskriminační analýze.,C
"Tvrzení 1| Metrika BLEU používá přesnost, zatímco metrika ROGUE používá úplnost. Tvrzení 2| Skryté Markovovy modely byly často používány k modelování anglických vět.","Pravda, Pravda","Nepravda, Nepravda","Pravda, Nepravda","Nepravda, Pravda",A
Tvrzení 1| ImageNet obsahuje obrázky různých rozlišení. Tvrzení 2| Caltech-101 má více obrázků než ImageNet.,"Pravda, Pravda","Nepravda, Nepravda","Pravda, Nepravda","Nepravda, Pravda",C
Která z následujících možností je vhodnější pro výběr příznaků?,Hřeben,Laso,"jak (a), tak (b)",ani (a) ani (b),B
"Předpokládejme, že máte k dispozici EM algoritmus, který nachází odhady maximální věrohodnosti pro model se skrytými proměnnými. Jste požádáni, abyste upravili algoritmus tak, aby místo toho nacházel MAP odhady. Který krok nebo kroky musíte upravit?",Očekávání,Maximalizace,Žádná úprava není nutná,Obojí,B
"Pro gaussovský Bayesův klasifikátor, který z těchto strukturálních předpokladů nejvíce ovlivňuje kompromis mezi podtrénováním a přetrénováním:",Zda se naučíme středy tříd pomocí maximální věrohodnosti nebo gradientního sestupu,Ať už předpokládáme úplné kovarianční matice tříd nebo diagonální kovarianční matice tříd,Zda máme rovnoměrné apriorní pravděpodobnosti tříd nebo apriorní pravděpodobnosti odhadnuté z dat,"Zda povolíme třídám mít různé vektory průměrů, nebo je přinutíme sdílet stejný vektor průměru",B
"Tvrzení 1| Pro libovolné dvě proměnné x a y se společným rozdělením p(x, y) vždy platí H[x, y] ≥ H[x] + H[y], kde H je funkce entropie. Tvrzení 2| Pro některé orientované grafy moralizace snižuje počet hran přítomných v grafu.","Pravda, Pravda","Nepravda, Nepravda","Pravda, Nepravda","Nepravda, Pravda",B
Která z následujících možností NENÍ učení s učitelem?,PCA,Rozhodovací strom,Lineární regrese,Naivní bayesovský,A
Tvrzení 1| Konvergence neuronové sítě závisí na rychlosti učení. Tvrzení 2| Dropout náhodně násobí vybrané hodnoty aktivace nulou.,"Pravda, Pravda","Nepravda, Nepravda","Pravda, Nepravda","Nepravda, Pravda",A
"Která z následujících možností se rovná P(A, B, C) za předpokladu, že A, B a C jsou booleovské náhodné proměnné, a mezi nimi neexistují žádné předpoklady nezávislosti nebo podmíněné nezávislosti?",P(A | B) * P(B | C) * P(C | A),"P(C | A, B) * P(A) * P(B)","P(A, B | C) * P(C)","P(A | B, C) * P(B | A, C) * P(C | A, B)",C
Který z následujících úkolů lze nejlépe vyřešit pomocí shlukování.,Předpovídání množství srážek na základě různých ukazatelů,Odhalování podvodných transakcí s kreditními kartami,Trénování robota k řešení bludiště,Vše výše uvedené,B
"Po aplikaci regularizační penalizace v lineární regresi zjistíte, že některé koeficienty w jsou vynulovány. Která z následujících penalizací mohla být použita?",L0 norma,L1 norma,L2 norma,buď (a) nebo (b),D
"A a B jsou dvě události. Pokud se P(A, B) snižuje, zatímco P(A) se zvyšuje, která z následujících možností je pravdivá?",P(A|B) klesá,P(B|A) klesá,P(B) klesá,Vše výše uvedené,B
"Tvrzení 1| Při učení HMM pro pevnou sadu pozorování, za předpokladu, že neznáme skutečný počet skrytých stavů (což je často případ), můžeme vždy zvýšit pravděpodobnost trénovacích dat tím, že povolíme více skrytých stavů. Tvrzení 2| Kolaborativní filtrování je často užitečným modelem pro modelování filmových preferencí uživatelů.","Pravda, Pravda","Nepravda, Nepravda","Pravda, Nepravda","Nepravda, Pravda",A
"Trénujete model lineární regrese pro jednoduchý odhadovací úkol a všimnete si, že model se příliš přizpůsobuje datům. Rozhodnete se přidat $\ell_2$ regularizaci k penalizaci vah. Co se stane s vychýlením (bias) a rozptylem (variance) modelu, když zvýšíte koeficient $\ell_2$ regularizace?",Zvýšení zkreslení; Zvýšení rozptylu,Zkreslení se zvyšuje; rozptyl se snižuje,Zkreslení se snižuje; Rozptyl se zvyšuje,Snížení zkreslení ; Snížení rozptylu,B
"Který příkaz (nebo příkazy) PyTorch 1.8 vytvoří Gaussovskou matici $10\times 5$ s každým prvkem nezávisle vzorkovaným z $\mathcal{N}(\mu=5,\sigma^2=16)$ a uniformní matici $10\times 10$ s každým prvkem nezávisle vzorkovaným z $U[-1,1)$?","\texttt{5 + torch.randn(10,5) * 16} ; \texttt{torch.rand(10,10,low=-1,high=1)}","\texttt{5 + torch.randn(10,5) * 16} ; \texttt{(torch.rand(10,10) - 0,5) / 0,5}","\texttt{5 + torch.randn(10,5) * 4} ; \texttt{2 * torch.rand(10,10) - 1}","\texttt{torch.normal(torch.ones(10,5)*5,torch.ones(5,5)*16)} ; \texttt{2 * torch.rand(10,10) - 1}",C
Tvrzení 1| Gradient ReLU je nulový pro $x<0$ a gradient sigmoidy $\sigma(x)(1-\sigma(x))\le \frac{1}{4}$ pro všechna $x$. Tvrzení 2| Sigmoida má spojitý gradient a ReLU má nespojitý gradient.,"Pravda, Pravda","Nepravda, Nepravda","Pravda, Nepravda","Nepravda, Pravda",A
Která z následujících možností je pravdivá o dávkové normalizaci?,Po aplikaci dávkové normalizace budou aktivace vrstvy následovat standardní Gaussovo rozdělení.,"Parametr zkreslení (bias) afinních vrstev se stává nadbytečným, pokud bezprostředně následuje vrstva normalizace dávky (batch normalization).",Při použití normalizace dávek je nutné změnit standardní inicializaci vah.,Dávková normalizace je ekvivalentní k vrstvové normalizaci pro konvoluční neuronové sítě.,B
"Předpokládejme, že máme následující účelovou funkci: $\argmin_{w} \frac{1}{2} \norm{Xw-y}^2_2 + \frac{1}{2}\gamma \norm{w}^2_2$ Jaký je gradient $\frac{1}{2} \norm{Xw-y}^2_2 + \frac{1}{2}\lambda \norm{w}^2_2$ vzhledem k $w$?",$\nabla_w f(w) = (X^\top X + \lambda I)w - X^\top y + \lambda w$,$\nabla_w f(w) = X^\top X w - X^\top y + \lambda$,$\nabla_w f(w) = X^\top X w - X^\top y + \lambda w$,$\nabla_w f(w) = X^\top X w - X^\top y + (\lambda+1) w$,C
Která z následujících možností platí pro konvoluční jádro?,Konvoluce obrazu s $\begin{bmatrix}1 & 0 & 0\\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix}$ by obraz nezměnila,Konvoluce obrazu s $\begin{bmatrix}0 & 0 & 0\\ 0 & 1 & 0 \\ 0 & 0 & 0 \end{bmatrix}$ by obraz nezměnila,Konvoluce obrazu s $\begin{bmatrix}1 & 1 & 1\\ 1 & 1 & 1 \\ 1 & 1 & 1 \end{bmatrix}$ by obraz nezměnila,Konvoluce obrazu s $\begin{bmatrix}0 & 0 & 0\\ 0 & 0 & 0 \\ 0 & 0 & 0 \end{bmatrix}$ by obraz nezměnila,B
Která z následujících možností je nepravdivá?,"Modely sémantické segmentace předpovídají třídu každého pixelu, zatímco víceúrovňové klasifikátory obrazů předpovídají třídu celého obrazu.",Ohraničující rámeček s IoU (průnik nad sjednocením) rovným 96 % by byl pravděpodobně považován za skutečně pozitivní.,"Když předpovězený ohraničující rámeček neodpovídá žádnému objektu ve scéně, je považován za falešně pozitivní.",Ohraničující rámeček s IoU (průnik nad sjednocením) rovným 3 % by byl pravděpodobně považován za falešně negativní.,D
Která z následujících možností je nepravdivá?,"Následující plně propojená síť bez aktivačních funkcí je lineární: $g_3(g_2(g_1(x)))$, kde $g_i(x) = W_i x$ a $W_i$ jsou matice.","Prosakující ReLU $\max\{0.01x,x\}$ je konvexní.","Kombinace ReLU funkcí, jako je $ReLU(x) - ReLU(x-1)$, je konvexní.",Ztráta $\log \sigma(x)= -\log(1+e^{-x})$ je konkávní,C
"Trénujeme plně propojenou síť se dvěma skrytými vrstvami pro předpověď cen nemovitostí. Vstupy jsou 100-dimenzionální a mají několik charakteristik, jako je počet čtverečních stop, medián rodinného příjmu atd. První skrytá vrstva má 1000 aktivací. Druhá skrytá vrstva má 10 aktivací. Výstup je skalár představující cenu domu. Za předpokladu, že jde o běžnou síť s afinními transformacemi bez normalizace dávek a bez naučitelných parametrů v aktivační funkci, kolik parametrů má tato síť?",111021,110010,111110,110011,A
"Tvrzení 1| Derivace sigmoidy $\sigma(x)=(1+e^{-x})^{-1}$ vzhledem k $x$ se rovná $\text{Var}(B)$, kde $B\sim \text{Bern}(\sigma(x))$ je Bernoulliho náhodná proměnná. Tvrzení 2| Nastavení parametrů zkreslení v každé vrstvě neuronové sítě na 0 mění kompromis mezi zkreslením a rozptylem tak, že rozptyl modelu se zvyšuje a zkreslení modelu se snižuje.","Pravda, Pravda","Nepravda, Nepravda","Pravda, Nepravda","Nepravda, Pravda",C
