Énoncé 1| L'estimateur de régression linéaire a la plus petite variance parmi tous les estimateurs non biaisés. Énoncé 2| Les coefficients α attribués aux classificateurs assemblés par AdaBoost sont toujours non négatifs.,"Vrai, Vrai","Faux, Faux","Vrai, Faux","Faux, Vrai",D
Déclaration 1| RoBERTa effectue un pré-entraînement sur un corpus environ 10 fois plus grand que celui sur lequel BERT a été pré-entraîné. Déclaration 2| Les ResNeXts en 2018 utilisaient généralement des fonctions d'activation tanh.,"Vrai, Vrai","Faux, Faux","Vrai, Faux","Faux, Vrai",C
"Déclaration 1| Les machines à vecteurs de support, comme les modèles de régression logistique, donnent une distribution de probabilité sur les étiquettes possibles pour un exemple d'entrée donné. Déclaration 2| Nous nous attendrions à ce que les vecteurs de support restent généralement les mêmes lorsque nous passons d'un noyau linéaire à des noyaux polynomiaux d'ordre supérieur.","Vrai, Vrai","Faux, Faux","Vrai, Faux","Faux, Vrai",B
"Un problème d'apprentissage automatique implique quatre attributs plus une classe. Les attributs ont respectivement 3, 2, 2 et 2 valeurs possibles. La classe a 3 valeurs possibles. Combien y a-t-il au maximum d'exemples différents possibles ?",12,24,48,72,D
"En 2020, quelle architecture est la meilleure pour classifier des images à haute résolution ?",réseaux convolutifs,réseaux de graphes,réseaux entièrement connectés,Réseaux RBF,A
Énoncé 1| La log-vraisemblance des données augmentera toujours au fil des itérations successives de l'algorithme d'espérance-maximisation. Énoncé 2| Un inconvénient du Q-learning est qu'il ne peut être utilisé que lorsque l'apprenant a une connaissance préalable de la façon dont ses actions affectent son environnement.,"Vrai, Vrai","Faux, Faux","Vrai, Faux","Faux, Vrai",B
Disons que nous avons calculé le gradient de notre fonction de coût et l'avons stocké dans un vecteur g. Quel est le coût d'une mise à jour de descente de gradient étant donné le gradient ?,O(D),O(N),O(ND),O(ND^2),A
"Énoncé 1| Pour une variable aléatoire continue x et sa fonction de densité de probabilité p(x), il est vrai que 0 ≤ p(x) ≤ 1 pour tout x. Énoncé 2| L'arbre de décision est appris en minimisant le gain d'information.","Vrai, Vrai","Faux, Faux","Vrai, Faux","Faux, Vrai",B
Considérez le réseau bayésien donné ci-dessous. Combien de paramètres indépendants sont nécessaires pour ce réseau bayésien H -> U <- P <- W ?,2,4,8,16,C
"À mesure que le nombre d'exemples d'entraînement tend vers l'infini, votre modèle entraîné sur ces données aura :",Variance plus faible,Variance plus élevée,Même variance,Aucune des réponses ci-dessus,A
Énoncé 1| L'ensemble de tous les rectangles dans le plan 2D (qui inclut les rectangles non alignés sur les axes) peut pulvériser un ensemble de 5 points. Énoncé 2| La dimension VC du classificateur des k plus proches voisins lorsque k = 1 est infinie.,"Vrai, Vrai","Faux, Faux","Vrai, Faux","Faux, Vrai",A
_ fait référence à un modèle qui ne peut ni modéliser les données d'entraînement ni généraliser à de nouvelles données.,bien ajusté,surapprentissage,sous-apprentissage,tout ce qui précède,C
Statement 1| Le score F1 peut être particulièrement utile pour les ensembles de données présentant un fort déséquilibre de classes. Statement 2| L'aire sous la courbe ROC est l'une des principales métriques utilisées pour évaluer les détecteurs d'anomalies.,"Vrai, Vrai","Faux, Faux","Vrai, Faux","Faux, Vrai",A
"Énoncé 1| L'algorithme de rétropropagation apprend un réseau neuronal globalement optimal avec des couches cachées. Énoncé 2| La dimension VC d'une ligne devrait être au plus 2, car je peux trouver au moins un cas de 3 points qui ne peuvent être pulvérisés par aucune ligne.","Vrai, Vrai","Faux, Faux","Vrai, Faux","Faux, Vrai",B
Une entropie élevée signifie que les partitions dans la classification sont,pur,pas pur,utile,inutile,B
"Déclaration 1| La normalisation de couche est utilisée dans l'article original sur ResNet, pas la normalisation par lots. Déclaration 2| Les DCGANs utilisent l'auto-attention pour stabiliser l'entraînement.","Vrai, Vrai","Faux, Faux","Vrai, Faux","Faux, Vrai",B
"Dans la construction d'un modèle de régression linéaire pour un ensemble de données particulier, vous observez que le coefficient de l'une des caractéristiques a une valeur négative relativement élevée. Cela suggère que",Cette fonctionnalité a un effet important sur le modèle (doit être conservée),Cette fonctionnalité n'a pas d'effet important sur le modèle (devrait être ignorée),Il n'est pas possible de commenter l'importance de cette fonctionnalité sans informations supplémentaires,Rien ne peut être déterminé.,C
"Pour un réseau neuronal, laquelle de ces hypothèses structurelles est celle qui affecte le plus le compromis entre le sous-ajustement (c'est-à-dire un modèle à biais élevé) et le surajustement (c'est-à-dire un modèle à variance élevée) :",Le nombre de nœuds cachés,Le taux d'apprentissage,Le choix initial des poids,L'utilisation d'une entrée unitaire à terme constant,A
"Pour la régression polynomiale, laquelle de ces hypothèses structurelles est celle qui affecte le plus le compromis entre sous-ajustement et surajustement :",Le degré du polynôme,Que nous apprenions les poids par inversion de matrice ou par descente de gradient,La variance supposée du bruit gaussien,L'utilisation d'une entrée unitaire à terme constant,A
"Statement 1| En 2020, certains modèles atteignent une précision supérieure à 98 % sur CIFAR-10. Statement 2| Les ResNets originaux n'étaient pas optimisés avec l'optimiseur Adam.","Vrai, Vrai","Faux, Faux","Vrai, Faux","Faux, Vrai",A
L'algorithme des K-moyennes,Nécessite que la dimension de l'espace des caractéristiques ne soit pas plus grande que le nombre d'échantillons,A la plus petite valeur de la fonction objectif lorsque K = 1,Minimise la variance intra-classe pour un nombre donné de clusters,Converge vers l'optimum global si et seulement si les moyennes initiales sont choisies comme étant certains des échantillons eux-mêmes,C
Déclaration 1| Les VGGNets ont des noyaux de convolution de largeur et de hauteur plus petites que les noyaux de la première couche d'AlexNet. Déclaration 2| Les procédures d'initialisation des poids dépendantes des données ont été introduites avant la normalisation par lots.,"Vrai, Vrai","Faux, Faux","Vrai, Faux","Faux, Vrai",A
"Quel est le rang de la matrice suivante ? A = [[1, 1, 1], [1, 1, 1], [1, 1, 1]]",0,1,2,3,B
Énoncé 1| L'estimation de densité (en utilisant par exemple l'estimateur de densité par noyau) peut être utilisée pour effectuer une classification. Énoncé 2| La correspondance entre la régression logistique et le Naïve Bayes gaussien (avec des covariances de classe identiques) signifie qu'il existe une correspondance biunivoque entre les paramètres des deux classificateurs.,"Vrai, Vrai","Faux, Faux","Vrai, Faux","Faux, Vrai",C
Supposons que nous souhaitions effectuer un regroupement sur des données spatiales telles que les emplacements géométriques des maisons. Nous voulons produire des groupes de tailles et de formes très différentes. Laquelle des méthodes suivantes est la plus appropriée ?,Arbres de décision,Clustering basé sur la densité,Clustering basé sur des modèles,Partitionnement en K-moyennes,B
"Énoncé 1| Dans AdaBoost, les poids des exemples mal classés augmentent du même facteur multiplicatif. Énoncé 2| Dans AdaBoost, l'erreur d'entraînement pondérée e_t du t-ième classificateur faible sur les données d'entraînement avec les poids D_t tend à augmenter en fonction de t.","Vrai, Vrai","Faux, Faux","Vrai, Faux","Faux, Vrai",A
Les estimations du maximum de vraisemblance sont souvent indésirables car,ils sont biaisés,ils ont une variance élevée,ce ne sont pas des estimateurs convergents,Aucune des réponses ci-dessus,B
"La complexité computationnelle de la descente de gradient est,",linéaire en D,linéaire en N,polynôme en D,dépendant du nombre d'itérations,C
Faire la moyenne de la sortie de plusieurs arbres de décision aide _.,Augmenter le biais,Réduire les préjugés,Augmenter la variance,Réduire la variance,D
Le modèle obtenu en appliquant une régression linéaire sur le sous-ensemble de caractéristiques identifié peut différer du modèle obtenu à la fin du processus d'identification du sous-ensemble pendant,Sélection du meilleur sous-ensemble,Sélection progressive ascendante,Sélection progressive par étapes,Tout ce qui précède,C
Réseaux de neurones,Optimiser une fonction objectif convexe,Ne peut être entraîné qu'avec la descente de gradient stochastique,Peut utiliser un mélange de différentes fonctions d'activation,Aucune des réponses ci-dessus,C
"Disons que l'incidence d'une maladie D est d'environ 5 cas pour 100 personnes (c'est-à-dire, P(D) = 0,05). Soit la variable aléatoire booléenne D signifiant qu'un patient ""a la maladie D"" et soit la variable aléatoire booléenne TP signifiant ""test positif"". Les tests pour la maladie D sont connus pour être très précis dans le sens où la probabilité d'avoir un test positif quand on a la maladie est de 0,99, et la probabilité d'avoir un test négatif quand on n'a pas la maladie est de 0,97. Quelle est P(TP), la probabilité a priori d'avoir un test positif.","0,0368","0,473","0,078",Aucune des réponses ci-dessus,C
"Énoncé 1| Après avoir été projeté dans l'espace de caractéristiques Q par une fonction noyau à base radiale, le 1-NN utilisant la distance euclidienne non pondérée peut être en mesure d'obtenir de meilleures performances de classification que dans l'espace d'origine (bien que nous ne puissions pas le garantir). Énoncé 2| La dimension VC d'un Perceptron est plus petite que la dimension VC d'un SVM linéaire simple.","Vrai, Vrai","Faux, Faux","Vrai, Faux","Faux, Vrai",B
L'inconvénient de la recherche par grille est,Il ne peut pas être appliqué aux fonctions non différentiables.,Il ne peut pas être appliqué aux fonctions discontinues.,C'est difficile à mettre en œuvre.,Il fonctionne relativement lentement pour la régression linéaire multiple.,D
Prédire la quantité de précipitations dans une région en fonction de divers indices est un problème ______.,Apprentissage supervisé,Apprentissage non supervisé,Regroupement,Aucune des réponses ci-dessus,A
Laquelle des phrases suivantes est FAUSSE concernant la régression ?,Il relie les entrées aux sorties.,Il est utilisé pour la prédiction.,Il peut être utilisé pour l'interprétation.,Il découvre des relations causales,D
Laquelle des raisons suivantes est la principale pour élaguer un arbre de décision ?,Pour économiser du temps de calcul pendant les tests,Pour économiser de l'espace pour stocker l'Arbre de Décision,Pour réduire l'erreur de l'ensemble d'entraînement,Pour éviter le surajustement de l'ensemble d'entraînement,D
Énoncé 1| L'estimateur de densité par noyau est équivalent à effectuer une régression par noyau avec la valeur Yi = 1/n à chaque point Xi dans l'ensemble de données original. Énoncé 2| La profondeur d'un arbre de décision appris peut être supérieure au nombre d'exemples d'entraînement utilisés pour créer l'arbre.,"Vrai, Vrai","Faux, Faux","Vrai, Faux","Faux, Vrai",B
Supposons que votre modèle soit en surapprentissage. Laquelle des options suivantes N'EST PAS une méthode valide pour essayer de réduire le surapprentissage ?,Augmentez la quantité de données d'entraînement.,Améliorer l'algorithme d'optimisation utilisé pour la minimisation des erreurs.,Réduisez la complexité du modèle.,Réduisez le bruit dans les données d'entraînement.,B
Énoncé 1| La fonction softmax est couramment utilisée dans la régression logistique multiclasse. Énoncé 2| La température d'une distribution softmax non uniforme affecte son entropie.,"Vrai, Vrai","Faux, Faux","Vrai, Faux","Faux, Vrai",A
Laquelle ou lesquelles des affirmations suivantes est/sont vraie(s) concernant un SVM ?,"Pour des points de données bidimensionnels, l'hyperplan séparateur appris par un SVM linéaire sera une ligne droite.","En théorie, un SVM à noyau gaussien ne peut pas modéliser n'importe quelle hyperplan séparateur complexe.","Pour chaque fonction noyau utilisée dans un SVM, on peut obtenir une expansion de base équivalente sous forme fermée.",Le surapprentissage dans un SVM n'est pas une fonction du nombre de vecteurs de support.,A
"Laquelle des propositions suivantes est la probabilité conjointe de H, U, P et W décrite par le réseau bayésien donné H -> U <- P <- W ? [note : comme le produit des probabilités conditionnelles]","P(H, U, P, W) = P(H) * P(W) * P(P) * P(U)","P(H, U, P, W) = P(H) * P(W) * P(P | W) * P(W | H, P)","P(H, U, P, W) = P(H) * P(W) * P(P | W) * P(U | H, P)",Aucune des réponses ci-dessus,C
"Énoncé 1| Étant donné que la dimension VC pour un SVM avec un noyau à base radiale est infinie, un tel SVM doit être moins performant qu'un SVM avec un noyau polynomial qui a une dimension VC finie. Énoncé 2| Un réseau de neurones à deux couches avec des fonctions d'activation linéaires est essentiellement une combinaison pondérée de séparateurs linéaires, entraînée sur un ensemble de données donné ; l'algorithme de boosting construit sur des séparateurs linéaires trouve également une combinaison de séparateurs linéaires, par conséquent ces deux algorithmes donneront le même résultat.","Vrai, Vrai","Faux, Faux","Vrai, Faux","Faux, Vrai",B
Statement 1| L'algorithme ID3 est garanti de trouver l'arbre de décision optimal. Statement 2| Considérons une distribution de probabilité continue avec une densité f() qui est non nulle partout. La probabilité d'une valeur x est égale à f(x).,"Vrai, Vrai","Faux, Faux","Vrai, Faux","Faux, Vrai",B
"Étant donné un réseau neuronal avec N nœuds d'entrée, sans couches cachées, un nœud de sortie, avec une fonction de perte d'entropie et des fonctions d'activation sigmoïde, lequel des algorithmes suivants (avec les bons hyper-paramètres et une initialisation appropriée) peut être utilisé pour trouver l'optimum global ?",Descente de gradient stochastique,Descente de gradient par mini-lots,Descente de gradient par lots,Tout ce qui précède,D
"Ajouter plus de fonctions de base dans un modèle linéaire, choisissez l'option la plus probable :",Diminue le biais du modèle,Diminue le biais d'estimation,Diminue la variance,N'affecte pas le biais et la variance,A
Considérez le réseau bayésien donné ci-dessous. Combien de paramètres indépendants aurions-nous besoin si nous ne faisions aucune hypothèse sur l'indépendance ou l'indépendance conditionnelle H -> U <- P <- W ?,3,4,7,15,D
Un autre terme pour la détection hors distribution est ?,détection d'anomalies,détection à une classe,robustesse face au décalage entre l'entraînement et le test,détection d'arrière-plan,A
"Énoncé 1| Nous apprenons un classificateur f en boostant des apprenants faibles h. La forme fonctionnelle de la frontière de décision de f est la même que celle de h, mais avec des paramètres différents. (par exemple, si h était un classificateur linéaire, alors f est aussi un classificateur linéaire). Énoncé 2| La validation croisée peut être utilisée pour sélectionner le nombre d'itérations dans le boosting ; cette procédure peut aider à réduire le surapprentissage.","Vrai, Vrai","Faux, Faux","Vrai, Faux","Faux, Vrai",D
Déclaration 1| Les réseaux autoroutiers ont été introduits après les ResNets et évitent le max pooling au profit des convolutions. Déclaration 2| Les DenseNets coûtent généralement plus de mémoire que les ResNets.,"Vrai, Vrai","Faux, Faux","Vrai, Faux","Faux, Vrai",D
"Si N est le nombre d'instances dans l'ensemble de données d'entraînement, les plus proches voisins ont un temps d'exécution de classification de",O(1),O( N ),O(log N),O( N^2 ),B
"Statement 1| Les ResNets et les Transformers originaux sont des réseaux de neurones à propagation avant.

Statement 2| Les Transformers originaux utilisent l'auto-attention, mais le ResNet original ne l'utilise pas.","Vrai, Vrai","Faux, Faux","Vrai, Faux","Faux, Vrai",A
"Énoncé 1| Les RELUs ne sont pas monotones, mais les sigmoïdes sont monotones. Énoncé 2| Les réseaux de neurones entraînés avec la descente de gradient convergent avec une forte probabilité vers l'optimum global.","Vrai, Vrai","Faux, Faux","Vrai, Faux","Faux, Vrai",D
La sortie numérique d'un nœud sigmoïde dans un réseau neuronal :,"Est non borné, englobant tous les nombres réels.","Est illimité, englobant tous les entiers.",Est borné entre 0 et 1.,Est borné entre -1 et 1.,C
Lequel des éléments suivants ne peut être utilisé que lorsque les données d'entraînement sont linéairement séparables ?,SVM linéaire à marge dure,Régression logistique linéaire,SVM à marge souple linéaire,La méthode du centroïde.,A
"Parmi les suivants, quels sont les algorithmes de regroupement spatial ?",Clustering basé sur le partitionnement,Partitionnement en K-moyennes,Regroupement basé sur une grille,Tout ce qui précède,D
Énoncé 1| Les frontières de décision à marge maximale que construisent les machines à vecteurs de support ont la plus faible erreur de généralisation parmi tous les classificateurs linéaires. Énoncé 2| Toute frontière de décision que nous obtenons à partir d'un modèle génératif avec des distributions gaussiennes conditionnelles de classe pourrait en principe être reproduite avec une SVM et un noyau polynomial de degré inférieur ou égal à trois.,"Vrai, Vrai","Faux, Faux","Vrai, Faux","Faux, Vrai",D
Déclaration 1| La régularisation L2 des modèles linéaires tend à rendre les modèles plus épars que la régularisation L1. Déclaration 2| Les connexions résiduelles peuvent être trouvées dans les ResNets et les Transformers.,"Vrai, Vrai","Faux, Faux","Vrai, Faux","Faux, Vrai",D
"Supposons que nous voulions calculer P(H|E, F) et que nous n'ayons aucune information d'indépendance conditionnelle. Lequel des ensembles de nombres suivants est suffisant pour le calcul ?","P(E, F), P(H), P(E|H), P(F|H)","P(E, F), P(H), P(E, F|H)","P(H), P(E|H), P(F|H)","P(E, F), P(E|H), P(F|H)",B
"Parmi les éléments suivants, lequel empêche le surajustement lorsque nous effectuons le bagging ?",L'utilisation de l'échantillonnage avec remise comme technique d'échantillonnage,L'utilisation de classificateurs faibles,L'utilisation d'algorithmes de classification qui ne sont pas sujets au surapprentissage,La pratique de validation effectuée sur chaque classificateur entraîné,B
"Énoncé 1| L'ACP et le regroupement spectral (comme celui d'Andrew Ng) effectuent une décomposition en valeurs propres sur deux matrices différentes. Cependant, la taille de ces deux matrices est la même. Énoncé 2| Étant donné que la classification est un cas particulier de régression, la régression logistique est un cas particulier de régression linéaire.","Vrai, Vrai","Faux, Faux","Vrai, Faux","Faux, Vrai",B
"Déclaration 1| Le Stanford Sentiment Treebank contenait des critiques de films, pas des critiques de livres. Déclaration 2| Le Penn Treebank a été utilisé pour la modélisation du langage.","Vrai, Vrai","Faux, Faux","Vrai, Faux","Faux, Vrai",A
"Quelle est la dimensionnalité de l'espace nul de la matrice suivante ? A = [[3, 2, −9], [−6, −4, 18], [12, 8, −36]]",0,1,2,3,C
Que sont les vecteurs de support ?,Les exemples les plus éloignés de la frontière de décision.,Les seuls exemples nécessaires pour calculer f(x) dans un SVM.,Le centroïde des données.,Tous les exemples qui ont un poids non nul αk dans un SVM.,B
Déclaration 1| Les paramètres de Word2Vec n'ont pas été initialisés à l'aide d'une machine de Boltzmann restreinte. Déclaration 2| La fonction tanh est une fonction d'activation non linéaire.,"Vrai, Vrai","Faux, Faux","Vrai, Faux","Faux, Vrai",A
"Si votre perte d'entraînement augmente avec le nombre d'époques, lequel des éléments suivants pourrait être un problème possible avec le processus d'apprentissage ?",La régularisation est trop faible et le modèle est en surapprentissage,La régularisation est trop élevée et le modèle est sous-ajusté,La taille du pas est trop grande,La taille du pas est trop petite,C
"Disons que l'incidence d'une maladie D est d'environ 5 cas pour 100 personnes (c'est-à-dire, P(D) = 0,05). Soit la variable aléatoire booléenne D signifiant qu'un patient ""a la maladie D"" et soit la variable aléatoire booléenne TP signifiant ""test positif"". Les tests pour la maladie D sont connus pour être très précis dans le sens où la probabilité d'avoir un test positif quand on a la maladie est de 0,99, et la probabilité d'avoir un test négatif quand on n'a pas la maladie est de 0,97. Quelle est P(D | TP), la probabilité a posteriori d'avoir la maladie D lorsque le test est positif ?","0,0495","0,078","0,635","0,97",C
"Statement 1| Les résultats de l'apprentissage automatique traditionnel supposent que les ensembles d'entraînement et de test sont indépendants et identiquement distribués. Statement 2| En 2017, les modèles COCO étaient généralement pré-entraînés sur ImageNet.","Vrai, Vrai","Faux, Faux","Vrai, Faux","Faux, Vrai",A
"Énoncé 1| Les valeurs des marges obtenues par deux noyaux différents K1(x, x0) et K2(x, x0) sur le même ensemble d'entraînement ne nous indiquent pas quel classificateur fonctionnera mieux sur l'ensemble de test. Énoncé 2| La fonction d'activation de BERT est le GELU.","Vrai, Vrai","Faux, Faux","Vrai, Faux","Faux, Vrai",A
Lequel des éléments suivants est un algorithme de clustering en apprentissage automatique ?,Maximisation de l'espérance,PANIER,Bayes naïf gaussien,Apriori,A
"Vous venez de terminer l'entraînement d'un arbre de décision pour la classification des spams, et il obtient des performances anormalement mauvaises sur vos ensembles d'entraînement et de test. Vous savez que votre implémentation ne contient aucun bogue, alors qu'est-ce qui pourrait causer ce problème ?",Vos arbres de décision sont trop peu profonds.,Vous devez augmenter le taux d'apprentissage.,Vous êtes en surapprentissage.,Aucune des réponses ci-dessus.,A
La validation croisée k-fold est,linéaire en K,quadratique en K,cubique en K,exponentiel en K,A
"Déclaration 1| Les réseaux neuronaux à l'échelle industrielle sont normalement entraînés sur des CPU, pas des GPU. Déclaration 2| Le modèle ResNet-50 a plus d'un milliard de paramètres.","Vrai, Vrai","Faux, Faux","Vrai, Faux","Faux, Vrai",B
"Étant donné deux variables aléatoires booléennes, A et B, où P(A) = 1/2, P(B) = 1/3, et P(A | ¬B) = 1/4, quelle est P(A | B) ?",1/6,1/4,3/4,1,D
Les risques existentiels posés par l'IA sont le plus souvent associés à lequel des professeurs suivants ?,Nando de Frietas,Yann LeCun,Stuart Russell,Jitendra Malik,C
Énoncé 1| La maximisation de la vraisemblance du modèle de régression logistique produit plusieurs optimums locaux. Énoncé 2| Aucun classificateur ne peut surpasser un classificateur bayésien naïf si la distribution des données est connue.,"Vrai, Vrai","Faux, Faux","Vrai, Faux","Faux, Vrai",B
"Pour la régression par noyau, laquelle de ces hypothèses structurelles est celle qui affecte le plus le compromis entre sous-ajustement et surajustement :","Que la fonction noyau soit gaussienne, triangulaire ou rectangulaire","Que nous utilisions des métriques euclidiennes, L1 ou L∞",La largeur du noyau,La hauteur maximale de la fonction noyau,C
"Énoncé 1| L'algorithme d'apprentissage SVM est garanti de trouver l'hypothèse globalement optimale par rapport à sa fonction objectif. Énoncé 2| Après avoir été projeté dans l'espace de caractéristiques Q par une fonction noyau à base radiale, un Perceptron peut être capable d'obtenir de meilleures performances de classification que dans son espace d'origine (bien que nous ne puissions pas le garantir).","Vrai, Vrai","Faux, Faux","Vrai, Faux","Faux, Vrai",A
"Pour un classificateur bayésien gaussien, laquelle de ces hypothèses structurelles est celle qui affecte le plus le compromis entre sous-ajustement et surajustement :",Que nous apprenions les centres de classe par Maximum de Vraisemblance ou par Descente de Gradient,Que nous supposions des matrices de covariance de classe complètes ou des matrices de covariance de classe diagonales,Que nous ayons des a priori de classe égaux ou des a priori estimés à partir des données.,Que nous permettions aux classes d'avoir des vecteurs moyens différents ou que nous les forcions à partager le même vecteur moyen,B
Énoncé 1| Le surajustement est plus probable lorsque l'ensemble des données d'entraînement est petit. Énoncé 2| Le surajustement est plus probable lorsque l'espace des hypothèses est petit.,"Vrai, Vrai","Faux, Faux","Vrai, Faux","Faux, Vrai",D
"Statement 1| En plus de l'EM, la descente de gradient peut être utilisée pour effectuer l'inférence ou l'apprentissage sur un modèle de mélange gaussien. Statement 2 | En supposant un nombre fixe d'attributs, un classifieur bayésien optimal basé sur une distribution gaussienne peut être appris en un temps linéaire par rapport au nombre d'enregistrements dans l'ensemble de données.","Vrai, Vrai","Faux, Faux","Vrai, Faux","Faux, Vrai",A
"Énoncé 1| Dans un réseau bayésien, les résultats d'inférence de l'algorithme de l'arbre de jonction sont les mêmes que les résultats d'inférence de l'élimination de variables. Énoncé 2| Si deux variables aléatoires X et Y sont conditionnellement indépendantes étant donné une autre variable aléatoire Z, alors dans le réseau bayésien correspondant, les nœuds pour X et Y sont d-séparés étant donné Z.","Vrai, Vrai","Faux, Faux","Vrai, Faux","Faux, Vrai",C
"Étant donné un vaste ensemble de données de dossiers médicaux de patients souffrant de maladies cardiaques, essayez de déterminer s'il pourrait y avoir différents groupes de patients pour lesquels nous pourrions adapter des traitements distincts. Quel type de problème d'apprentissage est-ce ?",Apprentissage supervisé,Apprentissage non supervisé,Les deux (a) et (b),Ni (a) ni (b),B
Que feriez-vous en ACP pour obtenir la même projection qu'avec la DVS ?,Transformer les données pour obtenir une moyenne nulle,Transformer les données pour obtenir une médiane nulle,Pas possible,Aucun de ceux-ci,A
"Énoncé 1| L'erreur d'entraînement du classificateur du plus proche voisin (1-NN) est de 0.

Énoncé 2| Lorsque le nombre de points de données tend vers l'infini, l'estimation MAP se rapproche de l'estimation MLE pour tous les a priori possibles. En d'autres termes, avec suffisamment de données, le choix de l'a priori devient non pertinent.","Vrai, Vrai","Faux, Faux","Vrai, Faux","Faux, Vrai",C
"Lors de la régression des moindres carrés avec régularisation (en supposant que l'optimisation puisse être effectuée exactement), l'augmentation de la valeur du paramètre de régularisation λ l'erreur de test.",ne diminuera jamais l'erreur d'entraînement.,n'augmentera jamais l'erreur d'entraînement.,ne diminuera jamais l'erreur de test.,n'augmentera jamais,A
Laquelle des propositions suivantes décrit le mieux ce que les approches discriminatives essaient de modéliser ? (w sont les paramètres du modèle),"p(y|x, w)","p(y, x)","p(w|x, w)",Aucune des réponses ci-dessus,A
Énoncé 1| La performance de classification CIFAR-10 pour les réseaux de neurones convolutifs peut dépasser 95%. Énoncé 2| Les ensembles de réseaux de neurones n'améliorent pas la précision de classification car les représentations qu'ils apprennent sont hautement corrélées.,"Vrai, Vrai","Faux, Faux","Vrai, Faux","Faux, Vrai",C
Sur lequel des points suivants les bayésiens et les fréquentistes seraient-ils en désaccord ?,L'utilisation d'un modèle de bruit non gaussien dans la régression probabiliste.,L'utilisation de la modélisation probabiliste pour la régression.,L'utilisation de distributions a priori sur les paramètres dans un modèle probabiliste.,L'utilisation des probabilités a priori des classes dans l'Analyse Discriminante Gaussienne.,C
"Déclaration 1| La métrique BLEU utilise la précision, tandis que la métrique ROGUE utilise le rappel. Déclaration 2| Les modèles de Markov cachés étaient fréquemment utilisés pour modéliser les phrases en anglais.","Vrai, Vrai","Faux, Faux","Vrai, Faux","Faux, Vrai",A
Énoncé 1| ImageNet contient des images de diverses résolutions. Énoncé 2| Caltech-101 a plus d'images qu'ImageNet.,"Vrai, Vrai","Faux, Faux","Vrai, Faux","Faux, Vrai",C
Laquelle des options suivantes est la plus appropriée pour effectuer une sélection de caractéristiques ?,Crête,Lasso,à la fois (a) et (b),ni (a) ni (b),B
Supposez que l'on vous donne un algorithme EM qui trouve des estimations de maximum de vraisemblance pour un modèle avec des variables latentes. On vous demande de modifier l'algorithme pour qu'il trouve des estimations MAP à la place. Quelle(s) étape(s) devez-vous modifier ?,Attente,Maximisation,Aucune modification nécessaire,Les deux,B
"Pour un classificateur bayésien gaussien, laquelle de ces hypothèses structurelles est celle qui affecte le plus le compromis entre sous-ajustement et surajustement :",Que nous apprenions les centres de classe par Maximum de Vraisemblance ou par Descente de Gradient,Que nous supposions des matrices de covariance de classe complètes ou des matrices de covariance de classe diagonales,Que nous ayons des a priori de classe égaux ou des a priori estimés à partir des données,Que nous permettions aux classes d'avoir des vecteurs moyens différents ou que nous les forcions à partager le même vecteur moyen,B
"Énoncé 1| Pour toutes deux variables x et y ayant une distribution conjointe p(x, y), nous avons toujours H[x, y] ≥ H[x] + H[y] où H est la fonction d'entropie. Énoncé 2| Pour certains graphes orientés, la moralisation diminue le nombre d'arêtes présentes dans le graphe.","Vrai, Vrai","Faux, Faux","Vrai, Faux","Faux, Vrai",B
Lequel des éléments suivants N'EST PAS de l'apprentissage supervisé ?,ACP,Arbre de décision,Régression linéaire,Bayésien naïf,A
Énoncé 1| La convergence d'un réseau neuronal dépend du taux d'apprentissage. Énoncé 2| Le dropout multiplie par zéro des valeurs d'activation choisies aléatoirement.,"Vrai, Vrai","Faux, Faux","Vrai, Faux","Faux, Vrai",A
"Laquelle des propositions suivantes est égale à P(A, B, C) étant donné les variables aléatoires booléennes A, B et C, et sans hypothèses d'indépendance ou d'indépendance conditionnelle entre elles ?",P(A | B) * P(B | C) * P(C | A),"P(C | A, B) * P(A) * P(B)","P(A, B | C) * P(C)","P(A | B, C) * P(B | A, C) * P(C | A, B)",C
Laquelle des tâches suivantes peut être le mieux résolue en utilisant le Clustering.,Prédire la quantité de précipitations en fonction de divers indices,Détection des transactions frauduleuses par carte de crédit,Entraîner un robot à résoudre un labyrinthe,Tout ce qui précède,B
"Après avoir appliqué une pénalité de régularisation dans la régression linéaire, vous constatez que certains des coefficients de w sont réduits à zéro. Laquelle des pénalités suivantes aurait pu être utilisée ?",Norme L0,Norme L1,Norme L2,soit (a) soit (b),D
"A et B sont deux événements. Si P(A, B) diminue tandis que P(A) augmente, laquelle des affirmations suivantes est vraie ?",P(A|B) diminue,P(B|A) diminue,P(B) diminue,Tout ce qui précède,B
"Énoncé 1| Lors de l'apprentissage d'un MMC pour un ensemble fixe d'observations, en supposant que nous ne connaissons pas le véritable nombre d'états cachés (ce qui est souvent le cas), nous pouvons toujours augmenter la vraisemblance des données d'entraînement en autorisant plus d'états cachés. Énoncé 2| Le filtrage collaboratif est souvent un modèle utile pour modéliser les préférences cinématographiques des utilisateurs.","Vrai, Vrai","Faux, Faux","Vrai, Faux","Faux, Vrai",A
"Vous entraînez un modèle de régression linéaire pour une tâche d'estimation simple, et vous remarquez que le modèle est en surapprentissage par rapport aux données. Vous décidez d'ajouter une régularisation $\ell_2$ pour pénaliser les poids. À mesure que vous augmentez le coefficient de régularisation $\ell_2$, que se passera-t-il avec le biais et la variance du modèle ?",Augmentation du biais ; Augmentation de la variance,Augmentation du biais ; Diminution de la variance,Diminution du biais ; Augmentation de la variance,Diminution du biais ; Diminution de la variance,B
"Quelle(s) commande(s) PyTorch 1.8 produit(produisent) une matrice gaussienne $10\times 5$ avec chaque entrée échantillonnée de manière i.i.d. à partir de $\mathcal{N}(\mu=5,\sigma^2=16)$ et une matrice uniforme $10\times 10$ avec chaque entrée échantillonnée de manière i.i.d. à partir de $U[-1,1)$ ?","\texttt{5 + torch.randn(10,5) * 16} ; \texttt{torch.rand(10,10,low=-1,high=1)}","\texttt{5 + torch.randn(10,5) * 16} ; \texttt{(torch.rand(10,10) - 0,5) / 0,5}","\texttt{5 + torch.randn(10,5) * 4} ; \texttt{2 * torch.rand(10,10) - 1}","\texttt{torch.normal(torch.ones(10,5)*5,torch.ones(5,5)*16)} ; \texttt{2 * torch.rand(10,10) - 1}",C
"Énoncé 1| Le gradient de la ReLU est zéro pour $x<0$, et le gradient de la sigmoïde $\sigma(x)(1-\sigma(x))\le \frac{1}{4}$ pour tout $x$. Énoncé 2| La sigmoïde a un gradient continu et la ReLU a un gradient discontinu.","Vrai, Vrai","Faux, Faux","Vrai, Faux","Faux, Vrai",A
Qu'est-ce qui est vrai à propos de la normalisation par lots ?,"Après l'application de la normalisation par lots, les activations de la couche suivront une distribution gaussienne standard.",Le paramètre de biais des couches affines devient redondant si une couche de normalisation par lots suit immédiatement après.,L'initialisation standard des poids doit être modifiée lors de l'utilisation de la normalisation par lots.,La normalisation par lots est équivalente à la normalisation de couche pour les réseaux de neurones convolutifs.,B
Supposons que nous ayons la fonction objectif suivante : $\argmin_{w} \frac{1}{2} \norm{Xw-y}^2_2 + \frac{1}{2}\gamma \norm{w}^2_2$ Quel est le gradient de $\frac{1}{2} \norm{Xw-y}^2_2 + \frac{1}{2}\lambda \norm{w}^2_2$ par rapport à $w$ ?,$\nabla_w f(w) = (X^\top X + \lambda I)w - X^\top y + \lambda w$,$\nabla_w f(w) = X^\top X w - X^\top y + \lambda$,$\nabla_w f(w) = X^\top X w - X^\top y + \lambda w$,$\nabla_w f(w) = X^\top X w - X^\top y + (\lambda+1) w$,C
Laquelle des affirmations suivantes est vraie concernant un noyau de convolution ?,Convoluer une image avec $\begin{bmatrix}1 & 0 & 0\\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix}$ ne changerait pas l'image,Convoluer une image avec $\begin{bmatrix}0 & 0 & 0\\ 0 & 1 & 0 \\ 0 & 0 & 0 \end{bmatrix}$ ne modifierait pas l'image,Convoluer une image avec $\begin{bmatrix}1 & 1 & 1\\ 1 & 1 & 1 \\ 1 & 1 & 1 \end{bmatrix}$ ne changerait pas l'image,Convoluer une image avec $\begin{bmatrix}0 & 0 & 0\\ 0 & 0 & 0 \\ 0 & 0 & 0 \end{bmatrix}$ ne changerait pas l'image,B
Laquelle des affirmations suivantes est fausse ?,"Les modèles de segmentation sémantique prédisent la classe de chaque pixel, tandis que les classificateurs d'images multiclasses prédisent la classe de l'image entière.",Une boîte englobante avec un IoU (intersection sur union) égal à 96 % serait probablement considérée comme un vrai positif.,"Lorsqu'une boîte englobante prédite ne correspond à aucun objet dans la scène, elle est considérée comme un faux positif.",Une boîte englobante avec un IoU (intersection sur union) égal à 3 % serait probablement considérée comme un faux négatif.,D
Laquelle des affirmations suivantes est fausse ?,"Le réseau entièrement connecté suivant sans fonctions d'activation est linéaire : $g_3(g_2(g_1(x)))$, où $g_i(x) = W_i x$ et $W_i$ sont des matrices.","La fonction ReLU fuyante $\max\{0.01x,x\}$ est convexe.",Une combinaison de ReLUs telle que $ReLU(x) - ReLU(x-1)$ est convexe.,La perte $\log \sigma(x)= -\log(1+e^{-x})$ est concave,C
"Nous entraînons un réseau entièrement connecté avec deux couches cachées pour prédire les prix des logements. Les entrées sont de dimension $100$ et comportent plusieurs caractéristiques telles que le nombre de pieds carrés, le revenu médian des familles, etc. La première couche cachée a $1000$ activations. La deuxième couche cachée a $10$ activations. La sortie est un scalaire représentant le prix de la maison. En supposant un réseau classique avec des transformations affines et sans normalisation par lots ni paramètres apprenables dans la fonction d'activation, combien de paramètres ce réseau possède-t-il ?",111021,110010,111110,110011,A
Énoncé 1| La dérivée de la sigmoïde $\sigma(x)=(1+e^{-x})^{-1}$ par rapport à $x$ est égale à $\text{Var}(B)$ où $B\sim \text{Bern}(\sigma(x))$ est une variable aléatoire de Bernoulli. Énoncé 2| Régler les paramètres de biais dans chaque couche du réseau neuronal à 0 modifie le compromis biais-variance de telle sorte que la variance du modèle augmente et le biais du modèle diminue.,"Vrai, Vrai","Faux, Faux","Vrai, Faux","Faux, Vrai",C
